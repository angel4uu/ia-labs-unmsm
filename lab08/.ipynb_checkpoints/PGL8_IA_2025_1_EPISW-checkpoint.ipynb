{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9GnUElm5_e8m"
   },
   "source": [
    "# Práctica de Laboratorio 8 - Inteligencia Artificial 2025-1 Sección 1 EPISW-FISI\n",
    "## Implementación de una red PMC-BP con Python y Numpy\n",
    "### Prof. Rolando A. Maguiña Pérez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ky22d26K_e8n"
   },
   "source": [
    "## Introducción\n",
    "La Práctica Guiada de Laboratorio (PGL) 8 a realizarse el Jueves 05 de Junio del presente año, tratará sobre la red Perceptrón Multicapa con su algoritmo de aprendizaje llamado Backpropagation. Esta red se aplicará para resolver problemas genéricos de clasificación y de regresión.\n",
    "\n",
    "Se desea abordar el problema de la aproximación de una función mediante una Perceptrón Multicapa-Backpropagation (PMC-BP). Inicialmente se presenta la implementación del algoritmo de entrenamiento de esta red (presentado en las sesiones de teoría), con el lenguaje `Python` y sus bibliotecas `Numpy` y `Matplotlib`. Posteriormente, **se propondrán algunos ejercicios cuyas soluciones se podrán obtener en grupos de hasta 4 alumnos**, y deberán enviarse para su respectiva revisión (ver sección 'Instrucciones para el envío' en este mismo cuaderno).  \n",
    "\n",
    "Requiere: numpy, matplotlib\n",
    "\n",
    "Nomenclatura:\n",
    "- Z: número de instancias (muestras) en el conjunto de datos\n",
    "- N: número de atributos o variables de entrada\n",
    "- M: número de atributos o variables de salida\n",
    "- t: vector de salidas esperadas o targets\n",
    "- y: vector de salidas estimadas por la red.\n",
    "\n",
    "### Paso previo\n",
    "Importamos las bibliotecas de Python requeridas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "5r_BRXlK_e8n"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bKy9zmzb_e8p"
   },
   "source": [
    "## Dataset\n",
    "El primer paso consiste en obtener el arreglo conteniendo los pares entrada-salida (instancias) a usar en el entrenamiento/validación de la red PMC-BP a implementar; dicho arreglo se denominará 'Dataset'. El tamaño de dicho arreglo es de $Z \\times d$, donde $Z$ es el número de instancias (muestras) y $d$ es el número de características o atributos considerados para el problema abordado (incluye los atributos de entrada y los de salida)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "lHAeppdW_e8q",
    "outputId": "be4a459f-13d9-425f-bd6d-6e0cbbe7d8cd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.  ,  0.  ],\n",
       "       [ 1.  ,  0.84],\n",
       "       [ 2.  ,  0.91],\n",
       "       [ 3.  ,  0.14],\n",
       "       [ 4.  , -0.77],\n",
       "       [ 5.  , -0.96],\n",
       "       [ 6.  , -0.28],\n",
       "       [ 7.  ,  0.66],\n",
       "       [ 8.  ,  0.99]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dataset = np.array([[0.00000, 0.00000], [1.00000, 0.84000], [2.00000, 0.91000], [3.00000, 0.14000],\n",
    "[4.00000, -0.77000], [5.00000, -0.96000],[6.00000, -0.28000], [7.00000, 0.66000], [8.00000, 0.99000]])\n",
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "PQUmblTn_e8q",
    "outputId": "339662ed-f65c-4cb2-d5ba-a944a8775d4b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5xJFV52G_e8q"
   },
   "source": [
    "## Data para el entrenamiento/validación de la red\n",
    "Como sabemos, a partir del dataset obtenido, se deben determinar los conjuntos de datos a emplear en el entrenamiento y en la validación de la red PMC-BP. Enseguida, se deben obtener dos arreglos: uno con los vectores de entrada a usar en el entrenamiento, y el segundo, con los respectivos vectores de salida. Análogamente, se deben determinar los arreglos con los vectores de entrada y de salida, a usar en la validación del entrenamiento. **Sin embargo, para el problema planteado, el dataset se usará tanto para el entrenamiento como para la validación**.\n",
    "\n",
    "Separando los valores de entrada de los de salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Hr48ISV7_e8q",
    "outputId": "649fd8d4-c634-4166-8853-9208bad639a8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 1., 2., 3., 4., 5., 6., 7., 8.]),\n",
       " array([ 0.  ,  0.84,  0.91,  0.14, -0.77, -0.96, -0.28,  0.66,  0.99]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = Dataset[:,0]\n",
    "t = Dataset[:,1]\n",
    "X, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "L0tmqcxX_e8r",
    "outputId": "c5dd6b48-055e-4a0e-a4b4-5ba76e3800f1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9,), (9,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "sxiXCKVT_e8r",
    "outputId": "dc3bdc08-c4cd-47c4-a353-61323d72eba7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2358a551250>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(4, 4))\n",
    "plt.plot(X,t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cQJsVNIL_e8r"
   },
   "source": [
    "## Normalización de los datos\n",
    "Antes de iniciar algún cálculo, sabemos que debemos tener en cuenta las diferencias que existen en las unidades de nuestros datos. Se requiere que los datos de nuestras variables estén en el mismo orden de magnitud, y en un buen número de casos es necesario normalizarlos; de esta manera nuestro modelo trabajará con unidades normalizadas. A pesar de lo indicado, incluso sabedores que hay varios procedimientos de normalización, en este caso, **no vamos a normalizar inicialmente nuestros datos**.\n",
    "\n",
    "## Diseño de la red\n",
    "Inicialmente se considera una topología de la red como la mostrada en la figura, vale decir, con 10 neuronas ocultas. Como función de activación de las neuronas ocultas se usará la logística sigmoidea y en las neuronas de salida, dado que se trata de un problema de aproximación de funciones, se usará una función lineal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "WGsNXQEa_e8r",
    "outputId": "855b611b-4ff4-4ed2-bdaa-cf13deb780df"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:\\\\Cursos\\\\Redes Neuronales\\\\2021-2\\\\arquit-red_aprox-fc_2021-2.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m----> 2\u001b[0m i \u001b[38;5;241m=\u001b[39m Image(filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mCursos\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mRedes Neuronales\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m2021-2\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124marquit-red_aprox-fc_2021-2.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m i\n",
      "File \u001b[1;32mc:\\Users\\hp\\anaconda3\\Lib\\site-packages\\IPython\\core\\display.py:970\u001b[0m, in \u001b[0;36mImage.__init__\u001b[1;34m(self, data, url, filename, format, embed, width, height, retina, unconfined, metadata, alt)\u001b[0m\n\u001b[0;32m    968\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munconfined \u001b[38;5;241m=\u001b[39m unconfined\n\u001b[0;32m    969\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malt \u001b[38;5;241m=\u001b[39m alt\n\u001b[1;32m--> 970\u001b[0m \u001b[38;5;28msuper\u001b[39m(Image, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(data\u001b[38;5;241m=\u001b[39mdata, url\u001b[38;5;241m=\u001b[39murl, filename\u001b[38;5;241m=\u001b[39mfilename,\n\u001b[0;32m    971\u001b[0m         metadata\u001b[38;5;241m=\u001b[39mmetadata)\n\u001b[0;32m    973\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwidth \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwidth\u001b[39m\u001b[38;5;124m'\u001b[39m, {}):\n\u001b[0;32m    974\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwidth \u001b[38;5;241m=\u001b[39m metadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwidth\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\hp\\anaconda3\\Lib\\site-packages\\IPython\\core\\display.py:327\u001b[0m, in \u001b[0;36mDisplayObject.__init__\u001b[1;34m(self, data, url, filename, metadata)\u001b[0m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m--> 327\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreload()\n\u001b[0;32m    328\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_data()\n",
      "File \u001b[1;32mc:\\Users\\hp\\anaconda3\\Lib\\site-packages\\IPython\\core\\display.py:1005\u001b[0m, in \u001b[0;36mImage.reload\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1003\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[39;00m\n\u001b[0;32m   1004\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed:\n\u001b[1;32m-> 1005\u001b[0m     \u001b[38;5;28msuper\u001b[39m(Image,\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mreload()\n\u001b[0;32m   1006\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretina:\n\u001b[0;32m   1007\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retina_shape()\n",
      "File \u001b[1;32mc:\\Users\\hp\\anaconda3\\Lib\\site-packages\\IPython\\core\\display.py:353\u001b[0m, in \u001b[0;36mDisplayObject.reload\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    352\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_flags \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 353\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilename, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_flags, encoding\u001b[38;5;241m=\u001b[39mencoding) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    354\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m    355\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    356\u001b[0m     \u001b[38;5;66;03m# Deferred import\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:\\\\Cursos\\\\Redes Neuronales\\\\2021-2\\\\arquit-red_aprox-fc_2021-2.png'"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "i = Image(filename='D:\\\\Cursos\\\\Redes Neuronales\\\\2021-2\\\\arquit-red_aprox-fc_2021-2.png')\n",
    "i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DkStZNAt_e8s"
   },
   "source": [
    "## Inicialización de los pesos y biases de la red\n",
    "Según el algoritmo, los parámetros libres de la red se inicializan a valores aleatorios pequeños, los cuales pueden estar en el rangos: [-0.5,0.5] o [-1,1] o en torno de cero. A continuación se presenta el código para inicializarlos, aplicado al **problema planteado**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lVovH8l6_e8s"
   },
   "outputs": [],
   "source": [
    "# Implementación básica sin funciones\n",
    "intervalo = 0.5\n",
    "capa_entrada = 1\n",
    "capa_oculta = 10\n",
    "capa_salida = 1\n",
    "\n",
    "w1 = np.random.uniform(-intervalo, intervalo, capa_oculta)\n",
    "\n",
    "w2 = np.random.uniform(-intervalo, intervalo, capa_oculta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CZ0RMbRe_e8s",
    "outputId": "f683c42b-d53b-4b2e-ea4a-14907601f2eb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.31363851, -0.14824543, -0.11532856, -0.47448722,  0.30202055,\n",
       "        -0.07598017,  0.36598791, -0.46703207,  0.15857095, -0.43671307]),\n",
       " array([ 0.16887422, -0.3305602 ,  0.02856265, -0.0367157 ,  0.03012444,\n",
       "         0.28213003,  0.36947367, -0.12383828,  0.18071988,  0.46923731]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1, w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lmCcNM_d_e8s",
    "outputId": "9d457a8a-6bf1-4af0-b0c6-588ff5716fa7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10,), (10,))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mBsY50rR_e8t"
   },
   "source": [
    "### Definición de la función logística sigmoidea\n",
    "Sabemos que la expresión matemática de la función logística sigmoidea `f(n)` es:\n",
    "\n",
    "                         f(u) =  1/1 + exp(-u)\n",
    "\n",
    "donde `u` es el vector de entradas netas. A partir de dicho parámetro, es posible calcular la función logistica sigmoidea; en la sgte celda se presenta el respectivo código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "epFEjYWl_e8t"
   },
   "outputs": [],
   "source": [
    "# Funcion de activacion Logistica Sigmoidea para la unidad de salida\n",
    "def logistica(u):\n",
    "    return 1/(1 + np.exp(-u))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Th5dgmub_e8t"
   },
   "source": [
    "Supongamos que se desea aplicar esta función al arreglo 'a'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TAEvUwwY_e8t",
    "outputId": "8437d4cb-1c63-4f29-c846-8e595401d360"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0. ,  0.6, -0.8]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[0, 0.6, -0.8]])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9iHeZAXi_e8t",
    "outputId": "c8eae3ea-8366-4e37-df4a-31a00e9f57e7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5       , 0.64565631, 0.31002552]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistica(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t4M6Vnr-_e8t"
   },
   "source": [
    "A continuación se presenta la implementación de la derivada de la función logística sigmoidea:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "3XH0afOY_e8u"
   },
   "outputs": [],
   "source": [
    "def deriv_logistica(x):\n",
    "    return x * (1.0 - x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CQBQqCLR_e8u",
    "outputId": "7b7de73a-c865-432a-c424-840452937c46"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4.8576"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deriv_logistica(-1.76)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bPor0YoF_e8u"
   },
   "source": [
    "## Implementación\n",
    "Luego de haber determinado la topología de la red neuronal, la implementaremos en el lenguaje de programación `Python` con la ayuda de su biblioteca `Numpy`. Enseguida, se efectuarán las sgtes actividades:\n",
    "\n",
    "- Construiremos el algoritmo de aprendizaje de nuestra red PMC, Backpropagation, mediante la función `train()`. Dentro de ella se instancian constantes y variables importantes como globales, de modo que estos valores sean accesibles para toda la función.\n",
    "- Aplicaremos dicho algoritmo de aprendizaje para resolver el problema de aproximación de una función planteado; para tal efecto, se usará el conjunto de datos disponible.\n",
    "\n",
    "En las sgtes celdas se presentan las líneas de código correspondientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6M-xeAvc_e8u"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NETtXa7d_e8u"
   },
   "outputs": [],
   "source": [
    "Dataset = np.array([[0.00000, 0.00000], [1.00000, 0.84000], [2.00000, 0.91000], [3.00000, 0.14000],\n",
    "[4.00000, -0.77000], [5.00000, -0.96000],[6.00000, -0.28000], [7.00000, 0.66000], [8.00000, 0.99000]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0YVIEN_G_e8v",
    "outputId": "09e186ca-8f01-4101-f9fc-2c82d36c0d54"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 1., 2., 3., 4., 5., 6., 7., 8.]),\n",
       " array([ 0.  ,  0.84,  0.91,  0.14, -0.77, -0.96, -0.28,  0.66,  0.99]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = Dataset[:,0]\n",
    "t = Dataset[:,1]\n",
    "X, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2nIZHvFq_e8v"
   },
   "outputs": [],
   "source": [
    "def train(X, t, learning_rate=0.2, epochs=50):\n",
    "    global input_num\n",
    "    global hidden_num\n",
    "    global w1\n",
    "    global w2\n",
    "\n",
    "    input_num = 1\n",
    "    hidden_num = 10\n",
    "    output_num = 1\n",
    "    intervalo = 0.5\n",
    "\n",
    "    # inicializando los pesos\n",
    "    w1 = np.random.uniform(-intervalo, intervalo, hidden_num)\n",
    "\n",
    "    w2 = np.random.uniform(-intervalo, intervalo, hidden_num)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        gradient_out = 0.0                 # gradientes para la capa de salida y la capa oculta\n",
    "        gradient_hidden = []\n",
    "\n",
    "        for i in range(X.shape[0]):\n",
    "        # propagacion hacia adelante\n",
    "            x = X[i]\n",
    "\n",
    "            u1 = x * w1\n",
    "            o = logistica(u1)\n",
    "            u2 = o.dot(w2)\n",
    "            y = u2\n",
    "\n",
    "        # backpropagation\n",
    "            delta_hidden_s = []           # inicializamos los delta_j a lista vacía\n",
    "            gradient_hidden_s = []       # inicializamos los gradientes de neurs ocultas a lista vacía\n",
    "\n",
    "            delta_out_s = t[i] - y     # cálculo del único delta_k (f'(u) = 1 pq fc de activ es lineal)\n",
    "            gradient_out_s = delta_out_s * o     # error por la salida de la capa anterior\n",
    "\n",
    "            for j in range(hidden_num):\n",
    "\n",
    "                delta_hidden_s.append(deriv_logistica(u1[j]) * w2[j] * delta_out_s)\n",
    "                gradient_hidden_s.append(delta_hidden_s[j] * x)\n",
    "\n",
    "\n",
    "            gradient_out = gradient_out + gradient_out_s\n",
    "            gradient_hidden = gradient_hidden + gradient_hidden_s\n",
    "\n",
    "\n",
    "        print(\"\\n#\", epoch, \"Gradient out: \",gradient_out),\n",
    "        print(\"\\n     Weights  out: \", w1, w2)\n",
    "\n",
    "        # Ahora actualizando pesos\n",
    "        w2 = w2 + learning_rate * gradient_out\n",
    "\n",
    "        for j in range(hidden_num):\n",
    "            w1[j] = w1[j] + learning_rate * gradient_hidden[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oJgS7qCi_e8v",
    "outputId": "dc1f053a-dec7-40d4-fa7f-6db8b843e96a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# 0 Gradient out:  [4.55032902 3.99588928 2.18558607 3.57836834 5.68981355 1.58579721\n",
      " 6.1969063  6.14822678 6.55633718 3.62899824]\n",
      "\n",
      "     Weights  out:  [ 0.06344919 -0.00187795 -0.26144234 -0.05084979  0.23101741 -0.44071259\n",
      "  0.35373613  0.3391126   0.49485115 -0.04481661] [ 0.15432564 -0.39507629  0.3918821  -0.33849019  0.28772399  0.01785262\n",
      " -0.40524365 -0.02617312 -0.34005255 -0.3608523 ]\n",
      "\n",
      "# 1 Gradient out:  [-23.55619843 -20.69470758 -10.98868836 -18.54201844 -29.71057124\n",
      "  -7.38907647 -32.69695894 -32.40267715 -34.87905876 -18.80398552]\n",
      "\n",
      "     Weights  out:  [ 0.06344919 -0.00187795 -0.26144234 -0.05084979  0.23101741 -0.44071259\n",
      "  0.35373613  0.3391126   0.49485115 -0.04481661] [1.06439144 0.40410157 0.82899931 0.37718348 1.4256867  0.33501206\n",
      " 0.83413761 1.20347224 0.97121488 0.36494735]\n",
      "\n",
      "# 2 Gradient out:  [123.62970742 108.58078662  57.91034397  97.25731722 155.71418452\n",
      "  39.50420171 171.03719075 169.53429574 182.17011454  98.63434998]\n",
      "\n",
      "     Weights  out:  [ 0.06344919 -0.00187795 -0.26144234 -0.05084979  0.23101741 -0.44071259\n",
      "  0.35373613  0.3391126   0.49485115 -0.04481661] [-3.64684824 -3.73483995 -1.36873836 -3.33122021 -4.51642755 -1.14280324\n",
      " -5.70525418 -5.27706319 -6.00459687 -3.39584976]\n",
      "\n",
      "# 3 Gradient out:  [-647.22525137 -568.49088855 -303.01051265 -509.24992985 -815.37230649\n",
      " -206.17869457 -895.92810755 -888.01968073 -954.52485388 -516.45510756]\n",
      "\n",
      "     Weights  out:  [ 0.06344919 -0.00187795 -0.26144234 -0.05084979  0.23101741 -0.44071259\n",
      "  0.35373613  0.3391126   0.49485115 -0.04481661] [21.07909324 17.98131738 10.21333044 16.12024324 26.62640936  6.75803711\n",
      " 28.50218397 28.62979596 30.42942604 16.33102024]\n",
      "\n",
      "# 4 Gradient out:  [3389.89269928 2977.4499164  1587.13764084 2667.119295   4270.42798071\n",
      " 1080.43508898 4692.01905624 4650.6376908  4998.61446751 2704.86226686]\n",
      "\n",
      "     Weights  out:  [ 0.06344919 -0.00187795 -0.26144234 -0.05084979  0.23101741 -0.44071259\n",
      "  0.35373613  0.3391126   0.49485115 -0.04481661] [-108.36595703  -95.71686033  -50.3887721   -85.72974273 -136.44805194\n",
      "  -34.47770181 -150.68343754 -148.97414019 -160.47554474  -86.96000127]\n",
      "\n",
      "# 5 Gradient out:  [-17753.34293285 -15593.40102456  -8312.0204617  -13968.21759582\n",
      " -22364.95162301  -5657.89571931 -24573.2001143  -24356.44120969\n",
      " -26179.18914395 -14165.87632252]\n",
      "\n",
      "     Weights  out:  [ 0.06344919 -0.00187795 -0.26144234 -0.05084979  0.23101741 -0.44071259\n",
      "  0.35373613  0.3391126   0.49485115 -0.04481661] [569.61258282 499.77312295 267.03875607 447.69411627 717.6375442\n",
      " 181.60931599 787.72037371 781.15339797 839.24734876 454.0124521 ]\n",
      "\n",
      "# 6 Gradient out:  [ 92978.1766448   81665.99404378  43531.88180506  73154.47964563\n",
      " 117130.08938137  29632.0892401  128694.86515302 127559.68917889\n",
      " 137105.48155934  74189.66956426]\n",
      "\n",
      "     Weights  out:  [ 0.06344919 -0.00187795 -0.26144234 -0.05084979  0.23101741 -0.44071259\n",
      "  0.35373613  0.3391126   0.49485115 -0.04481661] [-2981.05600375 -2618.90708197 -1395.36533627 -2345.9494029\n",
      " -3755.3527804   -949.96982787 -4126.91964915 -4090.13484397\n",
      " -4396.59048003 -2379.1628124 ]\n",
      "\n",
      "# 7 Gradient out:  [-486945.850371   -427701.72090288 -227985.46948305 -383125.24501586\n",
      " -613434.47061135 -155188.95656804 -674001.88434517 -668056.69637034\n",
      " -718050.3137083  -388546.73932261]\n",
      "\n",
      "     Weights  out:  [ 0.06344919 -0.00187795 -0.26144234 -0.05084979  0.23101741 -0.44071259\n",
      "  0.35373613  0.3391126   0.49485115 -0.04481661] [15614.57932521 13714.29172679  7311.01102474 12284.94652623\n",
      " 19670.66509587  4976.44802015 21612.05338145 21421.80299181\n",
      " 23024.50583184 12458.77110045]\n",
      "\n",
      "# 8 Gradient out:  [2550237.00666104 2239963.03357524 1194007.39028284 2006506.64657669\n",
      " 3212684.2333405   812757.29737248 3529888.11810303 3498751.96563441\n",
      " 3760578.63469579 2034900.15242493]\n",
      "\n",
      "     Weights  out:  [ 0.06344919 -0.00187795 -0.26144234 -0.05084979  0.23101741 -0.44071259\n",
      "  0.35373613  0.3391126   0.49485115 -0.04481661] [ -81774.59074899  -71826.05245379  -38286.08287187  -64340.10247694\n",
      " -103016.2290264   -26061.34329346 -113188.32348758 -112189.53628226\n",
      " -120585.55690982  -65250.57676407]\n",
      "\n",
      "# 9 Gradient out:  [-13356121.92257883 -11731152.66981859  -6253265.28684436\n",
      " -10508493.07565057 -16825495.96048642  -4256578.78265215\n",
      " -18486759.0087647  -18323692.46181834 -19694933.54753455\n",
      " -10657195.764902  ]\n",
      "\n",
      "     Weights  out:  [ 0.06344919 -0.00187795 -0.26144234 -0.05084979  0.23101741 -0.44071259\n",
      "  0.35373613  0.3391126   0.49485115 -0.04481661] [428272.81058322 376166.55426126 200515.3951847  336961.2268384\n",
      " 539520.6176417  136490.11618103 592789.30013303 587560.85684463\n",
      " 631530.17002934 341729.45372091]\n",
      "\n",
      "# 10 Gradient out:  [6.99487913e+07 6.14384889e+07 3.27496521e+07 5.50351659e+07\n",
      " 8.81186254e+07 2.22925896e+07 9.68190056e+07 9.59649922e+07\n",
      " 1.03146467e+08 5.58139529e+07]\n",
      "\n",
      "     Weights  out:  [ 0.06344919 -0.00187795 -0.26144234 -0.05084979  0.23101741 -0.44071259\n",
      "  0.35373613  0.3391126   0.49485115 -0.04481661] [-2242951.57393254 -1970063.97970245 -1050137.66218417 -1764737.38829172\n",
      " -2825578.57445558  -714825.6403494  -3104562.50161991 -3077177.63551904\n",
      " -3307456.53947757 -1789709.69925949]\n",
      "\n",
      "# 11 Gradient out:  [-3.66336382e+08 -3.21766157e+08 -1.71516746e+08 -2.88230622e+08\n",
      " -4.61495586e+08 -1.16750932e+08 -5.07061289e+08 -5.02588643e+08\n",
      " -5.40199521e+08 -2.92309291e+08]\n",
      "\n",
      "     Weights  out:  [ 0.06344919 -0.00187795 -0.26144234 -0.05084979  0.23101741 -0.44071259\n",
      "  0.35373613  0.3391126   0.49485115 -0.04481661] [11746806.68328201 10317633.79371253  5499792.76277061  9242295.79556745\n",
      " 14798146.50818412  3743692.28171349 16259238.61606238 16115820.79802267\n",
      " 17321836.84039783  9373080.8754226 ]\n",
      "\n",
      "# 12 Gradient out:  [1.91857990e+09 1.68515635e+09 8.98268908e+08 1.50952377e+09\n",
      " 2.41694846e+09 6.11448940e+08 2.65558553e+09 2.63216135e+09\n",
      " 2.82913736e+09 1.53088461e+09]\n",
      "\n",
      "     Weights  out:  [ 0.06344919 -0.00187795 -0.26144234 -0.05084979  0.23101741 -0.44071259\n",
      "  0.35373613  0.3391126   0.49485115 -0.04481661] [-61520469.75787799 -54035597.57639053 -28803556.49492902\n",
      " -48403828.52395044 -77500970.73903395 -19606494.21521056\n",
      " -85153019.14001736 -84401907.73553213 -90718067.40678012\n",
      " -49088777.32394538]\n",
      "\n",
      "# 13 Gradient out:  [-1.00480024e+10 -8.82551464e+09 -4.70442128e+09 -7.90569025e+09\n",
      " -1.26580623e+10 -3.20228539e+09 -1.39078543e+10 -1.37851770e+10\n",
      " -1.48167813e+10 -8.01756142e+09]\n",
      "\n",
      "     Weights  out:  [ 0.06344919 -0.00187795 -0.26144234 -0.05084979  0.23101741 -0.44071259\n",
      "  0.35373613  0.3391126   0.49485115 -0.04481661] [3.22195510e+08 2.82995673e+08 1.50850225e+08 2.53500926e+08\n",
      " 4.05888722e+08 1.02683294e+08 4.45964087e+08 4.42030363e+08\n",
      " 4.75109404e+08 2.57088145e+08]\n",
      "\n",
      "# 14 Gradient out:  [5.26234802e+10 4.62210575e+10 2.46380336e+10 4.14037457e+10\n",
      " 6.62929074e+10 1.67710353e+10 7.28383284e+10 7.21958418e+10\n",
      " 7.75985684e+10 4.19896383e+10]\n",
      "\n",
      "     Weights  out:  [ 0.06344919 -0.00187795 -0.26144234 -0.05084979  0.23101741 -0.44071259\n",
      "  0.35373613  0.3391126   0.49485115 -0.04481661] [-1.68740496e+09 -1.48210726e+09 -7.90034031e+08 -1.32763712e+09\n",
      " -2.12572373e+09 -5.37773785e+08 -2.33560677e+09 -2.31500504e+09\n",
      " -2.48824686e+09 -1.34642414e+09]\n",
      "\n",
      "# 15 Gradient out:  [-2.75600122e+11 -2.42069301e+11 -1.29034511e+11 -2.16840036e+11\n",
      " -3.47189757e+11 -8.78334033e+10 -3.81469490e+11 -3.78104654e+11\n",
      " -4.06399858e+11 -2.19908478e+11]\n",
      "\n",
      "     Weights  out:  [ 0.06344919 -0.00187795 -0.26144234 -0.05084979  0.23101741 -0.44071259\n",
      "  0.35373613  0.3391126   0.49485115 -0.04481661] [8.83729108e+09 7.76210425e+09 4.13757270e+09 6.95311202e+09\n",
      " 1.11328577e+10 2.81643327e+09 1.22320589e+10 1.21241633e+10\n",
      " 1.30314668e+10 7.05150353e+09]\n",
      "\n",
      "# 16 Gradient out:  [1.44337521e+12 1.26776732e+12 6.75780600e+11 1.13563641e+12\n",
      " 1.81830504e+12 4.60001819e+11 1.99783513e+12 1.98021279e+12\n",
      " 2.12840066e+12 1.15170648e+12]\n",
      "\n",
      "     Weights  out:  [ 0.06344919 -0.00187795 -0.26144234 -0.05084979  0.23101741 -0.44071259\n",
      "  0.35373613  0.3391126   0.49485115 -0.04481661] [-4.62827332e+10 -4.06517559e+10 -2.16693296e+10 -3.64148952e+10\n",
      " -5.83050937e+10 -1.47502474e+10 -6.40618390e+10 -6.34967674e+10\n",
      " -6.82485048e+10 -3.69301920e+10]\n",
      "\n",
      "# 17 Gradient out:  [-7.55925648e+12 -6.63956141e+12 -3.53920369e+12 -5.94756432e+12\n",
      " -9.52284204e+12 -2.40912529e+12 -1.04630785e+13 -1.03707866e+13\n",
      " -1.11468774e+13 -6.03172660e+12]\n",
      "\n",
      "     Weights  out:  [ 0.06344919 -0.00187795 -0.26144234 -0.05084979  0.23101741 -0.44071259\n",
      "  0.35373613  0.3391126   0.49485115 -0.04481661] [2.42392309e+11 2.12901709e+11 1.13486790e+11 1.90712387e+11\n",
      " 3.05355914e+11 7.72501164e+10 3.35505188e+11 3.32545790e+11\n",
      " 3.57431628e+11 1.93411104e+11]\n",
      "\n",
      "# 18 Gradient out:  [3.95893999e+13 3.47727654e+13 1.85355465e+13 3.11486325e+13\n",
      " 4.98731063e+13 1.26170907e+13 5.47973205e+13 5.43139686e+13\n",
      " 5.83785175e+13 3.15894080e+13]\n",
      "\n",
      "     Weights  out:  [ 0.06344919 -0.00187795 -0.26144234 -0.05084979  0.23101741 -0.44071259\n",
      "  0.35373613  0.3391126   0.49485115 -0.04481661] [-1.26945899e+12 -1.11501057e+12 -5.94353948e+11 -9.98800476e+11\n",
      " -1.59921249e+12 -4.04574942e+11 -1.75711052e+12 -1.74161154e+12\n",
      " -1.87194385e+12 -1.01293422e+12]\n",
      "\n",
      "# 19 Gradient out:  [-2.07337929e+14 -1.82112212e+14 -9.70745161e+13 -1.63131873e+14\n",
      " -2.61195841e+14 -6.60783307e+13 -2.86984976e+14 -2.84453561e+14\n",
      " -3.05740450e+14 -1.65440306e+14]\n",
      "\n",
      "     Weights  out:  [ 0.06344919 -0.00187795 -0.26144234 -0.05084979  0.23101741 -0.44071259\n",
      "  0.35373613  0.3391126   0.49485115 -0.04481661] [6.64842100e+12 5.83954250e+12 3.11275536e+12 5.23092603e+12\n",
      " 8.37540877e+12 2.11884320e+12 9.20235359e+12 9.12118219e+12\n",
      " 9.80375965e+12 5.30494737e+12]\n",
      "\n",
      "# 20 Gradient out:  [1.08587190e+15 9.53759565e+14 5.08399451e+14 8.54355582e+14\n",
      " 1.36793700e+15 3.46065975e+14 1.50300006e+15 1.48974252e+15\n",
      " 1.60122639e+15 8.66445325e+14]\n",
      "\n",
      "     Weights  out:  [ 0.06344919 -0.00187795 -0.26144234 -0.05084979  0.23101741 -0.44071259\n",
      "  0.35373613  0.3391126   0.49485115 -0.04481661] [-3.48191649e+13 -3.05828998e+13 -1.63021479e+13 -2.73954486e+13\n",
      " -4.38637594e+13 -1.10968229e+13 -4.81946415e+13 -4.77695301e+13\n",
      " -5.13443303e+13 -2.77831138e+13]\n",
      "\n",
      "# 21 Gradient out:  [-5.68693721e+15 -4.99503740e+15 -2.66259376e+15 -4.47443805e+15\n",
      " -7.16417083e+15 -1.81241956e+15 -7.87152424e+15 -7.80209172e+15\n",
      " -8.38595594e+15 -4.53775455e+15]\n",
      "\n",
      "     Weights  out:  [ 0.06344919 -0.00187795 -0.26144234 -0.05084979  0.23101741 -0.44071259\n",
      "  0.35373613  0.3391126   0.49485115 -0.04481661] [1.82355215e+14 1.60169013e+14 8.53777423e+13 1.43475668e+14\n",
      " 2.29723640e+14 5.81163720e+13 2.52405371e+14 2.50178974e+14\n",
      " 2.68900947e+14 1.45505951e+14]\n",
      "\n",
      "# 22 Gradient out:  [2.97836741e+16 2.61600508e+16 1.39445579e+16 2.34335636e+16\n",
      " 3.75202541e+16 9.49201853e+15 4.12248112e+16 4.08611786e+16\n",
      " 4.39189971e+16 2.37651653e+16]\n",
      "\n",
      "     Weights  out:  [ 0.06344919 -0.00187795 -0.26144234 -0.05084979  0.23101741 -0.44071259\n",
      "  0.35373613  0.3391126   0.49485115 -0.04481661] [-9.55032227e+14 -8.38838467e+14 -4.47141011e+14 -7.51411942e+14\n",
      " -1.20311053e+15 -3.04367539e+14 -1.32189948e+15 -1.31023937e+15\n",
      " -1.40829024e+15 -7.62044959e+14]\n",
      "\n",
      "# 23 Gradient out:  [-1.55983302e+17 -1.37005632e+17 -7.30305526e+16 -1.22726451e+17\n",
      " -1.96501382e+17 -4.97116772e+16 -2.15902919e+17 -2.13998499e+17\n",
      " -2.30012931e+17 -1.24463118e+17]\n",
      "\n",
      "     Weights  out:  [ 0.06344919 -0.00187795 -0.26144234 -0.05084979  0.23101741 -0.44071259\n",
      "  0.35373613  0.3391126   0.49485115 -0.04481661] [5.00170260e+15 4.39317169e+15 2.34177056e+15 3.93530078e+15\n",
      " 6.30094030e+15 1.59403617e+15 6.92306276e+15 6.86199636e+15\n",
      " 7.37550918e+15 3.99098810e+15]\n",
      "\n",
      "# 24 Gradient out:  [8.16917026e+17 7.17527018e+17 3.82476208e+17 6.42743974e+17\n",
      " 1.02911865e+18 2.60350403e+17 1.13072853e+18 1.12075469e+18\n",
      " 1.20462561e+18 6.51839261e+17]\n",
      "\n",
      "     Weights  out:  [ 0.06344919 -0.00187795 -0.26144234 -0.05084979  0.23101741 -0.44071259\n",
      "  0.35373613  0.3391126   0.49485115 -0.04481661] [-2.61949578e+16 -2.30079547e+16 -1.22643400e+16 -2.06099894e+16\n",
      " -3.29993361e+16 -8.34829927e+15 -3.62575210e+16 -3.59377035e+16\n",
      " -3.86270771e+16 -2.09016355e+16]\n",
      "\n",
      "# 25 Gradient out:  [-4.27836454e+18 -3.75783838e+18 -2.00310753e+18 -3.36618401e+18\n",
      " -5.38970860e+18 -1.36350926e+18 -5.92186073e+18 -5.86962565e+18\n",
      " -6.30887509e+18 -3.41381792e+18]\n",
      "\n",
      "     Weights  out:  [ 0.06344919 -0.00187795 -0.26144234 -0.05084979  0.23101741 -0.44071259\n",
      "  0.35373613  0.3391126   0.49485115 -0.04481661] [1.37188447e+17 1.20497449e+17 6.42309017e+16 1.07938805e+17\n",
      " 1.72824393e+17 4.37217813e+16 1.89888186e+17 1.88213235e+17\n",
      " 2.02298044e+17 1.09466217e+17]\n",
      "\n",
      "# 26 Gradient out:  [2.24066858e+19 1.96805819e+19 1.04906911e+19 1.76294064e+19\n",
      " 2.82270260e+19 7.14098186e+18 3.10140175e+19 3.07404515e+19\n",
      " 3.30408923e+19 1.78788751e+19]\n",
      "\n",
      "     Weights  out:  [ 0.06344919 -0.00187795 -0.26144234 -0.05084979  0.23101741 -0.44071259\n",
      "  0.35373613  0.3391126   0.49485115 -0.04481661] [-7.18484460e+17 -6.31070226e+17 -3.36390604e+17 -5.65297996e+17\n",
      " -9.05117328e+17 -2.28980071e+17 -9.94483961e+17 -9.85711895e+17\n",
      " -1.05947697e+18 -5.73297367e+17]\n",
      "\n",
      "# 27 Gradient out:  [-1.17348478e+20 -1.03071305e+20 -5.49419336e+19 -9.23288717e+19\n",
      " -1.47830812e+20 -3.73988087e+19 -1.62426866e+20 -1.60994144e+20\n",
      " -1.73042031e+20 -9.36353913e+19]\n",
      "\n",
      "     Weights  out:  [ 0.06344919 -0.00187795 -0.26144234 -0.05084979  0.23101741 -0.44071259\n",
      "  0.35373613  0.3391126   0.49485115 -0.04481661] [3.76285270e+18 3.30504615e+18 1.76174763e+18 2.96058329e+18\n",
      " 4.74028788e+18 1.19921630e+18 5.20831954e+18 5.16237841e+18\n",
      " 5.54870148e+18 3.00247766e+18]\n",
      "\n",
      "# 28 Gradient out:  [6.14578411e+20 5.39805881e+20 2.87742344e+20 4.83545523e+20\n",
      " 7.74220738e+20 1.95865347e+20 8.50663315e+20 8.43159851e+20\n",
      " 9.06257144e+20 4.90388038e+20]\n",
      "\n",
      "     Weights  out:  [ 0.06344919 -0.00187795 -0.26144234 -0.05084979  0.23101741 -0.44071259\n",
      "  0.35373613  0.3391126   0.49485115 -0.04481661] [-1.97068430e+19 -1.73092148e+19 -9.22663909e+18 -1.55051911e+19\n",
      " -2.48258745e+19 -6.28054543e+18 -2.72770537e+19 -2.70364505e+19\n",
      " -2.90597048e+19 -1.57246006e+19]\n",
      "\n",
      "# 29 Gradient out:  [-3.21867509e+21 -2.82707578e+21 -1.50696656e+21 -2.53242857e+21\n",
      " -4.05475519e+21 -1.02578760e+21 -4.45510088e+21 -4.41580368e+21\n",
      " -4.74625733e+21 -2.56826425e+21]\n",
      "\n",
      "     Weights  out:  [ 0.06344919 -0.00187795 -0.26144234 -0.05084979  0.23101741 -0.44071259\n",
      "  0.35373613  0.3391126   0.49485115 -0.04481661] [1.03208839e+20 9.06519614e+19 4.83218298e+19 8.12039135e+19\n",
      " 1.30018273e+20 3.28925240e+19 1.42855609e+20 1.41595520e+20\n",
      " 1.52191724e+20 8.23530070e+19]\n",
      "\n",
      "# 30 Gradient out:  [1.68568715e+22 1.48059844e+22 7.89229761e+21 1.32628556e+22\n",
      " 2.12355971e+22 5.37226321e+21 2.33322908e+22 2.31264832e+22\n",
      " 2.48571378e+22 1.34505345e+22]\n",
      "\n",
      "     Weights  out:  [ 0.06344919 -0.00187795 -0.26144234 -0.05084979  0.23101741 -0.44071259\n",
      "  0.35373613  0.3391126   0.49485115 -0.04481661] [-5.40526178e+20 -4.74763194e+20 -2.53071482e+20 -4.25281801e+20\n",
      " -6.80932764e+20 -1.72264996e+20 -7.48164566e+20 -7.41565215e+20\n",
      " -7.97059743e+20 -4.31299842e+20]\n",
      "\n",
      "# 31 Gradient out:  [-8.82829455e+22 -7.75420228e+22 -4.13336057e+22 -6.94603362e+22\n",
      " -1.11215243e+23 -2.81356609e+22 -1.22196064e+23 -1.21118207e+23\n",
      " -1.30182005e+23 -7.04432496e+22]\n",
      "\n",
      "     Weights  out:  [ 0.06344919 -0.00187795 -0.26144234 -0.05084979  0.23101741 -0.44071259\n",
      "  0.35373613  0.3391126   0.49485115 -0.04481661] [2.83084813e+21 2.48643369e+21 1.32538804e+21 2.22728933e+21\n",
      " 3.56618665e+21 9.02187646e+20 3.91829359e+21 3.88373142e+21\n",
      " 4.17436781e+21 2.25880707e+21]\n",
      "\n",
      "# 32 Gradient out:  [4.62356165e+23 4.06103717e+23 2.16472698e+23 3.63778241e+23\n",
      " 5.82457380e+23 1.47352313e+23 6.39966229e+23 6.34321265e+23\n",
      " 6.81790265e+23 3.68925964e+23]\n",
      "\n",
      "     Weights  out:  [ 0.06344919 -0.00187795 -0.26144234 -0.05084979  0.23101741 -0.44071259\n",
      "  0.35373613  0.3391126   0.49485115 -0.04481661] [-1.48257410e+22 -1.30219709e+22 -6.94133309e+21 -1.16647779e+22\n",
      " -1.86768619e+22 -4.72494453e+21 -2.05209193e+22 -2.03399100e+22\n",
      " -2.18620333e+22 -1.18298429e+22]\n",
      "\n",
      "# 33 Gradient out:  [-2.42145549e+24 -2.12684971e+24 -1.13371259e+24 -1.90518238e+24\n",
      " -3.05045056e+24 -7.71714743e+23 -3.35163637e+24 -3.32207252e+24\n",
      " -3.57067756e+24 -1.93214208e+24]\n",
      "\n",
      "     Weights  out:  [ 0.06344919 -0.00187795 -0.26144234 -0.05084979  0.23101741 -0.44071259\n",
      "  0.35373613  0.3391126   0.49485115 -0.04481661] [7.76454920e+22 6.81987724e+22 3.63532065e+22 6.10908703e+22\n",
      " 9.78146141e+22 2.47455182e+22 1.07472326e+23 1.06524343e+23\n",
      " 1.14496020e+23 6.19553498e+22]\n",
      "\n",
      "# 34 Gradient out:  [1.26816665e+25 1.11387547e+25 5.93748884e+24 9.97783678e+24\n",
      " 1.59758446e+25 4.04163077e+24 1.75532174e+25 1.73983854e+25\n",
      " 1.87003817e+25 1.01190303e+25]\n",
      "\n",
      "     Weights  out:  [ 0.06344919 -0.00187795 -0.26144234 -0.05084979  0.23101741 -0.44071259\n",
      "  0.35373613  0.3391126   0.49485115 -0.04481661] [-4.06645607e+23 -3.57171169e+23 -1.90389311e+23 -3.19945606e+23\n",
      " -5.12275498e+23 -1.29597430e+23 -5.62854948e+23 -5.57890161e+23\n",
      " -5.99639491e+23 -3.24473065e+23]\n",
      "\n",
      "# 35 Gradient out:  [-6.64165278e+25 -5.83359774e+25 -3.10958652e+25 -5.22560085e+25\n",
      " -8.36688244e+25 -2.11668617e+25 -9.19298541e+25 -9.11189665e+25\n",
      " -9.79377923e+25 -5.29954683e+25]\n",
      "\n",
      "     Weights  out:  [ 0.06344919 -0.00187795 -0.26144234 -0.05084979  0.23101741 -0.44071259\n",
      "  0.35373613  0.3391126   0.49485115 -0.04481661] [2.12968770e+24 1.87057977e+24 9.97108457e+23 1.67562175e+24\n",
      " 2.68289343e+24 6.78728723e+23 2.94778854e+24 2.92178692e+24\n",
      " 3.14043685e+24 1.69933299e+24]\n",
      "\n",
      "# 36 Gradient out:  [3.47837183e+26 3.05517658e+26 1.62855520e+26 2.73675595e+26\n",
      " 4.38191052e+26 1.10855262e+26 4.81455785e+26 4.77208998e+26\n",
      " 5.12920608e+26 2.77548301e+26]\n",
      "\n",
      "     Weights  out:  [ 0.06344919 -0.00187795 -0.26144234 -0.05084979  0.23101741 -0.44071259\n",
      "  0.35373613  0.3391126   0.49485115 -0.04481661] [-1.11536179e+25 -9.79661570e+24 -5.22206458e+24 -8.77557994e+24\n",
      " -1.40508714e+25 -3.55464363e+24 -1.54381823e+25 -1.53020064e+25\n",
      " -1.64471216e+25 -8.89976067e+24]\n",
      "\n",
      "# 37 Gradient out:  [-1.82169574e+27 -1.60005958e+27 -8.52908261e+26 -1.43329607e+27\n",
      " -2.29489777e+27 -5.80572084e+26 -2.52148418e+27 -2.49924287e+27\n",
      " -2.68627201e+27 -1.45357823e+27]\n",
      "\n",
      "     Weights  out:  [ 0.06344919 -0.00187795 -0.26144234 -0.05084979  0.23101741 -0.44071259\n",
      "  0.35373613  0.3391126   0.49485115 -0.04481661] [5.84138188e+25 5.13069159e+25 2.73490393e+25 4.59595391e+25\n",
      " 7.35873390e+25 1.86164087e+25 8.08529746e+25 8.01397931e+25\n",
      " 8.61369999e+25 4.66098995e+25]\n",
      "\n",
      "# 38 Gradient out:  [9.54059986e+27 8.37984514e+27 4.46685813e+27 7.50646990e+27\n",
      " 1.20188574e+28 3.04057687e+27 1.32055376e+28 1.30890552e+28\n",
      " 1.40685657e+28 7.61269182e+27]\n",
      "\n",
      "     Weights  out:  [ 0.06344919 -0.00187795 -0.26144234 -0.05084979  0.23101741 -0.44071259\n",
      "  0.35373613  0.3391126   0.49485115 -0.04481661] [-3.05925330e+26 -2.68705000e+26 -1.43232613e+26 -2.40699675e+26\n",
      " -3.85392214e+26 -9.74980081e+25 -4.23443861e+26 -4.19708781e+26\n",
      " -4.51117401e+26 -2.44105747e+26]\n",
      "\n",
      "# 39 Gradient out:  [-4.99661077e+28 -4.38869936e+28 -2.33938659e+28 -3.93129456e+28\n",
      " -6.29452582e+28 -1.59241341e+28 -6.91601495e+28 -6.85501071e+28\n",
      " -7.36800077e+28 -3.98692519e+28]\n",
      "\n",
      "     Weights  out:  [ 0.06344919 -0.00187795 -0.26144234 -0.05084979  0.23101741 -0.44071259\n",
      "  0.35373613  0.3391126   0.49485115 -0.04481661] [1.60219464e+27 1.40726403e+27 7.50139012e+26 1.26059431e+27\n",
      " 2.01837926e+27 5.10617366e+26 2.21766365e+27 2.19810226e+27\n",
      " 2.36259575e+27 1.27843262e+27]\n",
      "\n",
      "# 40 Gradient out:  [2.61682908e+29 2.29845322e+29 1.22518546e+29 2.05890081e+29\n",
      " 3.29657422e+29 8.33980054e+28 3.62206101e+29 3.59011182e+29\n",
      " 3.85877539e+29 2.08803573e+29]\n",
      "\n",
      "     Weights  out:  [ 0.06344919 -0.00187795 -0.26144234 -0.05084979  0.23101741 -0.44071259\n",
      "  0.35373613  0.3391126   0.49485115 -0.04481661] [-8.39102690e+27 -7.37013468e+27 -3.92863418e+27 -6.60199482e+27\n",
      " -1.05706724e+28 -2.67420945e+27 -1.16143662e+28 -1.15119192e+28\n",
      " -1.23734058e+28 -6.69541777e+27]\n",
      "\n",
      "# 41 Gradient out:  [-1.37048787e+30 -1.20374780e+30 -6.41655134e+29 -1.07828922e+30\n",
      " -1.72648455e+30 -4.36772717e+29 -1.89694876e+30 -1.88021630e+30\n",
      " -2.02092101e+30 -1.09354778e+30]\n",
      "\n",
      "     Weights  out:  [ 0.06344919 -0.00187795 -0.26144234 -0.05084979  0.23101741 -0.44071259\n",
      "  0.35373613  0.3391126   0.49485115 -0.04481661] [4.39455548e+28 3.85989297e+28 2.05750751e+28 3.45760213e+28\n",
      " 5.53608119e+28 1.40053916e+28 6.08268540e+28 6.02903173e+28\n",
      " 6.48021021e+28 3.50652968e+28]\n",
      "\n",
      "# 42 Gradient out:  [7.17753029e+30 6.30427784e+30 3.36048152e+30 5.64722511e+30\n",
      " 9.04195901e+30 2.28746965e+30 9.93471557e+30 9.84708422e+30\n",
      " 1.05839841e+31 5.72713739e+30]\n",
      "\n",
      "     Weights  out:  [ 0.06344919 -0.00187795 -0.26144234 -0.05084979  0.23101741 -0.44071259\n",
      "  0.35373613  0.3391126   0.49485115 -0.04481661] [-2.30152019e+29 -2.02150631e+29 -1.07755952e+29 -1.81081822e+29\n",
      " -2.89936098e+29 -7.33491517e+28 -3.18562898e+29 -3.15752943e+29\n",
      " -3.39382100e+29 -1.83644259e+29]\n",
      "\n",
      "# 43 Gradient out:  [-3.75902204e+31 -3.30168155e+31 -1.75995413e+31 -2.95756936e+31\n",
      " -4.73546217e+31 -1.19799548e+31 -5.20301737e+31 -5.15712301e+31\n",
      " -5.54305279e+31 -2.99942108e+31]\n",
      "\n",
      "     Weights  out:  [ 0.06344919 -0.00187795 -0.26144234 -0.05084979  0.23101741 -0.44071259\n",
      "  0.35373613  0.3391126   0.49485115 -0.04481661] [1.20535404e+30 1.05870494e+30 5.64340352e+29 9.48363201e+29\n",
      " 1.51845570e+30 3.84144778e+29 1.66838022e+30 1.65366390e+30\n",
      " 1.77741471e+30 9.61783218e+29]\n",
      "\n",
      "# 44 Gradient out:  [1.96867810e+32 1.72915937e+32 9.21724619e+31 1.54894065e+32\n",
      " 2.48006013e+32 6.27415172e+31 2.72492852e+32 2.70089268e+32\n",
      " 2.90301215e+32 1.57085926e+32]\n",
      "\n",
      "     Weights  out:  [ 0.06344919 -0.00187795 -0.26144234 -0.05084979  0.23101741 -0.44071259\n",
      "  0.35373613  0.3391126   0.49485115 -0.04481661] [-6.31269004e+30 -5.54465817e+30 -2.95556791e+30 -4.96677552e+30\n",
      " -7.95246864e+30 -2.01184618e+30 -8.73765453e+30 -8.66058212e+30\n",
      " -9.30869088e+30 -5.03705894e+30]\n",
      "\n",
      "# 45 Gradient out:  [-1.03103771e+33 -9.05596759e+32 -4.82726372e+32 -8.11212464e+32\n",
      " -1.29885912e+33 -3.28590387e+32 -1.42710179e+33 -1.41451373e+33\n",
      " -1.52036790e+33 -8.22691700e+32]\n",
      "\n",
      "     Weights  out:  [ 0.06344919 -0.00187795 -0.26144234 -0.05084979  0.23101741 -0.44071259\n",
      "  0.35373613  0.3391126   0.49485115 -0.04481661] [3.30608720e+31 2.90385292e+31 1.54789245e+31 2.60120374e+31\n",
      " 4.16487339e+31 1.05364573e+31 4.57609158e+31 4.53572716e+31\n",
      " 4.87515521e+31 2.63801263e+31]\n",
      "\n",
      "# 46 Gradient out:  [5.39975912e+33 4.74279876e+33 2.52813851e+33 4.24848856e+33\n",
      " 6.80239562e+33 1.72089627e+33 7.47402920e+33 7.40810288e+33\n",
      " 7.96248321e+33 4.30860771e+33]\n",
      "\n",
      "     Weights  out:  [ 0.06344919 -0.00187795 -0.26144234 -0.05084979  0.23101741 -0.44071259\n",
      "  0.35373613  0.3391126   0.49485115 -0.04481661] [-1.73146669e+32 -1.52080823e+32 -8.10663500e+31 -1.36230455e+32\n",
      " -2.18123090e+32 -5.51816202e+31 -2.39659443e+32 -2.37545474e+32\n",
      " -2.55322028e+32 -1.38158214e+32]\n",
      "\n",
      "# 47 Gradient out:  [-2.82796627e+34 -2.48390245e+34 -1.32403877e+34 -2.22502190e+34\n",
      " -3.56255620e+34 -9.01269202e+33 -3.91430469e+34 -3.87977770e+34\n",
      " -4.17011822e+34 -2.25650756e+34]\n",
      "\n",
      "     Weights  out:  [ 0.06344919 -0.00187795 -0.26144234 -0.05084979  0.23101741 -0.44071259\n",
      "  0.35373613  0.3391126   0.49485115 -0.04481661] [9.06805155e+32 7.96478930e+32 4.24561352e+32 7.13467256e+32\n",
      " 1.14235603e+33 2.88997633e+32 1.25514640e+33 1.24407510e+33\n",
      " 1.33717461e+33 7.23563328e+32]\n",
      "\n",
      "# 48 Gradient out:  [1.48106481e+35 1.30087142e+35 6.93426668e+34 1.16529029e+35\n",
      " 1.86578485e+35 4.72013444e+34 2.05000286e+35 2.03192036e+35\n",
      " 2.18397773e+35 1.18177998e+35]\n",
      "\n",
      "     Weights  out:  [ 0.06344919 -0.00187795 -0.26144234 -0.05084979  0.23101741 -0.44071259\n",
      "  0.35373613  0.3391126   0.49485115 -0.04481661] [-4.74912738e+33 -4.17132597e+33 -2.22351619e+33 -3.73657655e+33\n",
      " -5.98275637e+33 -1.51354077e+33 -6.57346299e+33 -6.51548031e+33\n",
      " -7.00306183e+33 -3.78945178e+33]\n",
      "\n",
      "# 49 Gradient out:  [-7.75664474e+35 -6.81293448e+35 -3.63161982e+35 -6.10286787e+35\n",
      " -9.77150370e+35 -2.47203268e+35 -1.07362918e+36 -1.06415899e+36\n",
      " -1.14379460e+36 -6.18922781e+35]\n",
      "\n",
      "     Weights  out:  [ 0.06344919 -0.00187795 -0.26144234 -0.05084979  0.23101741 -0.44071259\n",
      "  0.35373613  0.3391126   0.49485115 -0.04481661] [2.48721688e+34 2.18461025e+34 1.16450172e+34 1.95692293e+34\n",
      " 3.13329406e+34 7.92672812e+33 3.44265942e+34 3.41229268e+34\n",
      " 3.66764928e+34 1.98461479e+34]\n"
     ]
    }
   ],
   "source": [
    "train(X, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6KIURxgz_e8v"
   },
   "source": [
    "## Ejercicios\n",
    "### Ejercicio A  (3 puntos)\n",
    "1. Obtenga, a partir del código presentado para el entrenamiento de la PMC-BP, el algoritmo de recuerdo de la red.\n",
    "2. Aplique ahora dicho algoritmo de recuerdo en la etapa denominada *Validación* para el mismo problema de regresión. Determine el error cuadrático de la red en cada época y grafique sus resultados.\n",
    "\n",
    "### Ejercicio B  (5 puntos)\n",
    "1. Use la función tangente hiperbólica en lugar de la lineal en la capa de salida para abordar el mismo problema. Con ese objetivo defina la(s) función(nes) que se requieran e insértelas en el código de modo que la red funcione correctamente. Mantenga inalterada la arquitectura de la red.\n",
    "2. A partir de las modificaciones pedidas, entrene nuevamente la red al mismo problema de regresión. Enseguida aplique el algoritmo de recuerdo.\n",
    "3. Compare los resultados que obtenga con los obtenidos con la red PMC-BP que usaba una función lineal en la salida.\n",
    "\n",
    "### Ejercicio C (4 puntos)\n",
    "1. Modifique la arquitectura de la red PMC usando ahora 4, 6, 8 y 12 neuronas en la capa oculta. Aplique las etapas de entrenamiento/validación de la red.\n",
    "2. Determine el mejor modelo, indicando específicamente con cuántas neuronas ocultas obtuvo la mejor respuesta de la red.\n",
    "\n",
    "### Ejercicio D (8 puntos)\n",
    "1. Modifique la arquitectura de la red PMC usando ahora **dos capas ocultas**, inicialmente con 10 neuronas en la primera capa oculta y 8 neuronas en la segunda capa oculta. Use diferentes números de neuronas en las capas ocultas. Aplique las etapas de entrenamiento/validación de la red para cada combinación.\n",
    "2. Determine con qué combinación de neuronas en las capas ocultas obtuvo el mejor modelo, el que produjo la mejor respuesta de la red.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lIjpy3jH_e8v"
   },
   "source": [
    "## Instrucciones para el envío de la solución\n",
    "La solución de la \"Práctica de Laboratorio 8 IA 2025-1 EPISW\" deberá enviarse al correo electrónico rmaguinacursos@gmail.com, hasta las 23:59 h del Domingo 08 de Junio del 2025 en un cuaderno computacional interactivo (archivo con extensión .ipynb).\n",
    "\n",
    "El documento deberá tener las sgtes características:\n",
    "- Nombre del archivo: solPGL8_IA_2025-1_EPISW_nombre-apellidos_integrantes.ipynb.\n",
    "- Todas las preguntas de la Práctica deben responderse en el mismo cci (**Sugerencia**: obtener una copia de este documento y desarrollar en ellas las respectivas soluciones); la solución a cada pregunta debe registrarse en una celda debajo del planteamiento de la misma, mencionando explícitamente como subtítulo: \\\"Solución del ejercicio n\\\", donde \\\"n\\\" corresponde al número del ejercicio.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solución del ejercicio A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Obtenga, a partir del código presentado para el entrenamiento de la PMC-BP, el algoritmo de recuerdo de la red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función auxiliar necesaria\n",
    "def logistica(u):\n",
    "    return 1/(1 + np.exp(-u))\n",
    "\n",
    "# Extraído de la fase de propagación hacia adelante del código de entrenamiento.\n",
    "def recall(x, w1, w2):\n",
    "    # Capa oculta\n",
    "    u1 = x * w1        # Entrada neta de las neuronas ocultas\n",
    "    o = logistica(u1)  # (función sigmoidea)\n",
    "    \n",
    "    # Capa de salida\n",
    "    u2 = o.dot(w2)     # Entrada neta de la neurona de salida\n",
    "    y = u2             # Salida final (función lineal)\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Aplique ahora dicho algoritmo de recuerdo en la etapa denominada Validación para el mismo problema de regresión. Determine el error cuadrático de la red en cada época y grafique sus resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCIÓN DE ENTRENAMIENTO MODIFICADA PARA INCLUIR VALIDACIÓN\n",
    "\n",
    "def train_with_validation(X, t, learning_rate=0.2, epochs=50):\n",
    "    # Variables globales\n",
    "    input_num = 1\n",
    "    hidden_num = 10\n",
    "    output_num = 1\n",
    "    intervalo = 0.5\n",
    "\n",
    "    # Inicializando los pesos\n",
    "    w1 = np.random.uniform(-intervalo, intervalo, hidden_num)\n",
    "    w2 = np.random.uniform(-intervalo, intervalo, hidden_num)\n",
    "    \n",
    "    validation_errors = []\n",
    "\n",
    "    print(\"APLICACIÓN DEL ALGORITMO DE RECUERDO EN VALIDACIÓN\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Época\\t\\tError Cuadrático de Validación\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # FASE DE ENTRENAMIENTO\n",
    "        gradient_out = 0.0\n",
    "        gradient_hidden = np.zeros(hidden_num)\n",
    "\n",
    "        for i in range(X.shape[0]):\n",
    "            # Propagación hacia adelante\n",
    "            x = X[i]\n",
    "            u1 = x * w1\n",
    "            o = logistica(u1)\n",
    "            u2 = o.dot(w2)\n",
    "            y = u2\n",
    "\n",
    "            # Backpropagation\n",
    "            delta_hidden_s = []\n",
    "            gradient_hidden_s = []\n",
    "\n",
    "            delta_out_s = t[i] - y\n",
    "            gradient_out_s = delta_out_s * o\n",
    "\n",
    "            for j in range(hidden_num):\n",
    "                delta_hidden_s.append(deriv_logistica(o[j]) * w2[j] * delta_out_s)\n",
    "                gradient_hidden_s.append(delta_hidden_s[j] * x)\n",
    "\n",
    "            gradient_out = gradient_out + gradient_out_s\n",
    "            gradient_hidden = gradient_hidden + np.array(gradient_hidden_s)\n",
    "\n",
    "        # Actualizar pesos\n",
    "        w2 = w2 + learning_rate * gradient_out\n",
    "        for j in range(hidden_num):\n",
    "            w1[j] = w1[j] + learning_rate * gradient_hidden[j]\n",
    "\n",
    "        # FASE DE VALIDACIÓN - APLICACIÓN DEL ALGORITMO DE RECUERDO\n",
    "        \n",
    "        total_squared_error = 0.0\n",
    "        \n",
    "        for i in range(X.shape[0]):\n",
    "            # Aplicar algoritmo de recuerdo para obtener predicción\n",
    "            y_pred = recall(X[i], w1, w2)\n",
    "            \n",
    "            # Calcular error cuadrático para esta muestra\n",
    "            squared_error = (t[i] - y_pred)**2\n",
    "            total_squared_error += squared_error\n",
    "        \n",
    "        # Error cuadrático medio de la época\n",
    "        mse_validation = total_squared_error / X.shape[0]\n",
    "        validation_errors.append(mse_validation)\n",
    "        \n",
    "        # Mostrar progreso\n",
    "        print(f\"{epoch}\\t\\t{mse_validation:.8f}\")\n",
    "\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Error final: {validation_errors[-1]:.8f}\")\n",
    "    \n",
    "    return validation_errors, w1, w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EJECUTAMOS ENTRENAMIENTO CON VALIDACIÓN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APLICACIÓN DEL ALGORITMO DE RECUERDO EN VALIDACIÓN\n",
      "============================================================\n",
      "Época\t\tError Cuadrático de Validación\n",
      "----------------------------------------\n",
      "0\t\t0.80122800\n",
      "1\t\t8.37253797\n",
      "2\t\t11.96626342\n",
      "3\t\t2.21781741\n",
      "4\t\t0.96777035\n",
      "5\t\t0.71226277\n",
      "6\t\t0.63499375\n",
      "7\t\t0.59467339\n",
      "8\t\t0.56555684\n",
      "9\t\t0.54172027\n",
      "10\t\t0.52119416\n",
      "11\t\t0.50311050\n",
      "12\t\t0.48708814\n",
      "13\t\t0.47304072\n",
      "14\t\t0.46104325\n",
      "15\t\t0.45116731\n",
      "16\t\t0.44331184\n",
      "17\t\t0.43713949\n",
      "18\t\t0.43218185\n",
      "19\t\t0.42801452\n",
      "20\t\t0.42435303\n",
      "21\t\t0.42104344\n",
      "22\t\t0.41801269\n",
      "23\t\t0.41522754\n",
      "24\t\t0.41267149\n",
      "25\t\t0.41033387\n",
      "26\t\t0.40820479\n",
      "27\t\t0.40627297\n",
      "28\t\t0.40452500\n",
      "29\t\t0.40294531\n",
      "30\t\t0.40151679\n",
      "31\t\t0.40022167\n",
      "32\t\t0.39904240\n",
      "33\t\t0.39796249\n",
      "34\t\t0.39696709\n",
      "35\t\t0.39604330\n",
      "36\t\t0.39518024\n",
      "37\t\t0.39436895\n",
      "38\t\t0.39360217\n",
      "39\t\t0.39287410\n",
      "40\t\t0.39218009\n",
      "41\t\t0.39151645\n",
      "42\t\t0.39088023\n",
      "43\t\t0.39026903\n",
      "44\t\t0.38968088\n",
      "45\t\t0.38911415\n",
      "46\t\t0.38856746\n",
      "47\t\t0.38803962\n",
      "48\t\t0.38752960\n",
      "49\t\t0.38703647\n",
      "----------------------------------------\n",
      "Error final: 0.38703647\n"
     ]
    }
   ],
   "source": [
    "validation_errors, final_w1, final_w2 = train_with_validation(X, t, learning_rate=0.2, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GRAFICAMOS LOS RESULTADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x500 with 0 Axes>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Gráfico 1: Evolución del error cuadrático en validación\n",
    "plt.subplot(1, 2, 1)\n",
    "epochs_range = range(len(validation_errors))\n",
    "plt.plot(epochs_range, validation_errors, 'r-', linewidth=2, marker='o', markersize=4)\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Error Cuadrático de Validación')\n",
    "plt.title('Error Cuadrático de Validación por Época\\n(usando Algoritmo de Recuerdo)')\n",
    "plt.grid(True, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error cuadrático final de validación: 0.38703647\n"
     ]
    }
   ],
   "source": [
    "# Gráfico 2: Predicciones finales vs valores reales\n",
    "plt.subplot(1, 2, 2)\n",
    "y_predictions = []\n",
    "for i in range(X.shape[0]):\n",
    "    y_pred = recall(X[i], final_w1, final_w2)\n",
    "    y_predictions.append(y_pred)\n",
    "\n",
    "plt.plot(X, t, 'bo-', label='Valores Reales', markersize=8, linewidth=2)\n",
    "plt.plot(X, y_predictions, 'r^-', label='Predicciones (Algoritmo de Recuerdo)', markersize=8, linewidth=2)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.title('Comparación Final: Valores Reales vs Predicciones')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nError cuadrático final de validación: {validation_errors[-1]:.8f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solución del ejercicio B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algoritmo de entramiento con tanh como función de activación de salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deriv_tanh(u):\n",
    "    return 1 - np.tanh(u) ** 2\n",
    "\n",
    "def train_tanh(X, t, learning_rate=0.2, epochs=50):\n",
    "    global input_num\n",
    "    global hidden_num\n",
    "    global w1\n",
    "    global w2\n",
    "\n",
    "    input_num = 1\n",
    "    hidden_num = 10\n",
    "    output_num = 1\n",
    "    intervalo = 0.5\n",
    "\n",
    "    # inicializando los pesos\n",
    "    w1 = np.random.uniform(-intervalo, intervalo, hidden_num)\n",
    "\n",
    "    w2 = np.random.uniform(-intervalo, intervalo, hidden_num)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        gradient_out = 0.0                 \n",
    "        gradient_hidden = []\n",
    "\n",
    "        for i in range(X.shape[0]):\n",
    "        # propagacion hacia adelante\n",
    "            x = X[i]\n",
    "\n",
    "            u1 = x * w1\n",
    "            o = logistica(u1)\n",
    "            u2 = o.dot(w2)\n",
    "            y = np.tanh(u2)  #Función de activación tanh para la salida\n",
    "\n",
    "        # backpropagation\n",
    "            delta_hidden_s = []           \n",
    "            gradient_hidden_s = []       \n",
    "\n",
    "            delta_out_s = (t[i] - y)* deriv_tanh(u2)     \n",
    "            gradient_out_s = delta_out_s * o     \n",
    "\n",
    "            for j in range(hidden_num):\n",
    "\n",
    "                delta_hidden_s.append(deriv_logistica(u1[j]) * w2[j] * delta_out_s)\n",
    "                gradient_hidden_s.append(delta_hidden_s[j] * x)\n",
    "\n",
    "\n",
    "            gradient_out = gradient_out + gradient_out_s\n",
    "            gradient_hidden = gradient_hidden + gradient_hidden_s\n",
    "\n",
    "\n",
    "        print(\"\\n#\", epoch, \"Gradient out: \",gradient_out),\n",
    "        print(\"\\n     Weights  out: \", w1, w2)\n",
    "\n",
    "        # Ahora actualizando pesos\n",
    "        w2 = w2 + learning_rate * gradient_out\n",
    "\n",
    "        for j in range(hidden_num):\n",
    "            w1[j] = w1[j] + learning_rate * gradient_hidden[j]\n",
    "\n",
    "def recall_tanh(X, t):\n",
    "    predictions = []\n",
    "    for i in range(X.shape[0]):\n",
    "        x = X[i]\n",
    "        u1 = x * w1\n",
    "        o = logistica(u1)\n",
    "        u2 = o.dot(w2)\n",
    "        y = np.tanh(u2)  # Activación en salida\n",
    "        predictions.append(y)\n",
    "\n",
    "    predictions = np.array(predictions)\n",
    "    mse = np.mean((predictions - t)**2)\n",
    "    print(\"\\nTanh error cuadrático medio (ECM):\", mse)\n",
    "\n",
    "    return predictions, mse\n",
    "\n",
    "def recall(X, t):\n",
    "    predictions = []\n",
    "    for i in range(X.shape[0]):\n",
    "        x = X[i]\n",
    "        u1 = x * w1\n",
    "        o = logistica(u1)\n",
    "        u2 = o.dot(w2)\n",
    "        y = u2  # salida lineal\n",
    "        predictions.append(y)\n",
    "\n",
    "    predictions = np.array(predictions)\n",
    "    mse = np.mean((predictions - t)**2)\n",
    "    print(\"\\nLineal error cuadrático medio (ECM):\", mse)\n",
    "\n",
    "    return predictions, mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hallando por cada algoritmo el promedio de sus MSE de 10 entrenamientos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# 0 Gradient out:  [-0.30212572 -1.58649243 -1.33859348 -0.81874667 -0.19067526 -1.71732829\n",
      " -1.14505589 -0.17059909 -0.90466491 -1.50675992]\n",
      "\n",
      "     Weights  out:  [-0.35903762  0.32954707  0.18081846 -0.06791612 -0.4647426   0.44286497\n",
      "  0.08508216 -0.48938243 -0.02758821  0.2760401 ] [ 0.34889113  0.38083148  0.04620543  0.47276791 -0.34887285 -0.01869391\n",
      " -0.22296936 -0.16979309  0.16439634  0.06628098]\n",
      "\n",
      "# 1 Gradient out:  [1.10587404 3.15440942 2.81212633 1.88418706 0.96370972 3.31982755\n",
      " 2.49042089 0.93639162 2.04326017 3.05130954]\n",
      "\n",
      "     Weights  out:  [-0.35903762  0.32954707  0.18081846 -0.06791612 -0.4647426   0.44286497\n",
      "  0.08508216 -0.48938243 -0.02758821  0.2760401 ] [ 0.28846599  0.06353299 -0.22151326  0.30901858 -0.3870079  -0.36215957\n",
      " -0.45198054 -0.20391291 -0.01653664 -0.235071  ]\n",
      "\n",
      "# 2 Gradient out:  [-0.16040196 -0.48086076 -0.42190965 -0.28680935 -0.13411334 -0.51151357\n",
      " -0.37296714 -0.12921825 -0.30946312 -0.46223415]\n",
      "\n",
      "     Weights  out:  [-0.35903762  0.32954707  0.18081846 -0.06791612 -0.4647426   0.44286497\n",
      "  0.08508216 -0.48938243 -0.02758821  0.2760401 ] [ 0.5096408   0.69441487  0.340912    0.68585599 -0.19426595  0.30180595\n",
      "  0.04610364 -0.01663458  0.39211539  0.37519091]\n",
      "\n",
      "# 3 Gradient out:  [-0.28583354 -0.93422593 -0.81503666 -0.54151794 -0.23308015 -0.99584585\n",
      " -0.71593881 -0.2233117  -0.58737968 -0.89661974]\n",
      "\n",
      "     Weights  out:  [-0.35903762  0.32954707  0.18081846 -0.06791612 -0.4647426   0.44286497\n",
      "  0.08508216 -0.48938243 -0.02758821  0.2760401 ] [ 0.4775604   0.59824272  0.25653007  0.62849412 -0.22108862  0.19950323\n",
      " -0.02848979 -0.04247823  0.33022277  0.28274408]\n",
      "\n",
      "# 4 Gradient out:  [-0.47178081 -1.91358611 -1.64221293 -1.04578907 -0.35069899 -2.05535067\n",
      " -1.42333315 -0.32856673 -1.1451237  -1.82714877]\n",
      "\n",
      "     Weights  out:  [-0.35903762  0.32954707  0.18081846 -0.06791612 -0.4647426   0.44286497\n",
      "  0.08508216 -0.48938243 -0.02758821  0.2760401 ] [ 4.20393696e-01  4.11397536e-01  9.35227434e-02  5.20190536e-01\n",
      " -2.67704652e-01  3.34061088e-04 -1.71677554e-01 -8.71405723e-02\n",
      "  2.12746835e-01  1.03420130e-01]\n",
      "\n",
      "# 5 Gradient out:  [1.08699669 3.00506044 2.68111715 1.8187436  0.94938666 3.1646436\n",
      " 2.38101682 0.92274732 1.9663047  2.90672953]\n",
      "\n",
      "     Weights  out:  [-0.35903762  0.32954707  0.18081846 -0.06791612 -0.4647426   0.44286497\n",
      "  0.08508216 -0.48938243 -0.02758821  0.2760401 ] [ 0.32603753  0.02868031 -0.23491984  0.31103272 -0.33784445 -0.41073607\n",
      " -0.45634418 -0.15285392 -0.0162779  -0.26200962]\n",
      "\n",
      "# 6 Gradient out:  [-0.20557882 -0.66943281 -0.58423634 -0.38843343 -0.16782725 -0.71350881\n",
      " -0.51333237 -0.16082434 -0.42127312 -0.64255555]\n",
      "\n",
      "     Weights  out:  [-0.35903762  0.32954707  0.18081846 -0.06791612 -0.4647426   0.44286497\n",
      "  0.08508216 -0.48938243 -0.02758821  0.2760401 ] [ 0.54343687  0.6296924   0.30130359  0.67478144 -0.14796712  0.22219265\n",
      "  0.01985918  0.03169555  0.37698304  0.31933628]\n",
      "\n",
      "# 7 Gradient out:  [-0.41384511 -1.49850783 -1.29836874 -0.84221441 -0.32544732 -1.60188846\n",
      " -1.13272573 -0.30915555 -0.91860964 -1.43529902]\n",
      "\n",
      "     Weights  out:  [-0.35903762  0.32954707  0.18081846 -0.06791612 -0.4647426   0.44286497\n",
      "  0.08508216 -0.48938243 -0.02758821  0.2760401 ] [ 5.02321108e-01  4.95805840e-01  1.84456317e-01  5.97094757e-01\n",
      " -1.81532567e-01  7.94908856e-02 -8.28072938e-02 -4.69321545e-04\n",
      "  2.92728412e-01  1.90825172e-01]\n",
      "\n",
      "# 8 Gradient out:  [0.50696473 1.59430878 1.44401378 0.89297216 0.4617539  1.64858979\n",
      " 1.26442593 0.45321483 0.99019372 1.55478157]\n",
      "\n",
      "     Weights  out:  [-0.35903762  0.32954707  0.18081846 -0.06791612 -0.4647426   0.44286497\n",
      "  0.08508216 -0.48938243 -0.02758821  0.2760401 ] [ 0.41955209  0.19610427 -0.07521743  0.42865187 -0.24662203 -0.24088681\n",
      " -0.30935244 -0.06230043  0.10900648 -0.09623463]\n",
      "\n",
      "# 9 Gradient out:  [-0.37227725 -1.33234659 -1.15552489 -0.7511588  -0.29421684 -1.42361683\n",
      " -1.00883358 -0.27981323 -0.8189157  -1.27654296]\n",
      "\n",
      "     Weights  out:  [-0.35903762  0.32954707  0.18081846 -0.06791612 -0.4647426   0.44286497\n",
      "  0.08508216 -0.48938243 -0.02758821  0.2760401 ] [ 0.52094503  0.51496603  0.21358533  0.60724631 -0.15427125  0.08883115\n",
      " -0.05646725  0.02834254  0.30704523  0.21472168]\n",
      "\n",
      "# 10 Gradient out:  [0.04855723 0.05098715 0.09013584 0.01534346 0.08361648 0.01143969\n",
      " 0.07898793 0.09010091 0.03172261 0.06970984]\n",
      "\n",
      "     Weights  out:  [-0.35903762  0.32954707  0.18081846 -0.06791612 -0.4647426   0.44286497\n",
      "  0.08508216 -0.48938243 -0.02758821  0.2760401 ] [ 0.44648958  0.24849671 -0.01751965  0.45701455 -0.21311462 -0.19589222\n",
      " -0.25823397 -0.02762011  0.14326209 -0.04058691]\n",
      "\n",
      "# 11 Gradient out:  [-0.05322697 -0.28656881 -0.20692872 -0.17695965 -0.00133669 -0.34581772\n",
      " -0.18128602  0.00829766 -0.17842649 -0.25551132]\n",
      "\n",
      "     Weights  out:  [-0.35903762  0.32954707  0.18081846 -0.06791612 -0.4647426   0.44286497\n",
      "  0.08508216 -0.48938243 -0.02758821  0.2760401 ] [ 0.45620103  0.25869414  0.00050752  0.46008324 -0.19639132 -0.19360428\n",
      " -0.24243638 -0.00959993  0.14960661 -0.02664494]\n",
      "\n",
      "# 12 Gradient out:  [0.34775674 1.17747432 1.07381402 0.63278182 0.32394367 1.20706689\n",
      " 0.93366671 0.31954241 0.7114585  1.15265739]\n",
      "\n",
      "     Weights  out:  [-0.35903762  0.32954707  0.18081846 -0.06791612 -0.4647426   0.44286497\n",
      "  0.08508216 -0.48938243 -0.02758821  0.2760401 ] [ 0.44555563  0.20138038 -0.04087823  0.42469131 -0.19665866 -0.26276782\n",
      " -0.27869359 -0.00794039  0.11392131 -0.07774721]\n",
      "\n",
      "# 13 Gradient out:  [-0.47755424 -1.80079721 -1.55464607 -1.00186578 -0.36802805 -1.92883643\n",
      " -1.35309406 -0.3478507  -1.09424614 -1.72274939]\n",
      "\n",
      "     Weights  out:  [-0.35903762  0.32954707  0.18081846 -0.06791612 -0.4647426   0.44286497\n",
      "  0.08508216 -0.48938243 -0.02758821  0.2760401 ] [ 0.51510698  0.43687524  0.17388458  0.55124767 -0.13186993 -0.02135444\n",
      " -0.09196025  0.05596809  0.25621301  0.15278427]\n",
      "\n",
      "# 14 Gradient out:  [1.05277912 3.18599683 2.83638599 1.85737148 0.91170883 3.35061969\n",
      " 2.49940739 0.88468891 2.02578707 3.08198357]\n",
      "\n",
      "     Weights  out:  [-0.35903762  0.32954707  0.18081846 -0.06791612 -0.4647426   0.44286497\n",
      "  0.08508216 -0.48938243 -0.02758821  0.2760401 ] [ 0.41959614  0.0767158  -0.13704464  0.35087452 -0.20547554 -0.40712173\n",
      " -0.36257906 -0.01360205  0.03736378 -0.19176561]\n",
      "\n",
      "# 15 Gradient out:  [-0.09380092 -0.30241511 -0.26411516 -0.17602448 -0.07678871 -0.32226338\n",
      " -0.23222847 -0.07362538 -0.19080186 -0.29032988]\n",
      "\n",
      "     Weights  out:  [-0.35903762  0.32954707  0.18081846 -0.06791612 -0.4647426   0.44286497\n",
      "  0.08508216 -0.48938243 -0.02758821  0.2760401 ] [ 0.63015196  0.71391517  0.43023256  0.72234881 -0.02313377  0.26300221\n",
      "  0.13730242  0.16333573  0.4425212   0.42463111]\n",
      "\n",
      "# 16 Gradient out:  [-0.14252022 -0.47784548 -0.41632617 -0.27464761 -0.1152921  -0.50963713\n",
      " -0.36504915 -0.11024081 -0.29841629 -0.45844943]\n",
      "\n",
      "     Weights  out:  [-0.35903762  0.32954707  0.18081846 -0.06791612 -0.4647426   0.44286497\n",
      "  0.08508216 -0.48938243 -0.02758821  0.2760401 ] [ 0.61139177  0.65343215  0.37740953  0.68714392 -0.03849151  0.19854953\n",
      "  0.09085673  0.14861065  0.40436082  0.36656513]\n",
      "\n",
      "# 17 Gradient out:  [-0.26155508 -0.93104013 -0.80811539 -0.52543553 -0.20733945 -0.99440787\n",
      " -0.70573474 -0.19731659 -0.5728423  -0.89229437]\n",
      "\n",
      "     Weights  out:  [-0.35903762  0.32954707  0.18081846 -0.06791612 -0.4647426   0.44286497\n",
      "  0.08508216 -0.48938243 -0.02758821  0.2760401 ] [ 0.58288773  0.55786305  0.2941443   0.63221439 -0.06154993  0.09662211\n",
      "  0.0178469   0.12656249  0.34467756  0.27487524]\n",
      "\n",
      "# 18 Gradient out:  [-0.51218711 -1.95158338 -1.67801109 -1.08754713 -0.38752718 -2.09699431\n",
      " -1.46038661 -0.3644986  -1.18566362 -1.86388143]\n",
      "\n",
      "     Weights  out:  [-0.35903762  0.32954707  0.18081846 -0.06791612 -0.4647426   0.44286497\n",
      "  0.08508216 -0.48938243 -0.02758821  0.2760401 ] [ 0.53057671  0.37165502  0.13252122  0.52712729 -0.10301782 -0.10225947\n",
      " -0.12330005  0.08709917  0.23010911  0.09641637]\n",
      "\n",
      "# 19 Gradient out:  [1.09791655 2.92320106 2.61060871 1.79802682 0.96068098 3.08150534\n",
      " 2.32649828 0.93377522 1.93674382 2.82731349]\n",
      "\n",
      "     Weights  out:  [-0.35903762  0.32954707  0.18081846 -0.06791612 -0.4647426   0.44286497\n",
      "  0.08508216 -0.48938243 -0.02758821  0.2760401 ] [ 0.42813929 -0.01866165 -0.203081    0.30961786 -0.18052326 -0.52165833\n",
      " -0.41537737  0.01419945 -0.00702362 -0.27635992]\n",
      "\n",
      "# 20 Gradient out:  [-0.21122876 -0.77130551 -0.66862074 -0.43185513 -0.16603094 -0.82414459\n",
      " -0.58292603 -0.15767787 -0.47157556 -0.7389669 ]\n",
      "\n",
      "     Weights  out:  [-0.35903762  0.32954707  0.18081846 -0.06791612 -0.4647426   0.44286497\n",
      "  0.08508216 -0.48938243 -0.02758821  0.2760401 ] [0.6477226  0.56597856 0.31904074 0.66922322 0.01161294 0.09464274\n",
      " 0.04992228 0.2009545  0.38032515 0.28910278]\n",
      "\n",
      "# 21 Gradient out:  [-0.46200716 -1.76753251 -1.52418032 -0.97973429 -0.35318428 -1.89462866\n",
      " -1.32549611 -0.33307967 -1.07068047 -1.69026245]\n",
      "\n",
      "     Weights  out:  [-0.35903762  0.32954707  0.18081846 -0.06791612 -0.4647426   0.44286497\n",
      "  0.08508216 -0.48938243 -0.02758821  0.2760401 ] [ 0.60547685  0.41171746  0.18531659  0.5828522  -0.02159325 -0.07018618\n",
      " -0.06666292  0.16941892  0.28601003  0.1413094 ]\n",
      "\n",
      "# 22 Gradient out:  [0.91230376 2.97214491 2.63741813 1.68674643 0.77945581 3.12751726\n",
      " 2.31114519 0.75411707 1.85051589 2.87316128]\n",
      "\n",
      "     Weights  out:  [-0.35903762  0.32954707  0.18081846 -0.06791612 -0.4647426   0.44286497\n",
      "  0.08508216 -0.48938243 -0.02758821  0.2760401 ] [ 0.51307542  0.05821096 -0.11951947  0.38690534 -0.09223011 -0.44911191\n",
      " -0.33176214  0.10280299  0.07187394 -0.19674309]\n",
      "\n",
      "# 23 Gradient out:  [-0.10972401 -0.38847484 -0.33742439 -0.21948205 -0.08722073 -0.41476759\n",
      " -0.29476684 -0.08305326 -0.23927561 -0.3723999 ]\n",
      "\n",
      "     Weights  out:  [-0.35903762  0.32954707  0.18081846 -0.06791612 -0.4647426   0.44286497\n",
      "  0.08508216 -0.48938243 -0.02758821  0.2760401 ] [0.69553617 0.65263994 0.40796416 0.72425463 0.06366106 0.17639154\n",
      " 0.13046689 0.2536264  0.44197712 0.37788917]\n",
      "\n",
      "# 24 Gradient out:  [-0.18593377 -0.68550627 -0.59398112 -0.38266866 -0.14568614 -0.73256292\n",
      " -0.51752442 -0.13824898 -0.41812516 -0.65669398]\n",
      "\n",
      "     Weights  out:  [-0.35903762  0.32954707  0.18081846 -0.06791612 -0.4647426   0.44286497\n",
      "  0.08508216 -0.48938243 -0.02758821  0.2760401 ] [0.67359137 0.57494497 0.34047928 0.68035822 0.04621691 0.09343802\n",
      " 0.07151353 0.23701575 0.39412199 0.30340919]\n",
      "\n",
      "# 25 Gradient out:  [-0.40209423 -1.54931768 -1.33690608 -0.85580675 -0.30780983 -1.65950517\n",
      " -1.16191511 -0.2904022  -0.93631    -1.48210675]\n",
      "\n",
      "     Weights  out:  [-0.35903762  0.32954707  0.18081846 -0.06791612 -0.4647426   0.44286497\n",
      "  0.08508216 -0.48938243 -0.02758821  0.2760401 ] [ 0.63640461  0.43784371  0.22168305  0.60382448  0.01707968 -0.05307456\n",
      " -0.03199136  0.20936595  0.31049696  0.17207039]\n",
      "\n",
      "# 26 Gradient out:  [0.401142   1.77117379 1.57433033 0.89391999 0.34081745 1.84422097\n",
      " 1.34966888 0.32995741 1.01326912 1.7181635 ]\n",
      "\n",
      "     Weights  out:  [-0.35903762  0.32954707  0.18081846 -0.06791612 -0.4647426   0.44286497\n",
      "  0.08508216 -0.48938243 -0.02758821  0.2760401 ] [ 0.55598577  0.12798018 -0.04569816  0.43266313 -0.04448228 -0.3849756\n",
      " -0.26437438  0.15128551  0.12323496 -0.12435096]\n",
      "\n",
      "# 27 Gradient out:  [-0.33023389 -1.25970126 -1.08857238 -0.69699275 -0.25473127 -1.34798352\n",
      " -0.94653286 -0.24079749 -0.76261042 -1.20570893]\n",
      "\n",
      "     Weights  out:  [-0.35903762  0.32954707  0.18081846 -0.06791612 -0.4647426   0.44286497\n",
      "  0.08508216 -0.48938243 -0.02758821  0.2760401 ] [ 0.63621417  0.48221494  0.2691679   0.61144713  0.02368121 -0.0161314\n",
      "  0.0055594   0.217277    0.32588879  0.21928174]\n",
      "\n",
      "# 28 Gradient out:  [-0.25499275 -0.63845272 -0.52821205 -0.44055845 -0.18481692 -0.7179198\n",
      " -0.48086339 -0.17125297 -0.45150002 -0.59689826]\n",
      "\n",
      "     Weights  out:  [-0.35903762  0.32954707  0.18081846 -0.06791612 -0.4647426   0.44286497\n",
      "  0.08508216 -0.48938243 -0.02758821  0.2760401 ] [ 0.57016739  0.23027468  0.05145343  0.47204858 -0.02726505 -0.28572811\n",
      " -0.18374718  0.1691175   0.1733667  -0.02186005]\n",
      "\n",
      "# 29 Gradient out:  [0.65831585 2.44180915 2.16606957 1.31666987 0.55900451 2.55944571\n",
      " 1.87934146 0.54047836 1.46414905 2.36315031]\n",
      "\n",
      "     Weights  out:  [-0.35903762  0.32954707  0.18081846 -0.06791612 -0.4647426   0.44286497\n",
      "  0.08508216 -0.48938243 -0.02758821  0.2760401 ] [ 0.51916884  0.10258414 -0.05418898  0.38393689 -0.06422843 -0.42931206\n",
      " -0.27991985  0.1348669   0.0830667  -0.1412397 ]\n",
      "\n",
      "# 30 Gradient out:  [-0.17618626 -0.64084956 -0.55570643 -0.35918543 -0.13870475 -0.6846618\n",
      " -0.48460106 -0.13177385 -0.39215988 -0.6140401 ]\n",
      "\n",
      "     Weights  out:  [-0.35903762  0.32954707  0.18081846 -0.06791612 -0.4647426   0.44286497\n",
      "  0.08508216 -0.48938243 -0.02758821  0.2760401 ] [0.65083201 0.59094597 0.37902493 0.64727086 0.04757247 0.08257708\n",
      " 0.09594844 0.24296258 0.37589651 0.33139036]\n",
      "\n",
      "# 31 Gradient out:  [-0.37157649 -1.42188836 -1.22803236 -0.78643204 -0.28582767 -1.52213206\n",
      " -1.06765585 -0.27000183 -0.86038538 -1.36064834]\n",
      "\n",
      "     Weights  out:  [-0.35903762  0.32954707  0.18081846 -0.06791612 -0.4647426   0.44286497\n",
      "  0.08508216 -0.48938243 -0.02758821  0.2760401 ] [ 0.61559476  0.46277606  0.26788365  0.57543378  0.01983152 -0.05435528\n",
      " -0.00097177  0.21660781  0.29746453  0.20858234]\n",
      "\n",
      "# 32 Gradient out:  [0.09752736 0.72211347 0.65885822 0.2992937  0.09616251 0.72660069\n",
      " 0.54891085 0.0964535  0.3644783  0.7108348 ]\n",
      "\n",
      "     Weights  out:  [-0.35903762  0.32954707  0.18081846 -0.06791612 -0.4647426   0.44286497\n",
      "  0.08508216 -0.48938243 -0.02758821  0.2760401 ] [ 0.54127946  0.17839839  0.02227717  0.41814737 -0.03733401 -0.35878169\n",
      " -0.21450294  0.16260744  0.12538745 -0.06354733]\n",
      "\n",
      "# 33 Gradient out:  [-0.50500529 -1.85589534 -1.59144139 -1.05165133 -0.38000842 -2.00108822\n",
      " -1.38943284 -0.35672972 -1.14060978 -1.76979738]\n",
      "\n",
      "     Weights  out:  [-0.35903762  0.32954707  0.18081846 -0.06791612 -0.4647426   0.44286497\n",
      "  0.08508216 -0.48938243 -0.02758821  0.2760401 ] [ 0.56078493  0.32282108  0.15404882  0.47800611 -0.01810151 -0.21346155\n",
      " -0.10472077  0.18189814  0.19828311  0.07861963]\n",
      "\n",
      "# 34 Gradient out:  [1.08061112 2.80357337 2.50556171 1.74403338 0.9468969  2.95729938\n",
      " 2.23837052 0.92047868 1.87380931 2.71149547]\n",
      "\n",
      "     Weights  out:  [-0.35903762  0.32954707  0.18081846 -0.06791612 -0.4647426   0.44286497\n",
      "  0.08508216 -0.48938243 -0.02758821  0.2760401 ] [ 0.45978387 -0.04835799 -0.16423946  0.26767584 -0.09410319 -0.6136792\n",
      " -0.38260734  0.1105522  -0.02983884 -0.27533984]\n",
      "\n",
      "# 35 Gradient out:  [-0.25307324 -0.97761897 -0.84466842 -0.53858422 -0.1946326  -1.0459724\n",
      " -0.73382029 -0.18385009 -0.58991842 -0.93574621]\n",
      "\n",
      "     Weights  out:  [-0.35903762  0.32954707  0.18081846 -0.06791612 -0.4647426   0.44286497\n",
      "  0.08508216 -0.48938243 -0.02758821  0.2760401 ] [ 0.6759061   0.51235669  0.33687288  0.61648252  0.09527619 -0.02221932\n",
      "  0.06506676  0.29464793  0.34492302  0.26695925]\n",
      "\n",
      "# 36 Gradient out:  [-0.51953572 -1.90657825 -1.63671882 -1.07937495 -0.39227447 -2.05432699\n",
      " -1.42889649 -0.36850487 -1.17140777 -1.81893126]\n",
      "\n",
      "     Weights  out:  [-0.35903762  0.32954707  0.18081846 -0.06791612 -0.4647426   0.44286497\n",
      "  0.08508216 -0.48938243 -0.02758821  0.2760401 ] [ 0.62529145  0.31683289  0.1679392   0.50876568  0.05634967 -0.2314138\n",
      " -0.0816973   0.25787791  0.22693933  0.07981001]\n",
      "\n",
      "# 37 Gradient out:  [1.07027694 2.79850573 2.49850847 1.7366643  0.93445808 2.95441894\n",
      " 2.23088349 0.90752647 1.86641559 2.70555681]\n",
      "\n",
      "     Weights  out:  [-0.35903762  0.32954707  0.18081846 -0.06791612 -0.4647426   0.44286497\n",
      "  0.08508216 -0.48938243 -0.02758821  0.2760401 ] [ 0.5213843  -0.06448276 -0.15940457  0.29289069 -0.02210523 -0.6422792\n",
      " -0.3674766   0.18417694 -0.00734222 -0.28397624]\n",
      "\n",
      "# 38 Gradient out:  [-0.23916822 -0.9567675  -0.82517596 -0.52186938 -0.18144301 -1.02430972\n",
      " -0.71535531 -0.17080341 -0.57274305 -0.91534621]\n",
      "\n",
      "     Weights  out:  [-0.35903762  0.32954707  0.18081846 -0.06791612 -0.4647426   0.44286497\n",
      "  0.08508216 -0.48938243 -0.02758821  0.2760401 ] [ 0.73543969  0.49521839  0.34029713  0.64022355  0.16478639 -0.05139541\n",
      "  0.0787001   0.36568223  0.3659409   0.25713512]\n",
      "\n",
      "# 39 Gradient out:  [-0.51682815 -1.90163541 -1.63207959 -1.07588549 -0.38923452 -2.04963846\n",
      " -1.42467657 -0.36533377 -1.16772584 -1.81402459]\n",
      "\n",
      "     Weights  out:  [-0.35903762  0.32954707  0.18081846 -0.06791612 -0.4647426   0.44286497\n",
      "  0.08508216 -0.48938243 -0.02758821  0.2760401 ] [ 0.68760605  0.30386489  0.17526194  0.53584967  0.12849779 -0.25625736\n",
      " -0.06437096  0.33152155  0.25139229  0.07406588]\n",
      "\n",
      "# 40 Gradient out:  [1.03659171 2.78812878 2.4838824  1.71214554 0.89850132 2.94657218\n",
      " 2.21273458 0.8710836  1.84356996 2.69380247]\n",
      "\n",
      "     Weights  out:  [-0.35903762  0.32954707  0.18081846 -0.06791612 -0.4647426   0.44286497\n",
      "  0.08508216 -0.48938243 -0.02758821  0.2760401 ] [ 0.58424042 -0.07646219 -0.15115398  0.32067257  0.05065088 -0.66618505\n",
      " -0.34930628  0.2584548   0.01784712 -0.28873904]\n",
      "\n",
      "# 41 Gradient out:  [-0.22451841 -0.92646506 -0.7978418  -0.50096876 -0.1682188  -0.99236535\n",
      " -0.69037802 -0.157853   -0.55077005 -0.88600353]\n",
      "\n",
      "     Weights  out:  [-0.35903762  0.32954707  0.18081846 -0.06791612 -0.4647426   0.44286497\n",
      "  0.08508216 -0.48938243 -0.02758821  0.2760401 ] [ 0.79155876  0.48116356  0.3456225   0.66310168  0.23035115 -0.07687061\n",
      "  0.09324064  0.43267152  0.38656111  0.25002146]\n",
      "\n",
      "# 42 Gradient out:  [-0.50505311 -1.88915247 -1.62062055 -1.063063   -0.37821506 -2.03627209\n",
      " -1.41309117 -0.3544357  -1.1552204  -1.80200008]\n",
      "\n",
      "     Weights  out:  [-0.35903762  0.32954707  0.18081846 -0.06791612 -0.4647426   0.44286497\n",
      "  0.08508216 -0.48938243 -0.02758821  0.2760401 ] [ 0.74665508  0.29587055  0.18605414  0.56290793  0.19670739 -0.27534368\n",
      " -0.04483496  0.40110092  0.2764071   0.07282075]\n",
      "\n",
      "# 43 Gradient out:  [0.9708857  2.76254651 2.45262213 1.66079414 0.83133153 2.92284729\n",
      " 2.17484262 0.8036884  1.79574344 2.66673306]\n",
      "\n",
      "     Weights  out:  [-0.35903762  0.32954707  0.18081846 -0.06791612 -0.4647426   0.44286497\n",
      "  0.08508216 -0.48938243 -0.02758821  0.2760401 ] [ 0.64564446 -0.08195994 -0.13806997  0.35029533  0.12106437 -0.6825981\n",
      " -0.3274532   0.33021378  0.04536302 -0.28757927]\n",
      "\n",
      "# 44 Gradient out:  [-0.21102624 -0.89322402 -0.76831895 -0.47961305 -0.15647066 -0.9571071\n",
      " -0.66384093 -0.14643612 -0.52805145 -0.8539568 ]\n",
      "\n",
      "     Weights  out:  [-0.35903762  0.32954707  0.18081846 -0.06791612 -0.4647426   0.44286497\n",
      "  0.08508216 -0.48938243 -0.02758821  0.2760401 ] [ 0.8398216   0.47054936  0.35245445  0.68245416  0.28733068 -0.09802864\n",
      "  0.10751533  0.49095146  0.40451171  0.24576734]\n",
      "\n",
      "# 45 Gradient out:  [-0.48701937 -1.86375318 -1.59798718 -1.04090612 -0.36217405 -2.00863424\n",
      " -1.3911794  -0.33878284 -1.13311592 -1.77771674]\n",
      "\n",
      "     Weights  out:  [-0.35903762  0.32954707  0.18081846 -0.06791612 -0.4647426   0.44286497\n",
      "  0.08508216 -0.48938243 -0.02758821  0.2760401 ] [ 0.79761635  0.29190456  0.19879066  0.58653155  0.25603655 -0.28945006\n",
      " -0.02525286  0.46166424  0.29890142  0.07497599]\n",
      "\n",
      "# 46 Gradient out:  [0.87531087 2.70536477 2.39183805 1.57736359 0.73692578 2.86479167\n",
      " 2.10709768 0.70969254 1.71641075 2.60910011]\n",
      "\n",
      "     Weights  out:  [-0.35903762  0.32954707  0.18081846 -0.06791612 -0.4647426   0.44286497\n",
      "  0.08508216 -0.48938243 -0.02758821  0.2760401 ] [ 0.70021247 -0.08084608 -0.12080677  0.37835032  0.18360174 -0.69117691\n",
      " -0.30348874  0.39390767  0.07227824 -0.28056736]\n",
      "\n",
      "# 47 Gradient out:  [-0.2045203  -0.8820244  -0.75803242 -0.47121286 -0.15044141 -0.94536661\n",
      " -0.65425062 -0.14050209 -0.51933812 -0.84305937]\n",
      "\n",
      "     Weights  out:  [-0.35903762  0.32954707  0.18081846 -0.06791612 -0.4647426   0.44286497\n",
      "  0.08508216 -0.48938243 -0.02758821  0.2760401 ] [ 0.87527465  0.46022687  0.35756084  0.69382304  0.33098689 -0.11821857\n",
      "  0.1179308   0.53584618  0.41556039  0.24125266]\n",
      "\n",
      "# 48 Gradient out:  [-0.47617793 -1.8404618  -1.57706087 -1.02509097 -0.35236156 -1.98412617\n",
      " -1.37213947 -0.32915092 -1.11645207 -1.75517954]\n",
      "\n",
      "     Weights  out:  [-0.35903762  0.32954707  0.18081846 -0.06791612 -0.4647426   0.44286497\n",
      "  0.08508216 -0.48938243 -0.02758821  0.2760401 ] [ 0.83437059  0.283822    0.20595435  0.59958047  0.30089861 -0.3072919\n",
      " -0.01291933  0.50774576  0.31169276  0.07264078]\n",
      "\n",
      "# 49 Gradient out:  [0.80325056 2.63045044 2.31927242 1.50259298 0.66764265 2.78697801\n",
      " 2.03435686 0.64107065 1.64216088 2.53531801]\n",
      "\n",
      "     Weights  out:  [-0.35903762  0.32954707  0.18081846 -0.06791612 -0.4647426   0.44286497\n",
      "  0.08508216 -0.48938243 -0.02758821  0.2760401 ] [ 0.739135   -0.08427037 -0.10945782  0.39456228  0.2304263  -0.70411713\n",
      " -0.28734722  0.44191558  0.08840235 -0.27839512]\n",
      "\n",
      "Lineal error cuadrático medio (ECM): 2.4235061672486706\n",
      "\n",
      "# 0 Gradient out:  [-1.98252553 -0.33954405 -1.93171986 -0.39380665 -0.46373867 -0.95628884\n",
      " -0.8660354  -1.22981013 -1.82841981 -0.39164052]\n",
      "\n",
      "     Weights  out:  [ 0.47447494 -0.46006565  0.42401751 -0.40894672 -0.3538319  -0.08877332\n",
      " -0.13041017  0.03367436  0.34219491 -0.41082189] [ 0.35128693  0.34030614 -0.0135779   0.15911283  0.21084769  0.04214927\n",
      " -0.45620102 -0.19096599  0.40716873  0.43483938]\n",
      "\n",
      "# 1 Gradient out:  [3.47011905 0.98107969 3.40703197 1.04775517 1.13243168 1.85266993\n",
      " 1.69806059 2.35864446 3.28171402 1.04512687]\n",
      "\n",
      "     Weights  out:  [ 0.47447494 -0.46006565  0.42401751 -0.40894672 -0.3538319  -0.08877332\n",
      " -0.13041017  0.03367436  0.34219491 -0.41082189] [-0.04521817  0.27239733 -0.39992188  0.0803515   0.11809996 -0.1491085\n",
      " -0.6294081  -0.43692801  0.04148477  0.35651128]\n",
      "\n",
      "# 2 Gradient out:  [-0.80831154 -0.1624345  -0.79005766 -0.18190773 -0.20706351 -0.39842777\n",
      " -0.36104061 -0.51543279 -0.75288234 -0.18113086]\n",
      "\n",
      "     Weights  out:  [ 0.47447494 -0.46006565  0.42401751 -0.40894672 -0.3538319  -0.08877332\n",
      " -0.13041017  0.03367436  0.34219491 -0.41082189] [ 0.64880564  0.46861327  0.28148452  0.28990253  0.34458629  0.22142549\n",
      " -0.28979599  0.03480088  0.69782758  0.56553665]\n",
      "\n",
      "# 3 Gradient out:  [-1.72233264 -0.32330061 -1.68220067 -0.36612648 -0.42144378 -0.83682663\n",
      " -0.75652484 -1.08683737 -1.60045961 -0.36441751]\n",
      "\n",
      "     Weights  out:  [ 0.47447494 -0.46006565  0.42401751 -0.40894672 -0.3538319  -0.08877332\n",
      " -0.13041017  0.03367436  0.34219491 -0.41082189] [ 0.48714333  0.43612637  0.12347299  0.25352099  0.30317359  0.14173993\n",
      " -0.36200411 -0.06828568  0.54725111  0.52931048]\n",
      "\n",
      "# 4 Gradient out:  [1.11360057 0.23767074 1.10497837 0.24690642 0.2603564  0.50277398\n",
      " 0.4354723  0.74240558 1.08498791 0.24652758]\n",
      "\n",
      "     Weights  out:  [ 0.47447494 -0.46006565  0.42401751 -0.40894672 -0.3538319  -0.08877332\n",
      " -0.13041017  0.03367436  0.34219491 -0.41082189] [ 0.1426768   0.37146625 -0.21296715  0.18029569  0.21888484 -0.02562539\n",
      " -0.51330908 -0.28565315  0.22715919  0.45642698]\n",
      "\n",
      "# 5 Gradient out:  [-2.15332728 -0.39538573 -2.10018843 -0.45206485 -0.5249574  -1.04950737\n",
      " -0.95130403 -1.35058924 -1.99249153 -0.44980594]\n",
      "\n",
      "     Weights  out:  [ 0.47447494 -0.46006565  0.42401751 -0.40894672 -0.3538319  -0.08877332\n",
      " -0.13041017  0.03367436  0.34219491 -0.41082189] [ 0.36539691  0.41900039  0.00802852  0.22967698  0.27095611  0.0749294\n",
      " -0.42621462 -0.13717204  0.44415677  0.5057325 ]\n",
      "\n",
      "# 6 Gradient out:  [3.45722281 0.95757371 3.39356226 1.02481995 1.11007432 1.83316037\n",
      " 1.67802769 2.34083557 3.267384   1.02217134]\n",
      "\n",
      "     Weights  out:  [ 0.47447494 -0.46006565  0.42401751 -0.40894672 -0.3538319  -0.08877332\n",
      " -0.13041017  0.03367436  0.34219491 -0.41082189] [-0.06526854  0.33992325 -0.41200916  0.13926401  0.16596464 -0.13497207\n",
      " -0.61647542 -0.40728989  0.04565846  0.41577131]\n",
      "\n",
      "# 7 Gradient out:  [-0.77502207 -0.14731252 -0.75735706 -0.16616326 -0.19054124 -0.37653681\n",
      " -0.3401519  -0.49044984 -0.72133175 -0.16541086]\n",
      "\n",
      "     Weights  out:  [ 0.47447494 -0.46006565  0.42401751 -0.40894672 -0.3538319  -0.08877332\n",
      " -0.13041017  0.03367436  0.34219491 -0.41082189] [ 0.62617602  0.53143799  0.26670329  0.344228    0.3879795   0.23166\n",
      " -0.28086989  0.06087723  0.69913526  0.62020558]\n",
      "\n",
      "# 8 Gradient out:  [-1.630802   -0.29651996 -1.5925619  -0.33732373 -0.39002286 -0.78611762\n",
      " -0.70948449 -1.02479175 -1.5146887  -0.33569561]\n",
      "\n",
      "     Weights  out:  [ 0.47447494 -0.46006565  0.42401751 -0.40894672 -0.3538319  -0.08877332\n",
      " -0.13041017  0.03367436  0.34219491 -0.41082189] [ 0.4711716   0.50197548  0.11523188  0.31099534  0.34987125  0.15635264\n",
      " -0.34890026 -0.03721274  0.55486891  0.58712341]\n",
      "\n",
      "# 9 Gradient out:  [ 0.29931384  0.00493637  0.30809867 -0.00418684 -0.01366656  0.05851705\n",
      "  0.02487526  0.19160398  0.32202959 -0.0038457 ]\n",
      "\n",
      "     Weights  out:  [ 0.47447494 -0.46006565  0.42401751 -0.40894672 -0.3538319  -0.08877332\n",
      " -0.13041017  0.03367436  0.34219491 -0.41082189] [ 0.1450112   0.44267149 -0.2032805   0.2435306   0.27186668 -0.00087088\n",
      " -0.49079716 -0.24217109  0.25193117  0.51998428]\n",
      "\n",
      "# 10 Gradient out:  [-0.86229066 -0.19429812 -0.83025074 -0.22827221 -0.27033901 -0.47964043\n",
      " -0.4536814  -0.53965853 -0.76814133 -0.22693522]\n",
      "\n",
      "     Weights  out:  [ 0.47447494 -0.46006565  0.42401751 -0.40894672 -0.3538319  -0.08877332\n",
      " -0.13041017  0.03367436  0.34219491 -0.41082189] [ 0.20487397  0.44365877 -0.14166076  0.24269323  0.26913337  0.01083253\n",
      " -0.48582211 -0.20385029  0.31633709  0.51921514]\n",
      "\n",
      "# 11 Gradient out:  [2.72869835 0.61008702 2.68406286 0.65750328 0.7194061  1.32747724\n",
      " 1.18803728 1.79457229 2.59242653 0.65561518]\n",
      "\n",
      "     Weights  out:  [ 0.47447494 -0.46006565  0.42401751 -0.40894672 -0.3538319  -0.08877332\n",
      " -0.13041017  0.03367436  0.34219491 -0.41082189] [ 0.03241584  0.40479914 -0.30771091  0.19703879  0.21506557 -0.08509556\n",
      " -0.57655839 -0.311782    0.16270882  0.4738281 ]\n",
      "\n",
      "# 12 Gradient out:  [-0.98810567 -0.18741354 -0.9654989  -0.21153717 -0.24272575 -0.48004854\n",
      " -0.43371058 -0.62499695 -0.91940946 -0.21057437]\n",
      "\n",
      "     Weights  out:  [ 0.47447494 -0.46006565  0.42401751 -0.40894672 -0.3538319  -0.08877332\n",
      " -0.13041017  0.03367436  0.34219491 -0.41082189] [ 0.57815551  0.52681655  0.22910166  0.32853944  0.35894679  0.18039989\n",
      " -0.33895093  0.04713246  0.68119413  0.60495113]\n",
      "\n",
      "# 13 Gradient out:  [-2.08311523 -0.38073915 -2.03276775 -0.43443541 -0.50356409 -1.0102488\n",
      " -0.91396977 -1.30760453 -1.93062286 -0.43229515]\n",
      "\n",
      "     Weights  out:  [ 0.47447494 -0.46006565  0.42401751 -0.40894672 -0.3538319  -0.08877332\n",
      " -0.13041017  0.03367436  0.34219491 -0.41082189] [ 0.38053438  0.48933384  0.03600188  0.28623201  0.31040164  0.08439018\n",
      " -0.42569305 -0.07786693  0.49731224  0.56283626]\n",
      "\n",
      "# 14 Gradient out:  [3.2968675  0.80805514 3.23774607 0.87065904 0.95094831 1.66916481\n",
      " 1.51121174 2.1905836  3.11891416 0.86818214]\n",
      "\n",
      "     Weights  out:  [ 0.47447494 -0.46006565  0.42401751 -0.40894672 -0.3538319  -0.08877332\n",
      " -0.13041017  0.03367436  0.34219491 -0.41082189] [-0.03608867  0.41318601 -0.37055167  0.19934493  0.20968882 -0.11765958\n",
      " -0.608487   -0.33938784  0.11118767  0.47637723]\n",
      "\n",
      "# 15 Gradient out:  [-0.70253193 -0.13193721 -0.68650052 -0.14904564 -0.17117605 -0.3402349\n",
      " -0.30713808 -0.44388669 -0.65379662 -0.14836271]\n",
      "\n",
      "     Weights  out:  [ 0.47447494 -0.46006565  0.42401751 -0.40894672 -0.3538319  -0.08877332\n",
      " -0.13041017  0.03367436  0.34219491 -0.41082189] [ 0.62328483  0.57479704  0.27699754  0.37347673  0.39987848  0.21617338\n",
      " -0.30624466  0.09872888  0.7349705   0.65001366]\n",
      "\n",
      "# 16 Gradient out:  [-1.43267334 -0.25818622 -1.39924322 -0.29386163 -0.33997066 -0.68843768\n",
      " -0.62076212 -0.8995801  -1.33110727 -0.29243779]\n",
      "\n",
      "     Weights  out:  [ 0.47447494 -0.46006565  0.42401751 -0.40894672 -0.3538319  -0.08877332\n",
      " -0.13041017  0.03367436  0.34219491 -0.41082189] [ 0.48277845  0.5484096   0.13969744  0.3436676   0.36564327  0.1481264\n",
      " -0.36767227  0.00995154  0.60421117  0.62034111]\n",
      "\n",
      "# 17 Gradient out:  [-1.14124744 -0.26347064 -1.10331784 -0.30366013 -0.35353439 -0.62395045\n",
      " -0.58548523 -0.72412786 -1.02965637 -0.30207898]\n",
      "\n",
      "     Weights  out:  [ 0.47447494 -0.46006565  0.42401751 -0.40894672 -0.3538319  -0.08877332\n",
      " -0.13041017  0.03367436  0.34219491 -0.41082189] [ 0.19624378  0.49677235 -0.1401512   0.28489528  0.29764914  0.01043887\n",
      " -0.4918247  -0.16996448  0.33798972  0.56185356]\n",
      "\n",
      "# 18 Gradient out:  [3.13620727 0.72702517 3.08078044 0.78578381 0.86155088 1.55602446\n",
      " 1.401638   2.06756264 2.9686402  0.78345414]\n",
      "\n",
      "     Weights  out:  [ 0.47447494 -0.46006565  0.42401751 -0.40894672 -0.3538319  -0.08877332\n",
      " -0.13041017  0.03367436  0.34219491 -0.41082189] [-0.03200571  0.44407822 -0.36081477  0.22416325  0.22694226 -0.11435122\n",
      " -0.60892174 -0.31479005  0.13205845  0.50143776]\n",
      "\n",
      "# 19 Gradient out:  [-0.78034567 -0.14421723 -0.76247165 -0.1632935  -0.18797416 -0.37647512\n",
      " -0.33958572 -0.49197881 -0.72599932 -0.16253193]\n",
      "\n",
      "     Weights  out:  [ 0.47447494 -0.46006565  0.42401751 -0.40894672 -0.3538319  -0.08877332\n",
      " -0.13041017  0.03367436  0.34219491 -0.41082189] [ 0.59523575  0.58948326  0.25534132  0.38132002  0.39925244  0.19685367\n",
      " -0.32859414  0.09872248  0.72578649  0.65812859]\n",
      "\n",
      "# 20 Gradient out:  [-1.64306066 -0.29319772 -1.6043303  -0.33452321 -0.38788835 -0.78864432\n",
      " -0.71115648 -1.02991017 -1.52547296 -0.33287437]\n",
      "\n",
      "     Weights  out:  [ 0.47447494 -0.46006565  0.42401751 -0.40894672 -0.3538319  -0.08877332\n",
      " -0.13041017  0.03367436  0.34219491 -0.41082189] [ 4.39166611e-01  5.60639813e-01  1.02846986e-01  3.48661317e-01\n",
      "  3.61657604e-01  1.21558645e-01 -3.96511287e-01  3.26717829e-04\n",
      "  5.80586622e-01  6.25622202e-01]\n",
      "\n",
      "# 21 Gradient out:  [ 0.36865995 -0.005394    0.37613579 -0.01306128 -0.02037996  0.075271\n",
      "  0.03632164  0.22584396  0.38687815 -0.01278207]\n",
      "\n",
      "     Weights  out:  [ 0.47447494 -0.46006565  0.42401751 -0.40894672 -0.3538319  -0.08877332\n",
      " -0.13041017  0.03367436  0.34219491 -0.41082189] [ 0.11055448  0.50200027 -0.21801907  0.28175668  0.28407993 -0.03617022\n",
      " -0.53874258 -0.20565532  0.27549203  0.55904733]\n",
      "\n",
      "# 22 Gradient out:  [-1.01153909 -0.23340856 -0.97637289 -0.27067866 -0.31687205 -0.55824224\n",
      " -0.52573582 -0.6392759  -0.90815728 -0.26921239]\n",
      "\n",
      "     Weights  out:  [ 0.47447494 -0.46006565  0.42401751 -0.40894672 -0.3538319  -0.08877332\n",
      " -0.13041017  0.03367436  0.34219491 -0.41082189] [ 0.18428647  0.50092147 -0.14279192  0.27914442  0.28000394 -0.02111602\n",
      " -0.53147825 -0.16048652  0.35286766  0.55649091]\n",
      "\n",
      "# 23 Gradient out:  [2.98609986 0.67567155 2.93482115 0.73008847 0.80064184 1.46560416\n",
      " 1.31592018 1.96373752 2.83039257 0.72792659]\n",
      "\n",
      "     Weights  out:  [ 0.47447494 -0.46006565  0.42401751 -0.40894672 -0.3538319  -0.08877332\n",
      " -0.13041017  0.03367436  0.34219491 -0.41082189] [-0.01802135  0.45423976 -0.33806649  0.22500869  0.21662953 -0.13276447\n",
      " -0.63662542 -0.2883417   0.1712362   0.50264844]\n",
      "\n",
      "# 24 Gradient out:  [-0.84207703 -0.15754525 -0.82280297 -0.17811365 -0.20471417 -0.40756726\n",
      " -0.36790208 -0.53172262 -0.78349343 -0.17729266]\n",
      "\n",
      "     Weights  out:  [ 0.47447494 -0.46006565  0.42401751 -0.40894672 -0.3538319  -0.08877332\n",
      " -0.13041017  0.03367436  0.34219491 -0.41082189] [ 0.57919862  0.58937407  0.24889774  0.37102638  0.3767579   0.16035636\n",
      " -0.37344138  0.1044058   0.73731472  0.64823375]\n",
      "\n",
      "# 25 Gradient out:  [-1.80254229 -0.32455543 -1.75985303 -0.37010001 -0.42887189 -0.86789359\n",
      " -0.78331986 -1.13077923 -1.67300726 -0.36828327]\n",
      "\n",
      "     Weights  out:  [ 0.47447494 -0.46006565  0.42401751 -0.40894672 -0.3538319  -0.08877332\n",
      " -0.13041017  0.03367436  0.34219491 -0.41082189] [ 0.41078322  0.55786502  0.08433714  0.33540365  0.33581507  0.07884291\n",
      " -0.4470218  -0.00193872  0.58061603  0.61277522]\n",
      "\n",
      "# 26 Gradient out:  [1.65375089 0.28681979 1.63313968 0.30891841 0.33939315 0.72633357\n",
      " 0.6289992  1.06234712 1.58799059 0.3080213 ]\n",
      "\n",
      "     Weights  out:  [ 0.47447494 -0.46006565  0.42401751 -0.40894672 -0.3538319  -0.08877332\n",
      " -0.13041017  0.03367436  0.34219491 -0.41082189] [ 0.05027476  0.49295393 -0.26763346  0.26138365  0.25004069 -0.09473581\n",
      " -0.60368577 -0.22809457  0.24601458  0.53911857]\n",
      "\n",
      "# 27 Gradient out:  [-1.96600964 -0.35578937 -1.91902862 -0.40590492 -0.470509   -0.94920821\n",
      " -0.85752145 -1.23344218 -1.82356665 -0.40390653]\n",
      "\n",
      "     Weights  out:  [ 0.47447494 -0.46006565  0.42401751 -0.40894672 -0.3538319  -0.08877332\n",
      " -0.13041017  0.03367436  0.34219491 -0.41082189] [ 0.38102494  0.55031789  0.05899447  0.32316733  0.31791932  0.05053091\n",
      " -0.47788593 -0.01562515  0.5636127   0.60072283]\n",
      "\n",
      "# 28 Gradient out:  [2.76432196 0.59504127 2.71824164 0.64400672 0.70793596 1.33115303\n",
      " 1.18883106 1.80712991 2.62361236 0.64205638]\n",
      "\n",
      "     Weights  out:  [ 0.47447494 -0.46006565  0.42401751 -0.40894672 -0.3538319  -0.08877332\n",
      " -0.13041017  0.03367436  0.34219491 -0.41082189] [-0.01217699  0.47916002 -0.32481125  0.24198635  0.22381752 -0.13931074\n",
      " -0.64939022 -0.26231358  0.19889937  0.51994152]\n",
      "\n",
      "# 29 Gradient out:  [-0.9921901  -0.18352416 -0.96937984 -0.2078669  -0.2393475  -0.47904341\n",
      " -0.43223152 -0.62548172 -0.9228591  -0.20689523]\n",
      "\n",
      "     Weights  out:  [ 0.47447494 -0.46006565  0.42401751 -0.40894672 -0.3538319  -0.08877332\n",
      " -0.13041017  0.03367436  0.34219491 -0.41082189] [ 0.5406874   0.59816827  0.21883708  0.37078769  0.36540471  0.12691987\n",
      " -0.41162401  0.0991124   0.72362184  0.6483528 ]\n",
      "\n",
      "# 30 Gradient out:  [-2.08883165 -0.3783421  -2.03808132 -0.4324577  -0.50207895 -1.01120999\n",
      " -0.91459326 -1.30944448 -1.93520769 -0.43030139]\n",
      "\n",
      "     Weights  out:  [ 0.47447494 -0.46006565  0.42401751 -0.40894672 -0.3538319  -0.08877332\n",
      " -0.13041017  0.03367436  0.34219491 -0.41082189] [ 0.34224938  0.56146344  0.02496111  0.32921431  0.31753521  0.03111119\n",
      " -0.49807031 -0.02598395  0.53905002  0.60697375]\n",
      "\n",
      "# 31 Gradient out:  [3.26828948 0.78235834 3.20912405 0.8450112  0.92535948 1.64288875\n",
      " 1.48524229 2.16309056 3.09020629 0.8425323 ]\n",
      "\n",
      "     Weights  out:  [ 0.47447494 -0.46006565  0.42401751 -0.40894672 -0.3538319  -0.08877332\n",
      " -0.13041017  0.03367436  0.34219491 -0.41082189] [-0.07551695  0.48579502 -0.38265516  0.24272277  0.21711942 -0.17113081\n",
      " -0.68098896 -0.28787284  0.15200848  0.52091347]\n",
      "\n",
      "# 32 Gradient out:  [-0.74123529 -0.13542466 -0.72423002 -0.15357443 -0.17706025 -0.35656966\n",
      " -0.32142425 -0.46663328 -0.68952341 -0.1528498 ]\n",
      "\n",
      "     Weights  out:  [ 0.47447494 -0.46006565  0.42401751 -0.40894672 -0.3538319  -0.08877332\n",
      " -0.13041017  0.03367436  0.34219491 -0.41082189] [ 0.57814095  0.64226669  0.25916965  0.41172501  0.40219132  0.15744694\n",
      " -0.38394051  0.14474527  0.77004974  0.68941993]\n",
      "\n",
      "# 33 Gradient out:  [-1.53826534 -0.27171136 -1.50208695 -0.31031746 -0.36019666 -0.73609521\n",
      " -0.66323849 -0.96318971 -1.42838028 -0.30877684]\n",
      "\n",
      "     Weights  out:  [ 0.47447494 -0.46006565  0.42401751 -0.40894672 -0.3538319  -0.08877332\n",
      " -0.13041017  0.03367436  0.34219491 -0.41082189] [ 0.42989389  0.61518176  0.11432365  0.38101012  0.36677926  0.08613301\n",
      " -0.44822536  0.05141861  0.63214506  0.65884997]\n",
      "\n",
      "# 34 Gradient out:  [-0.45928866 -0.16421656 -0.43458151 -0.19022431 -0.22124272 -0.32140702\n",
      " -0.31968073 -0.30177154 -0.38879431 -0.18921492]\n",
      "\n",
      "     Weights  out:  [ 0.47447494 -0.46006565  0.42401751 -0.40894672 -0.3538319  -0.08877332\n",
      " -0.13041017  0.03367436  0.34219491 -0.41082189] [ 0.12224082  0.56083949 -0.18609374  0.31894663  0.29473993 -0.06108603\n",
      " -0.58087306 -0.14121933  0.346469    0.59709461]\n",
      "\n",
      "# 35 Gradient out:  [1.67387962 0.28218041 1.65272064 0.30487466 0.33616946 0.73044945\n",
      " 0.63157676 1.07139611 1.6063608  0.30395307]\n",
      "\n",
      "     Weights  out:  [ 0.47447494 -0.46006565  0.42401751 -0.40894672 -0.3538319  -0.08877332\n",
      " -0.13041017  0.03367436  0.34219491 -0.41082189] [ 0.03038309  0.52799617 -0.27301004  0.28090177  0.25049139 -0.12536744\n",
      " -0.6448092  -0.20157364  0.26871014  0.55925162]\n",
      "\n",
      "# 36 Gradient out:  [-1.95963855 -0.35287976 -1.91278201 -0.40286198 -0.46729438 -0.94493135\n",
      " -0.85341478 -1.22868858 -1.81757344 -0.40086893]\n",
      "\n",
      "     Weights  out:  [ 0.47447494 -0.46006565  0.42401751 -0.40894672 -0.3538319  -0.08877332\n",
      " -0.13041017  0.03367436  0.34219491 -0.41082189] [ 0.36515901  0.58443226  0.05753408  0.3418767   0.31772528  0.02072245\n",
      " -0.51849385  0.01270559  0.5899823   0.62004223]\n",
      "\n",
      "# 37 Gradient out:  [2.70941484 0.57006452 2.66446957 0.61784489 0.68035398 1.29476805\n",
      " 1.15400064 1.76604641 2.57194207 0.61594022]\n",
      "\n",
      "     Weights  out:  [ 0.47447494 -0.46006565  0.42401751 -0.40894672 -0.3538319  -0.08877332\n",
      " -0.13041017  0.03367436  0.34219491 -0.41082189] [-0.0267687   0.5138563  -0.32502232  0.2613043   0.2242664  -0.16826382\n",
      " -0.68917681 -0.23303213  0.22646761  0.53986845]\n",
      "\n",
      "# 38 Gradient out:  [-1.04060211 -0.1908165  -1.01661854 -0.21641172 -0.24951204 -0.50141668\n",
      " -0.45224017 -0.65522187 -0.9677044  -0.21539004]\n",
      "\n",
      "     Weights  out:  [ 0.47447494 -0.46006565  0.42401751 -0.40894672 -0.3538319  -0.08877332\n",
      " -0.13041017  0.03367436  0.34219491 -0.41082189] [ 0.51511427  0.62786921  0.2078716   0.38487328  0.3603372   0.09068979\n",
      " -0.45837668  0.12017715  0.74085603  0.66305649]\n",
      "\n",
      "# 39 Gradient out:  [-2.13876279 -0.38854429 -2.086192   -0.44458574 -0.51658069 -1.03800845\n",
      " -0.93974051 -1.34035964 -1.97981244 -0.44235387]\n",
      "\n",
      "     Weights  out:  [ 0.47447494 -0.46006565  0.42401751 -0.40894672 -0.3538319  -0.08877332\n",
      " -0.13041017  0.03367436  0.34219491 -0.41082189] [ 0.30699385  0.58970591  0.00454789  0.34159094  0.31043479 -0.00959354\n",
      " -0.54882471 -0.01086722  0.54731515  0.61997848]\n",
      "\n",
      "# 40 Gradient out:  [3.37186088 0.85163837 3.30929601 0.91778724 1.00200696 1.73041636\n",
      " 1.57268961 2.24826694 3.18464869 0.9151775 ]\n",
      "\n",
      "     Weights  out:  [ 0.47447494 -0.46006565  0.42401751 -0.40894672 -0.3538319  -0.08877332\n",
      " -0.13041017  0.03367436  0.34219491 -0.41082189] [-0.12075871  0.51199705 -0.41269051  0.25267379  0.20711865 -0.21719523\n",
      " -0.73677282 -0.27893915  0.15135266  0.53150771]\n",
      "\n",
      "# 41 Gradient out:  [-0.75700511 -0.1358095  -0.73958276 -0.15440599 -0.17847659 -0.36255457\n",
      " -0.32650913 -0.47543875 -0.70401219 -0.15366343]\n",
      "\n",
      "     Weights  out:  [ 0.47447494 -0.46006565  0.42401751 -0.40894672 -0.3538319  -0.08877332\n",
      " -0.13041017  0.03367436  0.34219491 -0.41082189] [ 0.55361347  0.68232472  0.24916869  0.43623124  0.40752005  0.12888804\n",
      " -0.42223489  0.17071424  0.7882824   0.71454321]\n",
      "\n",
      "# 42 Gradient out:  [-1.57975123 -0.27557186 -1.54243034 -0.3153957  -0.36683809 -0.75396227\n",
      " -0.67900584 -0.98749067 -1.46641396 -0.3138066 ]\n",
      "\n",
      "     Weights  out:  [ 0.47447494 -0.46006565  0.42401751 -0.40894672 -0.3538319  -0.08877332\n",
      " -0.13041017  0.03367436  0.34219491 -0.41082189] [ 0.40221245  0.65516282  0.10125214  0.40535004  0.37182473  0.05637713\n",
      " -0.48753672  0.07562649  0.64747996  0.68381052]\n",
      "\n",
      "# 43 Gradient out:  [-0.16356534 -0.12458795 -0.14474656 -0.144275   -0.1668808  -0.19205195\n",
      " -0.20636511 -0.12031613 -0.11139379 -0.14352067]\n",
      "\n",
      "     Weights  out:  [ 0.47447494 -0.46006565  0.42401751 -0.40894672 -0.3538319  -0.08877332\n",
      " -0.13041017  0.03367436  0.34219491 -0.41082189] [ 0.0862622   0.60004845 -0.20723393  0.3422709   0.29845711 -0.09441533\n",
      " -0.62333789 -0.12187165  0.35419717  0.6210492 ]\n",
      "\n",
      "# 44 Gradient out:  [0.72593805 0.06348673 0.72557794 0.06414537 0.06749815 0.24775998\n",
      " 0.19163007 0.45314404 0.72054074 0.06409415]\n",
      "\n",
      "     Weights  out:  [ 0.47447494 -0.46006565  0.42401751 -0.40894672 -0.3538319  -0.08877332\n",
      " -0.13041017  0.03367436  0.34219491 -0.41082189] [ 0.05354913  0.57513086 -0.23618324  0.3134159   0.26508095 -0.13282572\n",
      " -0.66461091 -0.14593487  0.33191841  0.59234507]\n",
      "\n",
      "# 45 Gradient out:  [-1.7910564  -0.35319334 -1.74165259 -0.40573427 -0.47231095 -0.90556729\n",
      " -0.83068351 -1.12610831 -1.64329796 -0.40365186]\n",
      "\n",
      "     Weights  out:  [ 0.47447494 -0.46006565  0.42401751 -0.40894672 -0.3538319  -0.08877332\n",
      " -0.13041017  0.03367436  0.34219491 -0.41082189] [ 0.19873674  0.58782821 -0.09106766  0.32624498  0.27858058 -0.08327372\n",
      " -0.6262849  -0.05530607  0.47602656  0.6051639 ]\n",
      "\n",
      "# 46 Gradient out:  [3.39689329 0.91743788 3.33305617 0.98483348 1.07008621 1.78742768\n",
      " 1.6340549  2.28877834 3.206878   0.9821815 ]\n",
      "\n",
      "     Weights  out:  [ 0.47447494 -0.46006565  0.42401751 -0.40894672 -0.3538319  -0.08877332\n",
      " -0.13041017  0.03367436  0.34219491 -0.41082189] [-0.15947454  0.51718954 -0.43939817  0.24509812  0.18411839 -0.26438718\n",
      " -0.7924216  -0.28052773  0.14736696  0.52443353]\n",
      "\n",
      "# 47 Gradient out:  [-0.85364487 -0.151784   -0.83393603 -0.17282123 -0.20004949 -0.40806075\n",
      " -0.36736067 -0.53547451 -0.79369941 -0.17198122]\n",
      "\n",
      "     Weights  out:  [ 0.47447494 -0.46006565  0.42401751 -0.40894672 -0.3538319  -0.08877332\n",
      " -0.13041017  0.03367436  0.34219491 -0.41082189] [ 0.51990412  0.70067711  0.22721306  0.44206482  0.39813563  0.09309836\n",
      " -0.46561062  0.17722794  0.78874256  0.72086983]\n",
      "\n",
      "# 48 Gradient out:  [-1.82520048 -0.31889142 -1.78152275 -0.36548311 -0.42556807 -0.87309557\n",
      " -0.78704549 -1.14034212 -1.69273639 -0.3636251 ]\n",
      "\n",
      "     Weights  out:  [ 0.47447494 -0.46006565  0.42401751 -0.40894672 -0.3538319  -0.08877332\n",
      " -0.13041017  0.03367436  0.34219491 -0.41082189] [ 0.34917514  0.67032031  0.06042585  0.40750057  0.35812573  0.01148621\n",
      " -0.53908275  0.07013304  0.63000268  0.68647359]\n",
      "\n",
      "# 49 Gradient out:  [1.73922149 0.25770822 1.71635998 0.28228306 0.31629446 0.7372378\n",
      " 0.63260543 1.09681715 1.66599235 0.28128237]\n",
      "\n",
      "     Weights  out:  [ 0.47447494 -0.46006565  0.42401751 -0.40894672 -0.3538319  -0.08877332\n",
      " -0.13041017  0.03367436  0.34219491 -0.41082189] [-0.01586495  0.60654203 -0.2958787   0.33440395  0.27301212 -0.16313291\n",
      " -0.69649185 -0.15793539  0.2914554   0.61374856]\n",
      "\n",
      "Tanh error cuadrático medio (ECM): 0.8219370404374469\n",
      "\n",
      "# 0 Gradient out:  [-0.4938281  -0.53843896 -1.72512751 -1.80650595 -0.43277205 -0.70813567\n",
      " -0.88081359 -1.88477718 -1.71015051 -0.47739801]\n",
      "\n",
      "     Weights  out:  [-0.34483815 -0.31365224  0.33683119  0.40001332 -0.39225871 -0.21088055\n",
      " -0.11903972  0.47529957  0.32630097 -0.35698964] [ 0.23326126  0.16480109 -0.32267817  0.25493424  0.35090753  0.11179848\n",
      " -0.11209592  0.21480418  0.26286773  0.28036709]\n",
      "\n",
      "# 1 Gradient out:  [1.12570659 1.17927092 3.04148836 3.13769845 1.05359784 1.39981132\n",
      " 1.66539392 3.23185861 3.02354403 1.10622361]\n",
      "\n",
      "     Weights  out:  [-0.34483815 -0.31365224  0.33683119  0.40001332 -0.39225871 -0.21088055\n",
      " -0.11903972  0.47529957  0.32630097 -0.35698964] [ 0.13449564  0.05711329 -0.66770367 -0.10636695  0.26435312 -0.02982866\n",
      " -0.28825864 -0.16215126 -0.07916238  0.18488749]\n",
      "\n",
      "# 2 Gradient out:  [-0.21636382 -0.23412744 -0.78656798 -0.81833716 -0.19254141 -0.30535468\n",
      " -0.38509334 -0.8482542  -0.78061422 -0.20990244]\n",
      "\n",
      "     Weights  out:  [-0.34483815 -0.31365224  0.33683119  0.40001332 -0.39225871 -0.21088055\n",
      " -0.11903972  0.47529957  0.32630097 -0.35698964] [ 0.35963695  0.29296748 -0.059406    0.52117273  0.47507268  0.25013361\n",
      "  0.04482014  0.48422047  0.52554643  0.40613221]\n",
      "\n",
      "# 3 Gradient out:  [-0.47033132 -0.51149962 -1.7445321  -1.81853849 -0.4148291  -0.6744099\n",
      " -0.85271561 -1.88863188 -1.73072799 -0.45530849]\n",
      "\n",
      "     Weights  out:  [-0.34483815 -0.31365224  0.33683119  0.40001332 -0.39225871 -0.21088055\n",
      " -0.11903972  0.47529957  0.32630097 -0.35698964] [ 0.31636419  0.24614199 -0.2167196   0.3575053   0.4365644   0.18906267\n",
      " -0.03219853  0.31456963  0.36942359  0.36415172]\n",
      "\n",
      "# 4 Gradient out:  [0.75411617 0.80055677 2.68401689 2.76466464 0.69371687 1.00511046\n",
      " 1.27292589 2.83986732 2.66849759 0.73756009]\n",
      "\n",
      "     Weights  out:  [-0.34483815 -0.31365224  0.33683119  0.40001332 -0.39225871 -0.21088055\n",
      " -0.11903972  0.47529957  0.32630097 -0.35698964] [ 0.22229793  0.14384207 -0.56562602 -0.0062024   0.35359858  0.05418069\n",
      " -0.20274165 -0.06315675  0.02327799  0.27309002]\n",
      "\n",
      "# 5 Gradient out:  [-0.18319848 -0.19807289 -0.66148754 -0.68808543 -0.16325394 -0.25774877\n",
      " -0.32462916 -0.71313195 -0.65650222 -0.17778861]\n",
      "\n",
      "     Weights  out:  [-0.34483815 -0.31365224  0.33683119  0.40001332 -0.39225871 -0.21088055\n",
      " -0.11903972  0.47529957  0.32630097 -0.35698964] [ 0.37312116  0.30395342 -0.02882264  0.54673053  0.49234196  0.25520279\n",
      "  0.05184353  0.50481671  0.55697751  0.42060204]\n",
      "\n",
      "# 6 Gradient out:  [-0.38246752 -0.41558447 -1.42782177 -1.48717901 -0.33795503 -0.54758707\n",
      " -0.6938296  -1.54319553 -1.41672    -0.37040473]\n",
      "\n",
      "     Weights  out:  [-0.34483815 -0.31365224  0.33683119  0.40001332 -0.39225871 -0.21088055\n",
      " -0.11903972  0.47529957  0.32630097 -0.35698964] [ 0.33648146  0.26433884 -0.16112015  0.40911345  0.45969117  0.20365303\n",
      " -0.0130823   0.36219032  0.42567706  0.38504432]\n",
      "\n",
      "# 7 Gradient out:  [-0.02440564 -0.02675179  0.39863638  0.3903439  -0.01810377 -0.01312389\n",
      "  0.04496317  0.37812703  0.39948585 -0.02303366]\n",
      "\n",
      "     Weights  out:  [-0.34483815 -0.31365224  0.33683119  0.40001332 -0.39225871 -0.21088055\n",
      " -0.11903972  0.47529957  0.32630097 -0.35698964] [ 0.25998796  0.18122195 -0.4466845   0.11167764  0.39210016  0.09413562\n",
      " -0.15184823  0.05355122  0.14233306  0.31096338]\n",
      "\n",
      "# 8 Gradient out:  [-0.35865617 -0.39050269 -1.04063869 -1.10043674 -0.31375907 -0.50240233\n",
      " -0.59846957 -1.15988349 -1.02992138 -0.34671368]\n",
      "\n",
      "     Weights  out:  [-0.34483815 -0.31365224  0.33683119  0.40001332 -0.39225871 -0.21088055\n",
      " -0.11903972  0.47529957  0.32630097 -0.35698964] [ 0.25510683  0.17587159 -0.36695722  0.18974642  0.38847941  0.09151084\n",
      " -0.14285559  0.12917662  0.22223023  0.30635665]\n",
      "\n",
      "# 9 Gradient out:  [0.97862433 1.03265825 3.08142849 3.17667356 0.90726762 1.26366946\n",
      " 1.55540337 3.26738058 3.06335169 0.95918878]\n",
      "\n",
      "     Weights  out:  [-0.34483815 -0.31365224  0.33683119  0.40001332 -0.39225871 -0.21088055\n",
      " -0.11903972  0.47529957  0.32630097 -0.35698964] [ 0.1833756   0.09777105 -0.57508496 -0.03034092  0.32572759 -0.00896963\n",
      " -0.26254951 -0.10280007  0.01624596  0.23701391]\n",
      "\n",
      "# 10 Gradient out:  [-0.13348989 -0.14376974 -0.46410815 -0.48249898 -0.11969946 -0.1849989\n",
      " -0.23121976 -0.49983952 -0.46066268 -0.1297502 ]\n",
      "\n",
      "     Weights  out:  [-0.34483815 -0.31365224  0.33683119  0.40001332 -0.39225871 -0.21088055\n",
      " -0.11903972  0.47529957  0.32630097 -0.35698964] [0.37910046 0.3043027  0.04120074 0.60499379 0.50718112 0.24376427\n",
      " 0.04853117 0.55067604 0.6289163  0.42885167]\n",
      "\n",
      "# 11 Gradient out:  [-0.23854135 -0.25804186 -0.86344143 -0.8983277  -0.21238138 -0.33618066\n",
      " -0.4235677  -0.93119403 -0.85690537 -0.23144687]\n",
      "\n",
      "     Weights  out:  [-0.34483815 -0.31365224  0.33683119  0.40001332 -0.39225871 -0.21088055\n",
      " -0.11903972  0.47529957  0.32630097 -0.35698964] [ 0.35240249  0.27554876 -0.05162089  0.50849399  0.48324123  0.20676448\n",
      "  0.00228722  0.45070814  0.53678376  0.40290163]\n",
      "\n",
      "# 12 Gradient out:  [-0.51309011 -0.55806251 -1.88367837 -1.96470216 -0.45232152 -0.73503267\n",
      " -0.92686995 -2.04163755 -1.86859599 -0.49665655]\n",
      "\n",
      "     Weights  out:  [-0.34483815 -0.31365224  0.33683119  0.40001332 -0.39225871 -0.21088055\n",
      " -0.11903972  0.47529957  0.32630097 -0.35698964] [ 0.30469422  0.22394038 -0.22430918  0.32882845  0.44076495  0.13952835\n",
      " -0.08242632  0.26446933  0.36540269  0.35661225]\n",
      "\n",
      "# 13 Gradient out:  [1.02450317 1.07996247 3.11230545 3.21079098 0.95070661 1.31356881\n",
      " 1.6031576  3.3055896  3.09374168 1.00446667]\n",
      "\n",
      "     Weights  out:  [-0.34483815 -0.31365224  0.33683119  0.40001332 -0.39225871 -0.21088055\n",
      " -0.11903972  0.47529957  0.32630097 -0.35698964] [ 0.20207619  0.11232788 -0.60104485 -0.06411198  0.35030065 -0.00747818\n",
      " -0.26780031 -0.14385818 -0.00831651  0.25728094]\n",
      "\n",
      "# 14 Gradient out:  [-0.14175242 -0.15322873 -0.51158856 -0.53210429 -0.12636875 -0.19930763\n",
      " -0.25102017 -0.55141853 -0.50774225 -0.1375792 ]\n",
      "\n",
      "     Weights  out:  [-0.34483815 -0.31365224  0.33683119  0.40001332 -0.39225871 -0.21088055\n",
      " -0.11903972  0.47529957  0.32630097 -0.35698964] [0.40697683 0.32832038 0.02141624 0.57804622 0.54044197 0.25523558\n",
      " 0.05283121 0.51725974 0.61043182 0.45817428]\n",
      "\n",
      "# 15 Gradient out:  [-0.26623201 -0.28900425 -0.99482969 -1.03556494 -0.23568593 -0.38022502\n",
      " -0.48213187 -1.07391987 -0.987197   -0.25794745]\n",
      "\n",
      "     Weights  out:  [-0.34483815 -0.31365224  0.33683119  0.40001332 -0.39225871 -0.21088055\n",
      " -0.11903972  0.47529957  0.32630097 -0.35698964] [ 0.37862634  0.29767463 -0.08090148  0.47162536  0.51516822  0.21537405\n",
      "  0.00262717  0.40697604  0.50888338  0.43065844]\n",
      "\n",
      "# 16 Gradient out:  [-0.5408825  -0.58818475 -1.89877354 -1.98478618 -0.47635847 -0.77029075\n",
      " -0.96045096 -2.06746851 -1.88289838 -0.52350017]\n",
      "\n",
      "     Weights  out:  [-0.34483815 -0.31365224  0.33683119  0.40001332 -0.39225871 -0.21088055\n",
      " -0.11903972  0.47529957  0.32630097 -0.35698964] [ 0.32537994  0.23987378 -0.27986741  0.26451237  0.46803104  0.13932905\n",
      " -0.0937992   0.19219206  0.31144397  0.37906895]\n",
      "\n",
      "# 17 Gradient out:  [1.08792013 1.14118655 2.96734206 3.06338116 1.01593529 1.35906583\n",
      " 1.61949303 3.1579903  2.94949306 1.06850345]\n",
      "\n",
      "     Weights  out:  [-0.34483815 -0.31365224  0.33683119  0.40001332 -0.39225871 -0.21088055\n",
      " -0.11903972  0.47529957  0.32630097 -0.35698964] [ 0.21720344  0.12223683 -0.65962212 -0.13244487  0.37275934 -0.0147291\n",
      " -0.2858894  -0.22130164 -0.0651357   0.27436891]\n",
      "\n",
      "# 18 Gradient out:  [-0.21960194 -0.23910032 -0.84637506 -0.88121771 -0.19347505 -0.31736447\n",
      " -0.40503755 -0.91396941 -0.83984016 -0.21251272]\n",
      "\n",
      "     Weights  out:  [-0.34483815 -0.31365224  0.33683119  0.40001332 -0.39225871 -0.21088055\n",
      " -0.11903972  0.47529957  0.32630097 -0.35698964] [ 0.43478747  0.35047414 -0.06615371  0.48023136  0.5759464   0.25708407\n",
      "  0.03800921  0.41029642  0.52476291  0.4880696 ]\n",
      "\n",
      "# 19 Gradient out:  [-0.49034207 -0.5345196  -1.8231325  -1.90289596 -0.43051668 -0.70764645\n",
      " -0.89417976 -1.97892206 -1.80831451 -0.47417893]\n",
      "\n",
      "     Weights  out:  [-0.34483815 -0.31365224  0.33683119  0.40001332 -0.39225871 -0.21088055\n",
      " -0.11903972  0.47529957  0.32630097 -0.35698964] [ 0.39086708  0.30265408 -0.23542872  0.30398782  0.53725139  0.19361117\n",
      " -0.0429983   0.22750254  0.35679488  0.44556706]\n",
      "\n",
      "# 20 Gradient out:  [0.82972208 0.88138154 2.8230333  2.91423961 0.76138831 1.10144877\n",
      " 1.37801378 3.00127262 2.80574868 0.8111223 ]\n",
      "\n",
      "     Weights  out:  [-0.34483815 -0.31365224  0.33683119  0.40001332 -0.39225871 -0.21088055\n",
      " -0.11903972  0.47529957  0.32630097 -0.35698964] [ 0.29279866  0.19575016 -0.60005522 -0.07659137  0.45114805  0.05208188\n",
      " -0.22183425 -0.16828187 -0.00486802  0.35073128]\n",
      "\n",
      "# 21 Gradient out:  [-0.17968586 -0.19567852 -0.6957049  -0.72426445 -0.15827071 -0.25996509\n",
      " -0.33214427 -0.75108628 -0.69034522 -0.17387353]\n",
      "\n",
      "     Weights  out:  [-0.34483815 -0.31365224  0.33683119  0.40001332 -0.39225871 -0.21088055\n",
      " -0.11903972  0.47529957  0.32630097 -0.35698964] [ 0.45874308  0.37202646 -0.03544856  0.50625655  0.60342571  0.27237164\n",
      "  0.05376851  0.43197265  0.55628171  0.51295573]\n",
      "\n",
      "# 22 Gradient out:  [-0.38913839 -0.42484759 -1.50574941 -1.56984194 -0.34107322 -0.5666998\n",
      " -0.72294282 -1.63043667 -1.49377731 -0.37612028]\n",
      "\n",
      "     Weights  out:  [-0.34483815 -0.31365224  0.33683119  0.40001332 -0.39225871 -0.21088055\n",
      " -0.11903972  0.47529957  0.32630097 -0.35698964] [ 0.42280591  0.33289076 -0.17458954  0.36140367  0.57177157  0.22037862\n",
      " -0.01266035  0.28175539  0.41821267  0.47818103]\n",
      "\n",
      "# 23 Gradient out:  [0.06395071 0.07541324 1.00108192 1.01669364 0.0523396  0.14803752\n",
      " 0.27807285 1.02582443 0.99731389 0.0603948 ]\n",
      "\n",
      "     Weights  out:  [-0.34483815 -0.31365224  0.33683119  0.40001332 -0.39225871 -0.21088055\n",
      " -0.11903972  0.47529957  0.32630097 -0.35698964] [ 0.34497823  0.24792124 -0.47573942  0.04743528  0.50355693  0.10703866\n",
      " -0.15724891 -0.04433194  0.1194572   0.40295697]\n",
      "\n",
      "# 24 Gradient out:  [-0.53979169 -0.58689354 -1.87540021 -1.96124068 -0.4753942  -0.76737459\n",
      " -0.95440354 -2.04405018 -1.85958976 -0.52246017]\n",
      "\n",
      "     Weights  out:  [-0.34483815 -0.31365224  0.33683119  0.40001332 -0.39225871 -0.21088055\n",
      " -0.11903972  0.47529957  0.32630097 -0.35698964] [ 0.35776837  0.26300389 -0.27552304  0.25077401  0.51402485  0.13664616\n",
      " -0.10163434  0.16083295  0.31891998  0.41503593]\n",
      "\n",
      "# 25 Gradient out:  [1.06411881 1.11725849 2.93342599 3.02931906 0.9922425  1.33430027\n",
      " 1.59329803 3.1239255  2.91561845 1.04473884]\n",
      "\n",
      "     Weights  out:  [-0.34483815 -0.31365224  0.33683119  0.40001332 -0.39225871 -0.21088055\n",
      " -0.11903972  0.47529957  0.32630097 -0.35698964] [ 0.24981004  0.14562518 -0.65060308 -0.14147413  0.41894601 -0.01682875\n",
      " -0.29251505 -0.24797709 -0.05299797  0.3105439 ]\n",
      "\n",
      "# 26 Gradient out:  [-0.22289998 -0.24317661 -0.87466484 -0.91089166 -0.19573518 -0.32457588\n",
      " -0.41575291 -0.94492891 -0.86786918 -0.21552848]\n",
      "\n",
      "     Weights  out:  [-0.34483815 -0.31365224  0.33683119  0.40001332 -0.39225871 -0.21088055\n",
      " -0.11903972  0.47529957  0.32630097 -0.35698964] [ 0.4626338   0.36907688 -0.06391788  0.46438968  0.61739451  0.2500313\n",
      "  0.02614456  0.37680801  0.53012572  0.51949166]\n",
      "\n",
      "# 27 Gradient out:  [-0.49930951 -0.54448315 -1.84259239 -1.9243548  -0.43798168 -0.72054749\n",
      " -0.90855975 -2.00256372 -1.82743751 -0.48275763]\n",
      "\n",
      "     Weights  out:  [-0.34483815 -0.31365224  0.33683119  0.40001332 -0.39225871 -0.21088055\n",
      " -0.11903972  0.47529957  0.32630097 -0.35698964] [ 0.4180538   0.32044156 -0.23885085  0.28221135  0.57824747  0.18511612\n",
      " -0.05700602  0.18782223  0.35655188  0.47638597]\n",
      "\n",
      "# 28 Gradient out:  [0.85394369 0.90632904 2.82355858 2.91661404 0.78421561 1.12686535\n",
      " 1.40006335 3.00623358 2.80602339 0.83501442]\n",
      "\n",
      "     Weights  out:  [-0.34483815 -0.31365224  0.33683119  0.40001332 -0.39225871 -0.21088055\n",
      " -0.11903972  0.47529957  0.32630097 -0.35698964] [ 0.3181919   0.21154493 -0.60736933 -0.10265961  0.49065114  0.04100663\n",
      " -0.23871797 -0.21269051 -0.00893562  0.37983444]\n",
      "\n",
      "# 29 Gradient out:  [-0.1909587  -0.20851608 -0.75736073 -0.78870691 -0.16745401 -0.27910327\n",
      " -0.35834011 -0.81812662 -0.75147672 -0.18457847]\n",
      "\n",
      "     Weights  out:  [-0.34483815 -0.31365224  0.33683119  0.40001332 -0.39225871 -0.21088055\n",
      " -0.11903972  0.47529957  0.32630097 -0.35698964] [ 0.48898064  0.39281074 -0.04265761  0.4806632   0.64749426  0.2663797\n",
      "  0.0412947   0.3885562   0.55226906  0.54683733]\n",
      "\n",
      "# 30 Gradient out:  [-0.42590827 -0.46550428 -1.64444972 -1.7157093  -0.3724651  -0.62184556\n",
      " -0.79237189 -1.78333449 -1.6311718  -0.41144986]\n",
      "\n",
      "     Weights  out:  [-0.34483815 -0.31365224  0.33683119  0.40001332 -0.39225871 -0.21088055\n",
      " -0.11903972  0.47529957  0.32630097 -0.35698964] [ 0.4507889   0.35110752 -0.19412976  0.32292181  0.61400346  0.21055904\n",
      " -0.03037333  0.22493088  0.40197372  0.50992163]\n",
      "\n",
      "# 31 Gradient out:  [0.36079813 0.39376566 1.93951275 1.99453594 0.31962645 0.549472\n",
      " 0.76882283 2.04265282 1.92852639 0.34931397]\n",
      "\n",
      "     Weights  out:  [-0.34483815 -0.31365224  0.33683119  0.40001332 -0.39225871 -0.21088055\n",
      " -0.11903972  0.47529957  0.32630097 -0.35698964] [ 0.36560724  0.25800666 -0.5230197  -0.02022005  0.53951044  0.08618993\n",
      " -0.18884771 -0.13173602  0.07573936  0.42763166]\n",
      "\n",
      "# 32 Gradient out:  [-0.35615676 -0.38887153 -1.3872047  -1.44584564 -0.31218124 -0.51921822\n",
      " -0.66347963 -1.5011863  -1.37623764 -0.34423975]\n",
      "\n",
      "     Weights  out:  [-0.34483815 -0.31365224  0.33683119  0.40001332 -0.39225871 -0.21088055\n",
      " -0.11903972  0.47529957  0.32630097 -0.35698964] [ 0.43776687  0.3367598  -0.13511715  0.37868714  0.60343573  0.19608433\n",
      " -0.03508314  0.27679454  0.46144463  0.49749446]\n",
      "\n",
      "# 33 Gradient out:  [-0.18623082 -0.19508008  0.06662261  0.0458965  -0.17059951 -0.20494722\n",
      " -0.17028273  0.0203637   0.0696605  -0.18240436]\n",
      "\n",
      "     Weights  out:  [-0.34483815 -0.31365224  0.33683119  0.40001332 -0.39225871 -0.21088055\n",
      " -0.11903972  0.47529957  0.32630097 -0.35698964] [ 0.36653552  0.25898549 -0.41255809  0.08951801  0.54099948  0.09224069\n",
      " -0.16777907 -0.02344272  0.18619711  0.42864651]\n",
      "\n",
      "# 34 Gradient out:  [-0.12640385 -0.13314038  0.17143998  0.15490597 -0.11391963 -0.13596272\n",
      " -0.09517187  0.13411043  0.17376504 -0.12339237]\n",
      "\n",
      "     Weights  out:  [-0.34483815 -0.31365224  0.33683119  0.40001332 -0.39225871 -0.21088055\n",
      " -0.11903972  0.47529957  0.32630097 -0.35698964] [ 0.32928935  0.21996947 -0.39923357  0.09869731  0.50687958  0.05125124\n",
      " -0.20183561 -0.01936998  0.20012921  0.39216563]\n",
      "\n",
      "# 35 Gradient out:  [-0.19863545 -0.21425654 -0.24559502 -0.27741243 -0.17470077 -0.2556529\n",
      " -0.26306557 -0.3117743  -0.24030065 -0.19246602]\n",
      "\n",
      "     Weights  out:  [-0.34483815 -0.31365224  0.33683119  0.40001332 -0.39225871 -0.21088055\n",
      " -0.11903972  0.47529957  0.32630097 -0.35698964] [ 0.30400858  0.1933414  -0.36494558  0.12967851  0.48409565  0.0240587\n",
      " -0.22086999  0.00745211  0.23488222  0.36748716]\n",
      "\n",
      "# 36 Gradient out:  [0.23948248 0.25475349 1.24100072 1.26447183 0.22196871 0.33902776\n",
      " 0.47764239 1.28299957 1.23594425 0.23442338]\n",
      "\n",
      "     Weights  out:  [-0.34483815 -0.31365224  0.33683119  0.40001332 -0.39225871 -0.21088055\n",
      " -0.11903972  0.47529957  0.32630097 -0.35698964] [ 0.26428149  0.15049009 -0.41406458  0.07419602  0.4491555  -0.02707188\n",
      " -0.2734831  -0.05490275  0.18682209  0.32899396]\n",
      "\n",
      "# 37 Gradient out:  [-0.51752301 -0.56286828 -1.90453271 -1.98616511 -0.45629872 -0.74157117\n",
      " -0.93571133 -2.06357372 -1.88932608 -0.50096057]\n",
      "\n",
      "     Weights  out:  [-0.34483815 -0.31365224  0.33683119  0.40001332 -0.39225871 -0.21088055\n",
      " -0.11903972  0.47529957  0.32630097 -0.35698964] [ 0.31217799  0.20144079 -0.16586444  0.32709039  0.49354924  0.04073367\n",
      " -0.17795462  0.20169716  0.43401094  0.37587863]\n",
      "\n",
      "# 38 Gradient out:  [1.06093985 1.1172199  3.17073256 3.27075192 0.9859924  1.35386378\n",
      " 1.64650533 3.36710963 3.1518929  1.04059722]\n",
      "\n",
      "     Weights  out:  [-0.34483815 -0.31365224  0.33683119  0.40001332 -0.39225871 -0.21088055\n",
      " -0.11903972  0.47529957  0.32630097 -0.35698964] [ 0.20867339  0.08886713 -0.54677098 -0.07014263  0.4022895  -0.10758056\n",
      " -0.36509689 -0.21101758  0.05614572  0.27568652]\n",
      "\n",
      "# 39 Gradient out:  [-0.13769685 -0.14862447 -0.48936308 -0.50890758 -0.12304138 -0.19246757\n",
      " -0.24163416 -0.52732542 -0.48570056 -0.13372208]\n",
      "\n",
      "     Weights  out:  [-0.34483815 -0.31365224  0.33683119  0.40001332 -0.39225871 -0.21088055\n",
      " -0.11903972  0.47529957  0.32630097 -0.35698964] [ 0.42086135  0.31231111  0.08737553  0.58400775  0.59948798  0.16319219\n",
      " -0.03579582  0.46240434  0.6865243   0.48380596]\n",
      "\n",
      "# 40 Gradient out:  [-0.25288055 -0.27413439 -0.93347033 -0.97149211 -0.2243693  -0.35928527\n",
      " -0.45446697 -1.00730409 -0.92634659 -0.24514822]\n",
      "\n",
      "     Weights  out:  [-0.34483815 -0.31365224  0.33683119  0.40001332 -0.39225871 -0.21088055\n",
      " -0.11903972  0.47529957  0.32630097 -0.35698964] [ 0.39332198  0.28258622 -0.01049708  0.48222623  0.5748797   0.12469868\n",
      " -0.08412265  0.35693926  0.58938419  0.45706155]\n",
      "\n",
      "# 41 Gradient out:  [-0.53475996 -0.58199048 -1.9406658  -2.02606808 -0.47070194 -0.76624277\n",
      " -0.96306883 -2.10754778 -1.92482158 -0.51746291]\n",
      "\n",
      "     Weights  out:  [-0.34483815 -0.31365224  0.33683119  0.40001332 -0.39225871 -0.21088055\n",
      " -0.11903972  0.47529957  0.32630097 -0.35698964] [ 0.34274587  0.22775934 -0.19719115  0.28792781  0.53000585  0.05284162\n",
      " -0.17501605  0.15547844  0.40411487  0.4080319 ]\n",
      "\n",
      "# 42 Gradient out:  [1.08174461 1.1366747  3.06002806 3.15857358 1.0078878  1.36347983\n",
      " 1.63772125 3.25488972 3.04162796 1.06177963]\n",
      "\n",
      "     Weights  out:  [-0.34483815 -0.31365224  0.33683119  0.40001332 -0.39225871 -0.21088055\n",
      " -0.11903972  0.47529957  0.32630097 -0.35698964] [ 0.23579388  0.11136124 -0.58532431 -0.1172858   0.43586546 -0.10040693\n",
      " -0.36762981 -0.26603112  0.01915056  0.30453932]\n",
      "\n",
      "# 43 Gradient out:  [-0.18222046 -0.19791948 -0.68790012 -0.71595342 -0.16118474 -0.26096555\n",
      " -0.33168753 -0.74233381 -0.68263864 -0.17651281]\n",
      "\n",
      "     Weights  out:  [-0.34483815 -0.31365224  0.33683119  0.40001332 -0.39225871 -0.21088055\n",
      " -0.11903972  0.47529957  0.32630097 -0.35698964] [ 0.4521428   0.33869618  0.0266813   0.51442891  0.63744302  0.17228904\n",
      " -0.04008556  0.38494683  0.62747615  0.51689525]\n",
      "\n",
      "# 44 Gradient out:  [-0.39081103 -0.42601003 -1.49628284 -1.55941888 -0.3434639  -0.56605513\n",
      " -0.72072418 -1.61905842 -1.4844824  -0.37798398]\n",
      "\n",
      "     Weights  out:  [-0.34483815 -0.31365224  0.33683119  0.40001332 -0.39225871 -0.21088055\n",
      " -0.11903972  0.47529957  0.32630097 -0.35698964] [ 0.41569871  0.29911229 -0.11089872  0.37123823  0.60520607  0.12009593\n",
      " -0.10642307  0.23648007  0.49094842  0.48159268]\n",
      "\n",
      "# 45 Gradient out:  [0.07228342 0.08169269 0.92120539 0.93346672 0.06318097 0.14482171\n",
      " 0.26243366 0.93999551 0.91812159 0.06943717]\n",
      "\n",
      "     Weights  out:  [-0.34483815 -0.31365224  0.33683119  0.40001332 -0.39225871 -0.21088055\n",
      " -0.11903972  0.47529957  0.32630097 -0.35698964] [ 0.3375365   0.21391028 -0.41015529  0.05935445  0.53651329  0.0068849\n",
      " -0.2505679  -0.08733162  0.19405194  0.40599589]\n",
      "\n",
      "# 46 Gradient out:  [-0.53914157 -0.58633842 -1.87716149 -1.96312564 -0.47465111 -0.76725062\n",
      " -0.95466115 -2.04591809 -1.86131952 -0.52177994]\n",
      "\n",
      "     Weights  out:  [-0.34483815 -0.31365224  0.33683119  0.40001332 -0.39225871 -0.21088055\n",
      " -0.11903972  0.47529957  0.32630097 -0.35698964] [ 0.35199319  0.23024882 -0.22591421  0.2460478   0.54914948  0.03584924\n",
      " -0.19808117  0.10066748  0.37767626  0.41988332]\n",
      "\n",
      "# 47 Gradient out:  [1.09088085 1.14373369 2.94866707 3.04403659 1.0193967  1.35956485\n",
      " 1.61698164 3.13809181 2.93095562 1.07160576]\n",
      "\n",
      "     Weights  out:  [-0.34483815 -0.31365224  0.33683119  0.40001332 -0.39225871 -0.21088055\n",
      " -0.11903972  0.47529957  0.32630097 -0.35698964] [ 0.24416487  0.11298114 -0.60134651 -0.14657733  0.45421926 -0.11760088\n",
      " -0.3890134  -0.30851613  0.00541236  0.31552733]\n",
      "\n",
      "# 48 Gradient out:  [-0.23357508 -0.25443882 -0.90328605 -0.94057848 -0.20561106 -0.33813529\n",
      " -0.43181392 -0.97564638 -0.89629342 -0.22598824]\n",
      "\n",
      "     Weights  out:  [-0.34483815 -0.31365224  0.33683119  0.40001332 -0.39225871 -0.21088055\n",
      " -0.11903972  0.47529957  0.32630097 -0.35698964] [ 0.46234105  0.34172787 -0.0116131   0.46222998  0.6580986   0.15431209\n",
      " -0.06561708  0.31910223  0.59160348  0.52984849]\n",
      "\n",
      "# 49 Gradient out:  [-0.51649788 -0.56282621 -1.88995193 -1.97383235 -0.45358062 -0.74320411\n",
      " -0.93544923 -2.05408257 -1.87440918 -0.49951913]\n",
      "\n",
      "     Weights  out:  [-0.34483815 -0.31365224  0.33683119  0.40001332 -0.39225871 -0.21088055\n",
      " -0.11903972  0.47529957  0.32630097 -0.35698964] [ 0.41562603  0.29084011 -0.19227031  0.27411429  0.61697639  0.08668503\n",
      " -0.15197986  0.12397295  0.41234479  0.48465084]\n",
      "\n",
      "Lineal error cuadrático medio (ECM): 1.055149032814128\n",
      "\n",
      "# 0 Gradient out:  [2.49817586 2.12484982 2.50479918 2.37273462 2.39795871 3.00065817\n",
      " 0.72374597 0.71673123 3.09041754 0.7642118 ]\n",
      "\n",
      "     Weights  out:  [ 0.15539786  0.05301091  0.15750253  0.11797171  0.12516907  0.40308117\n",
      " -0.48445816 -0.4920217   0.48839335 -0.44367633] [-0.21848072  0.35601988 -0.03613576  0.02545433  0.12849994 -0.38670103\n",
      "  0.21654791  0.31431356 -0.27103387 -0.15634518]\n",
      "\n",
      "# 1 Gradient out:  [-0.22584528 -0.19496629 -0.22643434 -0.2150226  -0.21715249 -0.27838733\n",
      " -0.06707538 -0.06624777 -0.28920559 -0.07192591]\n",
      "\n",
      "     Weights  out:  [ 0.15539786  0.05301091  0.15750253  0.11797171  0.12516907  0.40308117\n",
      " -0.48445816 -0.4920217   0.48839335 -0.44367633] [ 0.28115445  0.78098984  0.46482408  0.50000125  0.60809168  0.21343061\n",
      "  0.36129711  0.45765981  0.34704964 -0.00350282]\n",
      "\n",
      "# 2 Gradient out:  [-0.34867343 -0.29989024 -0.34960354 -0.33158059 -0.33494499 -0.4314491\n",
      " -0.09818814 -0.09689374 -0.4483996  -0.10578221]\n",
      "\n",
      "     Weights  out:  [ 0.15539786  0.05301091  0.15750253  0.11797171  0.12516907  0.40308117\n",
      " -0.48445816 -0.4920217   0.48839335 -0.44367633] [ 0.2359854   0.74199658  0.41953721  0.45699673  0.56466118  0.15775314\n",
      "  0.34788203  0.44441025  0.28920852 -0.017888  ]\n",
      "\n",
      "# 3 Gradient out:  [-0.65772657 -0.56266914 -0.65953842 -0.62442478 -0.63098025 -0.81857117\n",
      " -0.17040379 -0.16791571 -0.85125102 -0.18502625]\n",
      "\n",
      "     Weights  out:  [ 0.15539786  0.05301091  0.15750253  0.11797171  0.12516907  0.40308117\n",
      " -0.48445816 -0.4920217   0.48839335 -0.44367633] [ 0.16625071  0.68201854  0.3496165   0.39068061  0.49767218  0.07146332\n",
      "  0.3282444   0.4250315   0.1995286  -0.03904444]\n",
      "\n",
      "# 4 Gradient out:  [-1.5716831  -1.33561809 -1.57621488 -1.48862789 -1.50494274 -1.97860396\n",
      " -0.35425344 -0.34798765 -2.06134851 -0.39120184]\n",
      "\n",
      "     Weights  out:  [ 0.15539786  0.05301091  0.15750253  0.11797171  0.12516907  0.40308117\n",
      " -0.48445816 -0.4920217   0.48839335 -0.44367633] [ 0.0347054   0.56948471  0.21770882  0.26579566  0.37147613 -0.09225091\n",
      "  0.29416365  0.39144836  0.0292784  -0.07604969]\n",
      "\n",
      "# 5 Gradient out:  [2.71175922 2.34821047 2.71836912 2.58788805 2.61261556 3.25008314\n",
      " 0.92409141 0.91558522 3.35753274 0.9728729 ]\n",
      "\n",
      "     Weights  out:  [ 0.15539786  0.05301091  0.15750253  0.11797171  0.12516907  0.40308117\n",
      " -0.48445816 -0.4920217   0.48839335 -0.44367633] [-0.27963122  0.30236109 -0.09753416 -0.03192992  0.07048758 -0.48797171\n",
      "  0.22331296  0.32185083 -0.3829913  -0.15429006]\n",
      "\n",
      "# 6 Gradient out:  [-0.28033765 -0.24066002 -0.28109396 -0.26643725 -0.26917349 -0.34760338\n",
      " -0.07667915 -0.07562832 -0.36136594 -0.08284459]\n",
      "\n",
      "     Weights  out:  [ 0.15539786  0.05301091  0.15750253  0.11797171  0.12516907  0.40308117\n",
      " -0.48445816 -0.4920217   0.48839335 -0.44367633] [0.26272062 0.77200318 0.44613967 0.48564769 0.5930107  0.16204492\n",
      " 0.40813124 0.50496788 0.28851524 0.04028452]\n",
      "\n",
      "# 7 Gradient out:  [-0.47670232 -0.40760265 -0.47801893 -0.45249959 -0.4572644  -0.59354347\n",
      " -0.12250295 -0.12069244 -0.61730603 -0.13313855]\n",
      "\n",
      "     Weights  out:  [ 0.15539786  0.05301091  0.15750253  0.11797171  0.12516907  0.40308117\n",
      " -0.48445816 -0.4920217   0.48839335 -0.44367633] [0.20665309 0.72387118 0.38992088 0.43236024 0.539176   0.09252424\n",
      " 0.39279541 0.48984221 0.21624206 0.0237156 ]\n",
      "\n",
      "# 8 Gradient out:  [-1.05876893 -0.90040253 -1.06179066 -1.00325175 -1.01417697 -1.32712525\n",
      " -0.24685924 -0.2427392  -1.38138969 -0.27111213]\n",
      "\n",
      "     Weights  out:  [ 0.15539786  0.05301091  0.15750253  0.11797171  0.12516907  0.40308117\n",
      " -0.48445816 -0.4920217   0.48839335 -0.44367633] [ 0.11131263  0.64235065  0.29431709  0.34186032  0.44772312 -0.02618445\n",
      "  0.36829482  0.46570372  0.09278085 -0.00291211]\n",
      "\n",
      "# 9 Gradient out:  [-0.18450881 -0.18728023 -0.18484922 -0.18122569 -0.18144731 -0.288019\n",
      " -0.07694303 -0.07423969 -0.32336866 -0.09289716]\n",
      "\n",
      "     Weights  out:  [ 0.15539786  0.05301091  0.15750253  0.11797171  0.12516907  0.40308117\n",
      " -0.48445816 -0.4920217   0.48839335 -0.44367633] [-0.10044116  0.46227014  0.08195896  0.14120997  0.24488772 -0.2916095\n",
      "  0.31892297  0.41715588 -0.18349709 -0.05713453]\n",
      "\n",
      "# 10 Gradient out:  [1.04637656 0.85442184 1.0495328  0.98456686 0.99727672 1.23371865\n",
      " 0.21615959 0.21457744 1.25429489 0.22526491]\n",
      "\n",
      "     Weights  out:  [ 0.15539786  0.05301091  0.15750253  0.11797171  0.12516907  0.40308117\n",
      " -0.48445816 -0.4920217   0.48839335 -0.44367633] [-0.13734292  0.4248141   0.04498911  0.10496483  0.20859826 -0.3492133\n",
      "  0.30353437  0.40230795 -0.24817082 -0.07571397]\n",
      "\n",
      "# 11 Gradient out:  [-1.46029603 -1.24030557 -1.46451044 -1.3829922  -1.3981867  -1.83734206\n",
      " -0.3279696  -0.32216437 -1.91391129 -0.3621753 ]\n",
      "\n",
      "     Weights  out:  [ 0.15539786  0.05301091  0.15750253  0.11797171  0.12516907  0.40308117\n",
      " -0.48445816 -0.4920217   0.48839335 -0.44367633] [ 0.07193239  0.59569847  0.25489567  0.30187821  0.40805361 -0.10246957\n",
      "  0.34676628  0.44522343  0.00268816 -0.03066098]\n",
      "\n",
      "# 12 Gradient out:  [2.66362085 2.27492213 2.6706014  2.53210581 2.55845587 3.21180698\n",
      " 0.78634804 0.77825339 3.31494023 0.83296592]\n",
      "\n",
      "     Weights  out:  [ 0.15539786  0.05301091  0.15750253  0.11797171  0.12516907  0.40308117\n",
      " -0.48445816 -0.4920217   0.48839335 -0.44367633] [-0.22012681  0.34763735 -0.03800641  0.02527977  0.12841627 -0.46993798\n",
      "  0.28117236  0.38079056 -0.3800941  -0.10309604]\n",
      "\n",
      "# 13 Gradient out:  [-0.20865541 -0.17920724 -0.20921682 -0.19833789 -0.20036876 -0.25862304\n",
      " -0.05743567 -0.05665309 -0.26886577 -0.06202556]\n",
      "\n",
      "     Weights  out:  [ 0.15539786  0.05301091  0.15750253  0.11797171  0.12516907  0.40308117\n",
      " -0.48445816 -0.4920217   0.48839335 -0.44367633] [0.31259736 0.80262178 0.49611387 0.53170093 0.64010744 0.17242341\n",
      " 0.43844197 0.53644124 0.28289395 0.06349714]\n",
      "\n",
      "# 14 Gradient out:  [-0.31354582 -0.26849371 -0.31440433 -0.2977652  -0.30087184 -0.38981041\n",
      " -0.08246334 -0.08127614 -0.40537225 -0.08943226]\n",
      "\n",
      "     Weights  out:  [ 0.15539786  0.05301091  0.15750253  0.11797171  0.12516907  0.40308117\n",
      " -0.48445816 -0.4920217   0.48839335 -0.44367633] [0.27086628 0.76678033 0.4542705  0.49203335 0.60003369 0.12069881\n",
      " 0.42695484 0.52511062 0.22912079 0.05109203]\n",
      "\n",
      "# 15 Gradient out:  [-0.56353227 -0.48059451 -0.56511233 -0.53448474 -0.54020361 -0.70361902\n",
      " -0.13864999 -0.1364875  -0.73203009 -0.15136066]\n",
      "\n",
      "     Weights  out:  [ 0.15539786  0.05301091  0.15750253  0.11797171  0.12516907  0.40308117\n",
      " -0.48445816 -0.4920217   0.48839335 -0.44367633] [0.20815711 0.71308159 0.39138964 0.43248031 0.53985932 0.04273672\n",
      " 0.41046217 0.50885539 0.14804634 0.03320557]\n",
      "\n",
      "# 16 Gradient out:  [-1.337007   -1.1349767  -1.34087118 -1.26608123 -1.2800287  -1.68169011\n",
      " -0.29879936 -0.29349905 -1.75156018 -0.33001797]\n",
      "\n",
      "     Weights  out:  [ 0.15539786  0.05301091  0.15750253  0.11797171  0.12516907  0.40308117\n",
      " -0.48445816 -0.4920217   0.48839335 -0.44367633] [ 0.09545066  0.61696268  0.27836717  0.32558336  0.4318186  -0.09798708\n",
      "  0.38273217  0.48155789  0.00164032  0.00293344]\n",
      "\n",
      "# 17 Gradient out:  [2.15523964 1.80823228 2.16133982 2.039243   2.06263151 2.60478229\n",
      " 0.52748092 0.52157975 2.68087247 0.56164691]\n",
      "\n",
      "     Weights  out:  [ 0.15539786  0.05301091  0.15750253  0.11797171  0.12516907  0.40308117\n",
      " -0.48445816 -0.4920217   0.48839335 -0.44367633] [-0.17195074  0.38996734  0.01019294  0.07236711  0.17581286 -0.4343251\n",
      "  0.3229723   0.42285808 -0.34867171 -0.06307015]\n",
      "\n",
      "# 18 Gradient out:  [-0.377753   -0.32288837 -0.37879828 -0.35853739 -0.36232048 -0.47052164\n",
      " -0.09650189 -0.09506248 -0.48940517 -0.10495532]\n",
      "\n",
      "     Weights  out:  [ 0.15539786  0.05301091  0.15750253  0.11797171  0.12516907  0.40308117\n",
      " -0.48445816 -0.4920217   0.48839335 -0.44367633] [0.25909719 0.7516138  0.4424609  0.48021571 0.58833916 0.08663136\n",
      " 0.42846848 0.52717403 0.18750278 0.04925923]\n",
      "\n",
      "# 19 Gradient out:  [-0.74733993 -0.63595051 -0.74946254 -0.70832152 -0.71600292 -0.93548097\n",
      " -0.1768292  -0.17393561 -0.97354325 -0.19384908]\n",
      "\n",
      "     Weights  out:  [ 0.15539786  0.05301091  0.15750253  0.11797171  0.12516907  0.40308117\n",
      " -0.48445816 -0.4920217   0.48839335 -0.44367633] [ 0.18354659  0.68703613  0.36670124  0.40850824  0.51587507 -0.00747297\n",
      "  0.4091681   0.50816154  0.08962175  0.02826817]\n",
      "\n",
      "# 20 Gradient out:  [-1.67564003 -1.42565199 -1.68048835 -1.58715414 -1.60448275 -2.12043948\n",
      " -0.37045384 -0.3634293  -2.21299739 -0.41183484]\n",
      "\n",
      "     Weights  out:  [ 0.15539786  0.05301091  0.15750253  0.11797171  0.12516907  0.40308117\n",
      " -0.48445816 -0.4920217   0.48839335 -0.44367633] [ 0.0340786   0.55984602  0.21680874  0.26684393  0.37267448 -0.19456917\n",
      "  0.37380226  0.47337441 -0.1050869  -0.01050165]\n",
      "\n",
      "# 21 Gradient out:  [2.25613468 1.98502529 2.26114224 2.16292912 2.18144737 2.68423042\n",
      " 0.88833801 0.88088118 2.77699975 0.93076182]\n",
      "\n",
      "     Weights  out:  [ 0.15539786  0.05301091  0.15750253  0.11797171  0.12516907  0.40308117\n",
      " -0.48445816 -0.4920217   0.48839335 -0.44367633] [-0.30104941  0.27471563 -0.11928894 -0.0505869   0.05177793 -0.61865706\n",
      "  0.2997115   0.40068855 -0.54768638 -0.09286862]\n",
      "\n",
      "# 22 Gradient out:  [-0.99998647 -0.84732517 -1.00289822 -0.94648104 -0.9570116  -1.2582863\n",
      " -0.21781839 -0.21386043 -1.31042907 -0.24111996]\n",
      "\n",
      "     Weights  out:  [ 0.15539786  0.05301091  0.15750253  0.11797171  0.12516907  0.40308117\n",
      " -0.48445816 -0.4920217   0.48839335 -0.44367633] [ 0.15017753  0.67172068  0.33293951  0.38199893  0.48806741 -0.08181098\n",
      "  0.4773791   0.57686479  0.00771357  0.09328375]\n",
      "\n",
      "# 23 Gradient out:  [-0.68035024 -0.61507533 -0.68195717 -0.65356513 -0.6584457  -0.89284214\n",
      " -0.2287951  -0.22427698 -0.95116446 -0.25517989]\n",
      "\n",
      "     Weights  out:  [ 0.15539786  0.05301091  0.15750253  0.11797171  0.12516907  0.40308117\n",
      " -0.48445816 -0.4920217   0.48839335 -0.44367633] [-0.04981976  0.50225565  0.13235987  0.19270272  0.29666509 -0.33346824\n",
      "  0.43381542  0.5340927  -0.25437224  0.04505976]\n",
      "\n",
      "# 24 Gradient out:  [2.44386083 2.06558982 2.45062212 2.3162149  2.34182575 2.96697766\n",
      " 0.63003441 0.62255235 3.06275004 0.67322586]\n",
      "\n",
      "     Weights  out:  [ 0.15539786  0.05301091  0.15750253  0.11797171  0.12516907  0.40308117\n",
      " -0.48445816 -0.4920217   0.48839335 -0.44367633] [-0.18588981  0.37924058 -0.00403157  0.06198969  0.16497595 -0.51203667\n",
      "  0.3880564   0.48923731 -0.44460513 -0.00597622]\n",
      "\n",
      "# 25 Gradient out:  [-0.28520488 -0.2431081  -0.28600655 -0.27046488 -0.27336723 -0.35626407\n",
      " -0.06955837 -0.06845845 -0.37069979 -0.07601943]\n",
      "\n",
      "     Weights  out:  [ 0.15539786  0.05301091  0.15750253  0.11797171  0.12516907  0.40308117\n",
      " -0.48445816 -0.4920217   0.48839335 -0.44367633] [0.30288235 0.79235855 0.48609286 0.52523267 0.6333411  0.08135887\n",
      " 0.51406328 0.61374778 0.16794488 0.12866895]\n",
      "\n",
      "# 26 Gradient out:  [-0.49157135 -0.41771116 -0.49297763 -0.46571187 -0.47080399 -0.61603014\n",
      " -0.11357859 -0.11166479 -0.64119278 -0.12483211]\n",
      "\n",
      "     Weights  out:  [ 0.15539786  0.05301091  0.15750253  0.11797171  0.12516907  0.40308117\n",
      " -0.48445816 -0.4920217   0.48839335 -0.44367633] [0.24584138 0.74373693 0.42889155 0.4711397  0.57866765 0.01010605\n",
      " 0.50015161 0.60005609 0.09380492 0.11346507]\n",
      "\n",
      "# 27 Gradient out:  [-1.11303684 -0.94272135 -1.11628799 -1.05331472 -1.06506592 -1.40189674\n",
      " -0.23967871 -0.23524845 -1.46027277 -0.26576437]\n",
      "\n",
      "     Weights  out:  [ 0.15539786  0.05301091  0.15750253  0.11797171  0.12516907  0.40308117\n",
      " -0.48445816 -0.4920217   0.48839335 -0.44367633] [ 0.14752711  0.6601947   0.33029602  0.37799733  0.48450685 -0.11309998\n",
      "  0.47743589  0.57772313 -0.03443364  0.08849864]\n",
      "\n",
      "# 28 Gradient out:  [ 0.28542918  0.1905702   0.28675356  0.25741972  0.2634562   0.31000332\n",
      " -0.04534665 -0.04411442  0.29502914 -0.05240542]\n",
      "\n",
      "     Weights  out:  [ 0.15539786  0.05301091  0.15750253  0.11797171  0.12516907  0.40308117\n",
      " -0.48445816 -0.4920217   0.48839335 -0.44367633] [-0.07508026  0.47165043  0.10703842  0.16733438  0.27149367 -0.39347932\n",
      "  0.42950015  0.53067344 -0.32648819  0.03534577]\n",
      "\n",
      "# 29 Gradient out:  [-0.94985451 -0.83666346 -0.9523239  -0.90683022 -0.91496251 -1.22868917\n",
      " -0.26988997 -0.26455355 -1.29803531 -0.30114162]\n",
      "\n",
      "     Weights  out:  [ 0.15539786  0.05301091  0.15750253  0.11797171  0.12516907  0.40308117\n",
      " -0.48445816 -0.4920217   0.48839335 -0.44367633] [-0.01799442  0.50976446  0.16438914  0.21881833  0.32418491 -0.33147866\n",
      "  0.42043082  0.52185056 -0.26748236  0.02486468]\n",
      "\n",
      "# 30 Gradient out:  [2.60147488 2.23578535 2.6081165  2.47695178 2.50181744 3.14087116\n",
      " 0.80574595 0.79725311 3.24816786 0.8544525 ]\n",
      "\n",
      "     Weights  out:  [ 0.15539786  0.05301091  0.15750253  0.11797171  0.12516907  0.40308117\n",
      " -0.48445816 -0.4920217   0.48839335 -0.44367633] [-0.20796532  0.34243177 -0.02607564  0.03745228  0.14119241 -0.57721649\n",
      "  0.36645283  0.46893985 -0.52708942 -0.03536364]\n",
      "\n",
      "# 31 Gradient out:  [-0.30631372 -0.26072878 -0.30718166 -0.2903539  -0.2934966  -0.38319357\n",
      " -0.0728961  -0.07170863 -0.3987864  -0.07987348]\n",
      "\n",
      "     Weights  out:  [ 0.15539786  0.05301091  0.15750253  0.11797171  0.12516907  0.40308117\n",
      " -0.48445816 -0.4920217   0.48839335 -0.44367633] [0.31232965 0.78958884 0.49554766 0.53284264 0.64155589 0.05095774\n",
      " 0.52760202 0.62839047 0.12254415 0.13552686]\n",
      "\n",
      "# 32 Gradient out:  [-0.54690211 -0.4640749  -0.54847907 -0.51790364 -0.52361395 -0.68640269\n",
      " -0.12313869 -0.12099794 -0.71456488 -0.13573069]\n",
      "\n",
      "     Weights  out:  [ 0.15539786  0.05301091  0.15750253  0.11797171  0.12516907  0.40308117\n",
      " -0.48445816 -0.4920217   0.48839335 -0.44367633] [ 0.25106691  0.73744308  0.43411133  0.47477186  0.58285657 -0.02568098\n",
      "  0.5130228   0.61404874  0.04278687  0.11955216]\n",
      "\n",
      "# 33 Gradient out:  [-1.29002408 -1.0920585  -1.29381022 -1.22052848 -1.23419504 -1.62770002\n",
      " -0.27275406 -0.26755777 -1.69617341 -0.30335288]\n",
      "\n",
      "     Weights  out:  [ 0.15539786  0.05301091  0.15750253  0.11797171  0.12516907  0.40308117\n",
      " -0.48445816 -0.4920217   0.48839335 -0.44367633] [ 0.14168649  0.64462811  0.32441551  0.37119113  0.47813378 -0.16296151\n",
      "  0.48839506  0.58984915 -0.10012611  0.09240602]\n",
      "\n",
      "# 34 Gradient out:  [1.73544822 1.42587085 1.74081033 1.63281965 1.6536038  2.11139207\n",
      " 0.31432793 0.30996258 2.16864866 0.33981155]\n",
      "\n",
      "     Weights  out:  [ 0.15539786  0.05301091  0.15750253  0.11797171  0.12516907  0.40308117\n",
      " -0.48445816 -0.4920217   0.48839335 -0.44367633] [-0.11631833  0.4262164   0.06565347  0.12708543  0.23129478 -0.48850152\n",
      "  0.43384425  0.5363376  -0.43936079  0.03173545]\n",
      "\n",
      "# 35 Gradient out:  [-0.70488263 -0.59763323 -0.70692537 -0.66732469 -0.67471965 -0.88566571\n",
      " -0.15606347 -0.15329301 -0.92213469 -0.17236557]\n",
      "\n",
      "     Weights  out:  [ 0.15539786  0.05301091  0.15750253  0.11797171  0.12516907  0.40308117\n",
      " -0.48445816 -0.4920217   0.48839335 -0.44367633] [ 0.23077132  0.71139058  0.41381554  0.45364936  0.56201554 -0.0662231\n",
      "  0.49670983  0.59833012 -0.00563106  0.09969776]\n",
      "\n",
      "# 36 Gradient out:  [-1.64787462 -1.3995554  -1.6526763  -1.56013498 -1.57733265 -2.08614525\n",
      " -0.35505552 -0.34813598 -2.17717381 -0.39577584]\n",
      "\n",
      "     Weights  out:  [ 0.15539786  0.05301091  0.15750253  0.11797171  0.12516907  0.40308117\n",
      " -0.48445816 -0.4920217   0.48839335 -0.44367633] [ 0.08979479  0.59186393  0.27243046  0.32018443  0.42707161 -0.24335625\n",
      "  0.46549714  0.56767152 -0.190058    0.06522464]\n",
      "\n",
      "# 37 Gradient out:  [2.37291748 2.07201737 2.37846175 2.26961427 2.29015375 2.84359088\n",
      " 0.86052643 0.85242777 2.94451619 0.90664196]\n",
      "\n",
      "     Weights  out:  [ 0.15539786  0.05301091  0.15750253  0.11797171  0.12516907  0.40308117\n",
      " -0.48445816 -0.4920217   0.48839335 -0.44367633] [-0.23978013  0.31195285 -0.0581048   0.00815743  0.11160508 -0.6605853\n",
      "  0.39448603  0.49804432 -0.62549276 -0.01393052]\n",
      "\n",
      "# 38 Gradient out:  [-0.70115592 -0.59290207 -0.70321713 -0.66325299 -0.6707166  -0.88334825\n",
      " -0.14759919 -0.1448169  -0.92000209 -0.16397814]\n",
      "\n",
      "     Weights  out:  [ 0.15539786  0.05301091  0.15750253  0.11797171  0.12516907  0.40308117\n",
      " -0.48445816 -0.4920217   0.48839335 -0.44367633] [ 0.23480336  0.72635632  0.41758755  0.46208029  0.56963583 -0.09186712\n",
      "  0.56659132  0.66852987 -0.03658952  0.16739787]\n",
      "\n",
      "# 39 Gradient out:  [-1.63330712 -1.38649707 -1.63808597 -1.54603289 -1.56313251 -2.0709249\n",
      " -0.34582177 -0.3388677  -2.16228649 -0.38671487]\n",
      "\n",
      "     Weights  out:  [ 0.15539786  0.05301091  0.15750253  0.11797171  0.12516907  0.40308117\n",
      " -0.48445816 -0.4920217   0.48839335 -0.44367633] [ 0.09457218  0.60777591  0.27694412  0.32942969  0.43549251 -0.26853677\n",
      "  0.53707148  0.63956649 -0.22058994  0.13460224]\n",
      "\n",
      "# 40 Gradient out:  [2.33043214 2.02878898 2.33599391 2.22683381 2.24742772 2.80369085\n",
      " 0.81236605 0.80418024 2.90560876 0.85895506]\n",
      "\n",
      "     Weights  out:  [ 0.15539786  0.05301091  0.15750253  0.11797171  0.12516907  0.40308117\n",
      " -0.48445816 -0.4920217   0.48839335 -0.44367633] [-0.23208925  0.3304765  -0.05067307  0.02022311  0.12286601 -0.68272175\n",
      "  0.46790713  0.57179295 -0.65304724  0.05725927]\n",
      "\n",
      "# 41 Gradient out:  [-0.72797161 -0.61415306 -0.73013844 -0.68812346 -0.69597049 -0.91932951\n",
      " -0.14627326 -0.14336038 -0.95773484 -0.16342884]\n",
      "\n",
      "     Weights  out:  [ 0.15539786  0.05301091  0.15750253  0.11797171  0.12516907  0.40308117\n",
      " -0.48445816 -0.4920217   0.48839335 -0.44367633] [ 0.23399718  0.73623429  0.41652571  0.46558987  0.57235155 -0.12198358\n",
      "  0.63038034  0.732629   -0.07192549  0.22905028]\n",
      "\n",
      "# 42 Gradient out:  [-1.63566848 -1.39043025 -1.6404441  -1.54865735 -1.56567623 -2.07851588\n",
      " -0.34685857 -0.33969611 -2.17238689 -0.3889239 ]\n",
      "\n",
      "     Weights  out:  [ 0.15539786  0.05301091  0.15750253  0.11797171  0.12516907  0.40308117\n",
      " -0.48445816 -0.4920217   0.48839335 -0.44367633] [ 0.08840286  0.61340368  0.27049802  0.32796518  0.43315745 -0.30584948\n",
      "  0.60112569  0.70395692 -0.26347245  0.19636451]\n",
      "\n",
      "# 43 Gradient out:  [2.20783111 1.92704818 2.21303329 2.11113257 2.13032704 2.65717511\n",
      " 0.7830644  0.77505721 2.75639272 0.82852163]\n",
      "\n",
      "     Weights  out:  [ 0.15539786  0.05301091  0.15750253  0.11797171  0.12516907  0.40308117\n",
      " -0.48445816 -0.4920217   0.48839335 -0.44367633] [-0.23873084  0.33531763 -0.0575908   0.01823371  0.1200222  -0.72155266\n",
      "  0.53175397  0.6360177  -0.69794983  0.11857973]\n",
      "\n",
      "# 44 Gradient out:  [-0.96378149 -0.81148269 -0.96668517 -0.91041451 -0.92091914 -1.2208615\n",
      " -0.18437636 -0.18046097 -1.27251588 -0.2074453 ]\n",
      "\n",
      "     Weights  out:  [ 0.15539786  0.05301091  0.15750253  0.11797171  0.12516907  0.40308117\n",
      " -0.48445816 -0.4920217   0.48839335 -0.44367633] [ 0.20283538  0.72072727  0.38501586  0.44046022  0.54608761 -0.19011763\n",
      "  0.68836685  0.79102914 -0.14667129  0.28428406]\n",
      "\n",
      "# 45 Gradient out:  [-0.8599283  -0.77544038 -0.86192313 -0.82618894 -0.83241449 -1.11438959\n",
      " -0.29603468 -0.29047901 -1.18500869 -0.328175  ]\n",
      "\n",
      "     Weights  out:  [ 0.15539786  0.05301091  0.15750253  0.11797171  0.12516907  0.40308117\n",
      " -0.48445816 -0.4920217   0.48839335 -0.44367633] [ 0.01007909  0.55843073  0.19167883  0.25837732  0.36190378 -0.43428993\n",
      "  0.65149158  0.75493695 -0.40117446  0.242795  ]\n",
      "\n",
      "# 46 Gradient out:  [2.26518727 1.91169374 2.2715738  2.14517315 2.16917606 2.77513539\n",
      " 0.54428161 0.53655319 2.87340117 0.5887542 ]\n",
      "\n",
      "     Weights  out:  [ 0.15539786  0.05301091  0.15750253  0.11797171  0.12516907  0.40308117\n",
      " -0.48445816 -0.4920217   0.48839335 -0.44367633] [-0.16190657  0.40334265  0.0192942   0.09313953  0.19542089 -0.65716785\n",
      "  0.59228464  0.69684115 -0.6381762   0.17716   ]\n",
      "\n",
      "# 47 Gradient out:  [-0.49686154 -0.41867057 -0.49834836 -0.46950553 -0.47489446 -0.62782757\n",
      " -0.09781342 -0.09582608 -0.65403641 -0.10951918]\n",
      "\n",
      "     Weights  out:  [ 0.15539786  0.05301091  0.15750253  0.11797171  0.12516907  0.40308117\n",
      " -0.48445816 -0.4920217   0.48839335 -0.44367633] [ 0.29113088  0.7856814   0.47360896  0.52217416  0.6292561  -0.10214077\n",
      "  0.70114097  0.80415179 -0.06349597  0.29491084]\n",
      "\n",
      "# 48 Gradient out:  [-1.12956859 -0.95147681 -1.13297008 -1.06709809 -1.07938808 -1.43190139\n",
      " -0.21618748 -0.21156022 -1.49291903 -0.24344485]\n",
      "\n",
      "     Weights  out:  [ 0.15539786  0.05301091  0.15750253  0.11797171  0.12516907  0.40308117\n",
      " -0.48445816 -0.4920217   0.48839335 -0.44367633] [ 0.19175857  0.70194729  0.37393929  0.42827306  0.53427721 -0.22770629\n",
      "  0.68157828  0.78498657 -0.19430325  0.273007  ]\n",
      "\n",
      "# 49 Gradient out:  [ 0.41355368  0.27714588  0.41560212  0.37170697  0.38054138  0.48508567\n",
      " -0.09974934 -0.09862639  0.47327696 -0.105733  ]\n",
      "\n",
      "     Weights  out:  [ 0.15539786  0.05301091  0.15750253  0.11797171  0.12516907  0.40308117\n",
      " -0.48445816 -0.4920217   0.48839335 -0.44367633] [-0.03415514  0.51165193  0.14734527  0.21485344  0.31839959 -0.51408657\n",
      "  0.63834079  0.74267453 -0.49288705  0.22431803]\n",
      "\n",
      "Tanh error cuadrático medio (ECM): 0.5437400432583718\n",
      "\n",
      "# 0 Gradient out:  [-0.65330732 -0.88918418 -1.60977224 -0.63381686 -0.42555728 -0.68490426\n",
      " -1.63899683 -1.73382916 -1.19950039 -0.53508214]\n",
      "\n",
      "     Weights  out:  [-0.22887062 -0.10604739  0.23919573 -0.24051849 -0.3962591  -0.21063555\n",
      "  0.25725079  0.32271895  0.0341226  -0.30558745] [ 0.49782837 -0.32909439  0.49337416  0.11585635 -0.28503834  0.19224249\n",
      "  0.19658671  0.32890022  0.15593343  0.40909426]\n",
      "\n",
      "# 1 Gradient out:  [0.79688154 0.99702281 1.70585387 0.78343003 0.66412188 0.81970206\n",
      " 1.72523021 1.7815613  1.32961619 0.72212076]\n",
      "\n",
      "     Weights  out:  [-0.22887062 -0.10604739  0.23919573 -0.24051849 -0.3962591  -0.21063555\n",
      "  0.25725079  0.32271895  0.0341226  -0.30558745] [ 0.36716691 -0.50693122  0.17141971 -0.01090702 -0.3701498   0.05526163\n",
      " -0.13121266 -0.01786561 -0.08396665  0.30207783]\n",
      "\n",
      "# 2 Gradient out:  [-0.60422004 -0.81955792 -1.47837385 -0.58645447 -0.39673368 -0.63302891\n",
      " -1.50500612 -1.59138887 -1.10354495 -0.49650345]\n",
      "\n",
      "     Weights  out:  [-0.22887062 -0.10604739  0.23919573 -0.24051849 -0.3962591  -0.21063555\n",
      "  0.25725079  0.32271895  0.0341226  -0.30558745] [ 0.52654322 -0.30752666  0.51259049  0.14577899 -0.23732542  0.21920205\n",
      "  0.21383338  0.33844665  0.18195659  0.44650198]\n",
      "\n",
      "# 3 Gradient out:  [0.28326347 0.32793744 0.54472792 0.28213345 0.29090853 0.28594143\n",
      " 0.54575771 0.54355652 0.44384092 0.28216334]\n",
      "\n",
      "     Weights  out:  [-0.22887062 -0.10604739  0.23919573 -0.24051849 -0.3962591  -0.21063555\n",
      "  0.25725079  0.32271895  0.0341226  -0.30558745] [ 0.40569921 -0.47143825  0.21691572  0.02848809 -0.31667216  0.09259626\n",
      " -0.08716784  0.02016887 -0.0387524   0.34720129]\n",
      "\n",
      "# 4 Gradient out:  [-0.5472282  -0.76705691 -1.40560837 -0.52800363 -0.31367512 -0.57804177\n",
      " -1.43471221 -1.53149977 -1.03277717 -0.42819524]\n",
      "\n",
      "     Weights  out:  [-0.22887062 -0.10604739  0.23919573 -0.24051849 -0.3962591  -0.21063555\n",
      "  0.25725079  0.32271895  0.0341226  -0.30558745] [ 0.4623519  -0.40585076  0.3258613   0.08491478 -0.25849045  0.14978455\n",
      "  0.0219837   0.12888018  0.05001579  0.40363396]\n",
      "\n",
      "# 5 Gradient out:  [1.36517644 1.76248074 3.08124491 1.33565928 1.04590509 1.41411151\n",
      " 3.12465838 3.25861757 2.35987143 1.19338536]\n",
      "\n",
      "     Weights  out:  [-0.22887062 -0.10604739  0.23919573 -0.24051849 -0.3962591  -0.21063555\n",
      "  0.25725079  0.32271895  0.0341226  -0.30558745] [ 0.35290626 -0.55926214  0.04473963 -0.02068594 -0.32122547  0.0341762\n",
      " -0.26495874 -0.17741977 -0.15653965  0.31799491]\n",
      "\n",
      "# 6 Gradient out:  [-0.27657486 -0.37299219 -0.66969485 -0.26867365 -0.18467607 -0.28940477\n",
      " -0.68152623 -0.71979652 -0.50136925 -0.22878035]\n",
      "\n",
      "     Weights  out:  [-0.22887062 -0.10604739  0.23919573 -0.24051849 -0.3962591  -0.21063555\n",
      "  0.25725079  0.32271895  0.0341226  -0.30558745] [ 0.62594155 -0.20676599  0.66098861  0.24644591 -0.11204445  0.3169985\n",
      "  0.35997294  0.47430374  0.31543464  0.55667198]\n",
      "\n",
      "# 7 Gradient out:  [-0.5353043  -0.72764069 -1.31678916 -0.51945782 -0.35035388 -0.5610079\n",
      " -1.34053942 -1.41753719 -0.98179426 -0.43926515]\n",
      "\n",
      "     Weights  out:  [-0.22887062 -0.10604739  0.23919573 -0.24051849 -0.3962591  -0.21063555\n",
      "  0.25725079  0.32271895  0.0341226  -0.30558745] [ 0.57062658 -0.28136443  0.52704964  0.19271118 -0.14897967  0.25911754\n",
      "  0.22366769  0.33034444  0.21516079  0.51091591]\n",
      "\n",
      "# 8 Gradient out:  [-0.30157938 -0.41997197 -0.72253879 -0.28987288 -0.14782124 -0.31991124\n",
      " -0.74060505 -0.80364353 -0.53367044 -0.22606114]\n",
      "\n",
      "     Weights  out:  [-0.22887062 -0.10604739  0.23919573 -0.24051849 -0.3962591  -0.21063555\n",
      "  0.25725079  0.32271895  0.0341226  -0.30558745] [ 0.46356572 -0.42689257  0.26369181  0.08881962 -0.21905045  0.14691596\n",
      " -0.04444019  0.046837    0.01880193  0.42306288]\n",
      "\n",
      "# 9 Gradient out:  [0.92136914 1.20430403 2.17817453 0.90149751 0.71812697 0.95474619\n",
      " 2.20705584 2.29310238 1.65443948 0.80877351]\n",
      "\n",
      "     Weights  out:  [-0.22887062 -0.10604739  0.23919573 -0.24051849 -0.3962591  -0.21063555\n",
      "  0.25725079  0.32271895  0.0341226  -0.30558745] [ 0.40324984 -0.51088696  0.11918405  0.03084504 -0.24861469  0.08293372\n",
      " -0.1925612  -0.11389171 -0.08793215  0.37785065]\n",
      "\n",
      "# 10 Gradient out:  [-0.4717223  -0.64166661 -1.16298914 -0.45774536 -0.30880053 -0.49440168\n",
      " -1.18393086 -1.25176837 -0.86677444 -0.38707045]\n",
      "\n",
      "     Weights  out:  [-0.22887062 -0.10604739  0.23919573 -0.24051849 -0.3962591  -0.21063555\n",
      "  0.25725079  0.32271895  0.0341226  -0.30558745] [ 0.58752367 -0.27002616  0.55481895  0.21114454 -0.1049893   0.27388295\n",
      "  0.24884997  0.34472877  0.24295574  0.53960535]\n",
      "\n",
      "# 11 Gradient out:  [-0.61746033 -0.84145127 -1.49034345 -0.5977891  -0.37693587 -0.64895654\n",
      " -1.52015352 -1.61963141 -1.11095365 -0.49535596]\n",
      "\n",
      "     Weights  out:  [-0.22887062 -0.10604739  0.23919573 -0.24051849 -0.3962591  -0.21063555\n",
      "  0.25725079  0.32271895  0.0341226  -0.30558745] [ 0.49317921 -0.39835948  0.32222113  0.11959547 -0.1667494   0.17500262\n",
      "  0.01206379  0.0943751   0.06960085  0.46219126]\n",
      "\n",
      "# 12 Gradient out:  [1.37171839 1.77075981 3.090152   1.34189668 1.04716343 1.42109069\n",
      " 3.13406829 3.27008682 2.36710557 1.19765941]\n",
      "\n",
      "     Weights  out:  [-0.22887062 -0.10604739  0.23919573 -0.24051849 -0.3962591  -0.21063555\n",
      "  0.25725079  0.32271895  0.0341226  -0.30558745] [ 3.69687145e-01 -5.66649731e-01  2.41524378e-02  3.76506226e-05\n",
      " -2.42136578e-01  4.52113100e-02 -2.91966912e-01 -2.29551188e-01\n",
      " -1.52589875e-01  3.63120073e-01]\n",
      "\n",
      "# 13 Gradient out:  [-0.28532238 -0.38859702 -0.70647267 -0.27686359 -0.18703341 -0.29905969\n",
      " -0.71913714 -0.76008081 -0.52615529 -0.23417395]\n",
      "\n",
      "     Weights  out:  [-0.22887062 -0.10604739  0.23919573 -0.24051849 -0.3962591  -0.21063555\n",
      "  0.25725079  0.32271895  0.0341226  -0.30558745] [ 0.64403082 -0.21249777  0.64218284  0.26841699 -0.03270389  0.32942945\n",
      "  0.33484675  0.42446618  0.32083124  0.60265196]\n",
      "\n",
      "# 14 Gradient out:  [-0.56388082 -0.77064167 -1.40276965 -0.54680719 -0.36426001 -0.59156194\n",
      " -1.42836947 -1.51145283 -1.04299891 -0.46031343]\n",
      "\n",
      "     Weights  out:  [-0.22887062 -0.10604739  0.23919573 -0.24051849 -0.3962591  -0.21063555\n",
      "  0.25725079  0.32271895  0.0341226  -0.30558745] [ 0.58696635 -0.29021717  0.5008883   0.21304427 -0.07011057  0.26961751\n",
      "  0.19101932  0.27245001  0.21560018  0.55581717]\n",
      "\n",
      "# 15 Gradient out:  [-0.11063865 -0.13325787 -0.1215125  -0.10611202 -0.03347071 -0.11707728\n",
      " -0.12901808 -0.15963451 -0.10549371 -0.07682984]\n",
      "\n",
      "     Weights  out:  [-0.22887062 -0.10604739  0.23919573 -0.24051849 -0.3962591  -0.21063555\n",
      "  0.25725079  0.32271895  0.0341226  -0.30558745] [ 0.47419018 -0.44434551  0.22033437  0.10368283 -0.14296258  0.15130512\n",
      " -0.09465457 -0.02984055  0.0070004   0.46375448]\n",
      "\n",
      "# 16 Gradient out:  [0.16211288 0.2282267  0.52290036 0.15967787 0.16066506 0.16708024\n",
      " 0.52574014 0.52794601 0.38115816 0.15448616]\n",
      "\n",
      "     Weights  out:  [-0.22887062 -0.10604739  0.23919573 -0.24051849 -0.3962591  -0.21063555\n",
      "  0.25725079  0.32271895  0.0341226  -0.30558745] [ 0.45206245 -0.47099708  0.19603187  0.08246043 -0.14965672  0.12788966\n",
      " -0.12045819 -0.06176745 -0.01409834  0.44838851]\n",
      "\n",
      "# 17 Gradient out:  [-0.52866291 -0.71194299 -1.22398826 -0.5119395  -0.31859755 -0.55523784\n",
      " -1.2494928  -1.33601596 -0.91900776 -0.42341786]\n",
      "\n",
      "     Weights  out:  [-0.22887062 -0.10604739  0.23919573 -0.24051849 -0.3962591  -0.21063555\n",
      "  0.25725079  0.32271895  0.0341226  -0.30558745] [ 0.48448503 -0.42535174  0.30061194  0.114396   -0.11752371  0.16130571\n",
      " -0.01531016  0.04382175  0.06213329  0.47928574]\n",
      "\n",
      "# 18 Gradient out:  [1.28524092 1.67084501 2.95461576 1.25672214 0.97812251 1.33256995\n",
      " 2.99652142 3.12547418 2.2533776  1.11961097]\n",
      "\n",
      "     Weights  out:  [-0.22887062 -0.10604739  0.23919573 -0.24051849 -0.3962591  -0.21063555\n",
      "  0.25725079  0.32271895  0.0341226  -0.30558745] [ 0.37875245 -0.56774034  0.05581429  0.0120081  -0.18124322  0.05025814\n",
      " -0.26520872 -0.22338144 -0.12166826  0.39460217]\n",
      "\n",
      "# 19 Gradient out:  [-0.29095099 -0.3971031  -0.72382378 -0.28225648 -0.18993264 -0.30507117\n",
      " -0.73684105 -0.77892341 -0.53848577 -0.23837868]\n",
      "\n",
      "     Weights  out:  [-0.22887062 -0.10604739  0.23919573 -0.24051849 -0.3962591  -0.21063555\n",
      "  0.25725079  0.32271895  0.0341226  -0.30558745] [ 0.63580063 -0.23357134  0.64673745  0.26335253  0.01438128  0.31677213\n",
      "  0.33409556  0.40171339  0.32900726  0.61852436]\n",
      "\n",
      "# 20 Gradient out:  [-0.57888327 -0.79195571 -1.44284503 -0.56127081 -0.37279669 -0.60743198\n",
      " -1.46925755 -1.55502089 -1.07224623 -0.4720047 ]\n",
      "\n",
      "     Weights  out:  [-0.22887062 -0.10604739  0.23919573 -0.24051849 -0.3962591  -0.21063555\n",
      "  0.25725079  0.32271895  0.0341226  -0.30558745] [ 0.57761043 -0.31299196  0.50197269  0.20690124 -0.02360524  0.2557579\n",
      "  0.18672735  0.24592871  0.22131011  0.57084863]\n",
      "\n",
      "# 21 Gradient out:  [0.00359469 0.02662474 0.18550961 0.0046013  0.04189586 0.00294762\n",
      " 0.18321429 0.16891759 0.11991141 0.0166494 ]\n",
      "\n",
      "     Weights  out:  [-0.22887062 -0.10604739  0.23919573 -0.24051849 -0.3962591  -0.21063555\n",
      "  0.25725079  0.32271895  0.0341226  -0.30558745] [ 0.46183378 -0.4713831   0.21340368  0.09464707 -0.09816458  0.1342715\n",
      " -0.10712416 -0.06507547  0.00686086  0.47644769]\n",
      "\n",
      "# 22 Gradient out:  [-0.21454857 -0.27477067 -0.38447185 -0.20712762 -0.10538828 -0.22575046\n",
      " -0.39626093 -0.44029764 -0.30115188 -0.16367586]\n",
      "\n",
      "     Weights  out:  [-0.22887062 -0.10604739  0.23919573 -0.24051849 -0.3962591  -0.21063555\n",
      "  0.25725079  0.32271895  0.0341226  -0.30558745] [ 0.46255272 -0.46605815  0.25050561  0.09556733 -0.08978541  0.13486103\n",
      " -0.0704813  -0.03129195  0.03084314  0.47977757]\n",
      "\n",
      "# 23 Gradient out:  [0.50810435 0.6861533  1.33475645 0.49678852 0.40540046 0.52758665\n",
      " 1.35082187 1.39523888 0.99486102 0.447368  ]\n",
      "\n",
      "     Weights  out:  [-0.22887062 -0.10604739  0.23919573 -0.24051849 -0.3962591  -0.21063555\n",
      "  0.25725079  0.32271895  0.0341226  -0.30558745] [ 0.419643   -0.52101229  0.17361124  0.05414181 -0.11086306  0.08971094\n",
      " -0.14973348 -0.11935148 -0.02938723  0.4470424 ]\n",
      "\n",
      "# 24 Gradient out:  [-0.7168388  -0.97823827 -1.7681972  -0.69494986 -0.45813343 -0.75222397\n",
      " -1.80109915 -1.90859902 -1.31601194 -0.58333478]\n",
      "\n",
      "     Weights  out:  [-0.22887062 -0.10604739  0.23919573 -0.24051849 -0.3962591  -0.21063555\n",
      "  0.25725079  0.32271895  0.0341226  -0.30558745] [ 0.52126387 -0.38378163  0.44056253  0.15349951 -0.02978297  0.19522827\n",
      "  0.12043089  0.1596963   0.16958497  0.536516  ]\n",
      "\n",
      "# 25 Gradient out:  [1.18642255 1.5529533  2.78178275 1.15960432 0.90080347 1.23104096\n",
      " 2.82109958 2.94126354 2.11276253 1.03147611]\n",
      "\n",
      "     Weights  out:  [-0.22887062 -0.10604739  0.23919573 -0.24051849 -0.3962591  -0.21063555\n",
      "  0.25725079  0.32271895  0.0341226  -0.30558745] [ 0.37789611 -0.57942928  0.08692309  0.01450954 -0.12140966  0.04478347\n",
      " -0.23978894 -0.2220235  -0.09361742  0.41984904]\n",
      "\n",
      "# 26 Gradient out:  [-0.31983106 -0.43711826 -0.79794988 -0.31021943 -0.2081162  -0.335439\n",
      " -0.81234157 -0.85887805 -0.59321693 -0.26170192]\n",
      "\n",
      "     Weights  out:  [-0.22887062 -0.10604739  0.23919573 -0.24051849 -0.3962591  -0.21063555\n",
      "  0.25725079  0.32271895  0.0341226  -0.30558745] [ 0.61518062 -0.26883862  0.64327964  0.24643041  0.05875104  0.29099166\n",
      "  0.32443098  0.36622921  0.32893509  0.62614426]\n",
      "\n",
      "# 27 Gradient out:  [-0.64387515 -0.88059956 -1.60111679 -0.62422068 -0.41307767 -0.6757042\n",
      " -1.63061532 -1.72660829 -1.1901367  -0.52439405]\n",
      "\n",
      "     Weights  out:  [-0.22887062 -0.10604739  0.23919573 -0.24051849 -0.3962591  -0.21063555\n",
      "  0.25725079  0.32271895  0.0341226  -0.30558745] [ 0.55121441 -0.35626227  0.48368966  0.18438652  0.0171278   0.22390386\n",
      "  0.16196266  0.19445359  0.2102917   0.57380388]\n",
      "\n",
      "# 28 Gradient out:  [0.56112307 0.767069   1.50467605 0.54763041 0.43426879 0.5841788\n",
      " 1.52396949 1.57853629 1.11516795 0.4875063 ]\n",
      "\n",
      "     Weights  out:  [-0.22887062 -0.10604739  0.23919573 -0.24051849 -0.3962591  -0.21063555\n",
      "  0.25725079  0.32271895  0.0341226  -0.30558745] [ 0.42243938 -0.53238218  0.1634663   0.05954238 -0.06548774  0.08876302\n",
      " -0.1641604  -0.15086806 -0.02773564  0.46892507]\n",
      "\n",
      "# 29 Gradient out:  [-0.68713443 -0.93871225 -1.70167235 -0.66615512 -0.43992608 -0.72107795\n",
      " -1.73318416 -1.83594728 -1.26570307 -0.55937853]\n",
      "\n",
      "     Weights  out:  [-0.22887062 -0.10604739  0.23919573 -0.24051849 -0.3962591  -0.21063555\n",
      "  0.25725079  0.32271895  0.0341226  -0.30558745] [ 0.53466399 -0.37896838  0.46440151  0.16906847  0.02136602  0.20559878\n",
      "  0.14063349  0.1648392   0.19529795  0.56642633]\n",
      "\n",
      "# 30 Gradient out:  [0.93370587 1.24374838 2.30307339 0.91169198 0.70664114 0.97059169\n",
      " 2.33513688 2.43121685 1.73142794 0.8083962 ]\n",
      "\n",
      "     Weights  out:  [-0.22887062 -0.10604739  0.23919573 -0.24051849 -0.3962591  -0.21063555\n",
      "  0.25725079  0.32271895  0.0341226  -0.30558745] [ 0.39723711 -0.56671083  0.12406704  0.03583744 -0.06661919  0.06138319\n",
      " -0.20600334 -0.20235026 -0.05784266  0.45455062]\n",
      "\n",
      "# 31 Gradient out:  [-0.44423963 -0.6086616  -1.11319896 -0.43072325 -0.28677245 -0.46617417\n",
      " -1.13344854 -1.19902296 -0.82656511 -0.36239705]\n",
      "\n",
      "     Weights  out:  [-0.22887062 -0.10604739  0.23919573 -0.24051849 -0.3962591  -0.21063555\n",
      "  0.25725079  0.32271895  0.0341226  -0.30558745] [ 0.58397828 -0.31796116  0.58468172  0.21817584  0.07470904  0.25550153\n",
      "  0.26102404  0.28389311  0.28844292  0.61622986]\n",
      "\n",
      "# 32 Gradient out:  [-0.69347154 -0.93387619 -1.63612824 -0.67253184 -0.43832903 -0.72704897\n",
      " -1.66782295 -1.77332436 -1.22725813 -0.56379736]\n",
      "\n",
      "     Weights  out:  [-0.22887062 -0.10604739  0.23919573 -0.24051849 -0.3962591  -0.21063555\n",
      "  0.25725079  0.32271895  0.0341226  -0.30558745] [ 0.49513035 -0.43969348  0.36204193  0.13203119  0.01735455  0.1622667\n",
      "  0.03433433  0.04408852  0.1231299   0.54375045]\n",
      "\n",
      "# 33 Gradient out:  [1.36799384 1.765137   3.07347378 1.33814708 1.04117675 1.41734266\n",
      " 3.11748006 3.25427814 2.35524396 1.19330441]\n",
      "\n",
      "     Weights  out:  [-0.22887062 -0.10604739  0.23919573 -0.24051849 -0.3962591  -0.21063555\n",
      "  0.25725079  0.32271895  0.0341226  -0.30558745] [ 0.35643605 -0.62646872  0.03481628 -0.00247518 -0.07031126  0.0168569\n",
      " -0.29923026 -0.31057636 -0.12232172  0.43099098]\n",
      "\n",
      "# 34 Gradient out:  [-0.30148448 -0.41463616 -0.76289616 -0.29221807 -0.19387578 -0.31653433\n",
      " -0.7767688  -0.82160473 -0.56533858 -0.24546382]\n",
      "\n",
      "     Weights  out:  [-0.22887062 -0.10604739  0.23919573 -0.24051849 -0.3962591  -0.21063555\n",
      "  0.25725079  0.32271895  0.0341226  -0.30558745] [ 0.63003481 -0.27344132  0.64951104  0.26515423  0.13792409  0.30032544\n",
      "  0.32426575  0.34027927  0.34872707  0.66965186]\n",
      "\n",
      "# 35 Gradient out:  [-0.60849886 -0.8348239  -1.52435141 -0.58972943 -0.38827045 -0.63890184\n",
      " -1.55251596 -1.64412068 -1.13123444 -0.4944471 ]\n",
      "\n",
      "     Weights  out:  [-0.22887062 -0.10604739  0.23919573 -0.24051849 -0.3962591  -0.21063555\n",
      "  0.25725079  0.32271895  0.0341226  -0.30558745] [ 0.56973792 -0.35636855  0.49693181  0.20671062  0.09914893  0.23701857\n",
      "  0.16891199  0.17595833  0.23565935  0.6205591 ]\n",
      "\n",
      "# 36 Gradient out:  [0.23432303 0.35817226 0.84083848 0.22752285 0.1863026  0.24651049\n",
      " 0.85010584 0.87207096 0.59532709 0.20129405]\n",
      "\n",
      "     Weights  out:  [-0.22887062 -0.10604739  0.23919573 -0.24051849 -0.3962591  -0.21063555\n",
      "  0.25725079  0.32271895  0.0341226  -0.30558745] [ 0.44803814 -0.52333333  0.19206152  0.08876473  0.02149485  0.1092382\n",
      " -0.1415912  -0.15286581  0.00941246  0.52166968]\n",
      "\n",
      "# 37 Gradient out:  [-0.68871247 -0.92317161 -1.60428414 -0.66816088 -0.43698767 -0.72162437\n",
      " -1.6354267  -1.73941317 -1.20661315 -0.56112074]\n",
      "\n",
      "     Weights  out:  [-0.22887062 -0.10604739  0.23919573 -0.24051849 -0.3962591  -0.21063555\n",
      "  0.25725079  0.32271895  0.0341226  -0.30558745] [ 0.49490275 -0.45169888  0.36022922  0.1342693   0.05875537  0.1585403\n",
      "  0.02842997  0.02154838  0.12847788  0.56192849]\n",
      "\n",
      "# 38 Gradient out:  [1.35724999 1.75313886 3.05730002 1.3274952  1.03138433 1.40644567\n",
      " 3.10117172 3.23756365 2.34135096 1.18308829]\n",
      "\n",
      "     Weights  out:  [-0.22887062 -0.10604739  0.23919573 -0.24051849 -0.3962591  -0.21063555\n",
      "  0.25725079  0.32271895  0.0341226  -0.30558745] [ 0.35716026 -0.6363332   0.03937239  0.00063713 -0.02864217  0.01421543\n",
      " -0.29865537 -0.32633425 -0.11284475  0.44970434]\n",
      "\n",
      "# 39 Gradient out:  [-0.30404869 -0.41914107 -0.77337071 -0.29462372 -0.1946149  -0.31935631\n",
      " -0.78748048 -0.83307933 -0.57242583 -0.2470723 ]\n",
      "\n",
      "     Weights  out:  [-0.22887062 -0.10604739  0.23919573 -0.24051849 -0.3962591  -0.21063555\n",
      "  0.25725079  0.32271895  0.0341226  -0.30558745] [ 0.62861026 -0.28570543  0.65083239  0.26613617  0.1776347   0.29550456\n",
      "  0.32157897  0.32117848  0.35542544  0.686322  ]\n",
      "\n",
      "# 40 Gradient out:  [-0.61565965 -0.84520021 -1.54389108 -0.59660226 -0.39183793 -0.64652175\n",
      " -1.57249368 -1.66557687 -1.14537144 -0.4998046 ]\n",
      "\n",
      "     Weights  out:  [-0.22887062 -0.10604739  0.23919573 -0.24051849 -0.3962591  -0.21063555\n",
      "  0.25725079  0.32271895  0.0341226  -0.30558745] [ 0.56780052 -0.36953364  0.49615825  0.20721142  0.13871172  0.2316333\n",
      "  0.16408288  0.15456261  0.24094028  0.63690754]\n",
      "\n",
      "# 41 Gradient out:  [0.28911966 0.43761603 0.99889664 0.2803931  0.21961905 0.30446763\n",
      " 1.01102236 1.04199814 0.70955785 0.24467294]\n",
      "\n",
      "     Weights  out:  [-0.22887062 -0.10604739  0.23919573 -0.24051849 -0.3962591  -0.21063555\n",
      "  0.25725079  0.32271895  0.0341226  -0.30558745] [ 0.44466859 -0.53857368  0.18738003  0.08789097  0.06034413  0.10232895\n",
      " -0.15041586 -0.17855276  0.01186599  0.53694662]\n",
      "\n",
      "# 42 Gradient out:  [-0.72566452 -0.97729706 -1.71829643 -0.70393809 -0.46243803 -0.76056369\n",
      " -1.75113457 -1.86004818 -1.28861329 -0.59152994]\n",
      "\n",
      "     Weights  out:  [-0.22887062 -0.10604739  0.23919573 -0.24051849 -0.3962591  -0.21063555\n",
      "  0.25725079  0.32271895  0.0341226  -0.30558745] [ 0.50249252 -0.45105048  0.38715936  0.14396959  0.10426794  0.16322247\n",
      "  0.05178861  0.02984687  0.15377756  0.58588121]\n",
      "\n",
      "# 43 Gradient out:  [1.34694621 1.74158931 3.04147397 1.3172784  1.02193238 1.39599531\n",
      " 3.08521973 3.22124409 2.32782743 1.17327149]\n",
      "\n",
      "     Weights  out:  [-0.22887062 -0.10604739  0.23919573 -0.24051849 -0.3962591  -0.21063555\n",
      "  0.25725079  0.32271895  0.0341226  -0.30558745] [ 0.35735962 -0.64650989  0.04350008  0.00318198  0.01178033  0.01110973\n",
      " -0.2984383  -0.34216277 -0.1039451   0.46757522]\n",
      "\n",
      "# 44 Gradient out:  [-0.30751546 -0.42484506 -0.78594636 -0.29790727 -0.19596635 -0.32312075\n",
      " -0.80033027 -0.84681264 -0.58109951 -0.24943333]\n",
      "\n",
      "     Weights  out:  [-0.22887062 -0.10604739  0.23919573 -0.24051849 -0.3962591  -0.21063555\n",
      "  0.25725079  0.32271895  0.0341226  -0.30558745] [ 0.62674886 -0.29819203  0.65179487  0.26663766  0.21616681  0.2903088\n",
      "  0.31860565  0.30208605  0.36162039  0.70222952]\n",
      "\n",
      "# 45 Gradient out:  [-0.62452123 -0.85769459 -1.56666716 -0.60513602 -0.3965845  -0.65590517\n",
      " -1.59576906 -1.69054357 -1.16206412 -0.50660735]\n",
      "\n",
      "     Weights  out:  [-0.22887062 -0.10604739  0.23919573 -0.24051849 -0.3962591  -0.21063555\n",
      "  0.25725079  0.32271895  0.0341226  -0.30558745] [ 0.56524576 -0.38316104  0.4946056   0.2070562   0.17697354  0.22568464\n",
      "  0.15853959  0.13272352  0.24540049  0.65234285]\n",
      "\n",
      "# 46 Gradient out:  [0.35786557 0.5342652  1.18387757 0.34693333 0.26343939 0.37682264\n",
      " 1.19928417 1.24068275 0.84511374 0.3002698 ]\n",
      "\n",
      "     Weights  out:  [-0.22887062 -0.10604739  0.23919573 -0.24051849 -0.3962591  -0.21063555\n",
      "  0.25725079  0.32271895  0.0341226  -0.30558745] [ 0.44034152 -0.55469996  0.18127217  0.086029    0.09765664  0.09450361\n",
      " -0.16061422 -0.20538519  0.01298766  0.55102138]\n",
      "\n",
      "# 47 Gradient out:  [-0.73783944 -0.99925693 -1.77788276 -0.71556121 -0.47058252 -0.77372072\n",
      " -1.81147807 -1.92222952 -1.32894926 -0.6009821 ]\n",
      "\n",
      "     Weights  out:  [-0.22887062 -0.10604739  0.23919573 -0.24051849 -0.3962591  -0.21063555\n",
      "  0.25725079  0.32271895  0.0341226  -0.30558745] [ 0.51191463 -0.44784692  0.41804768  0.15541566  0.15034452  0.16986814\n",
      "  0.07924261  0.04275136  0.18201041  0.61107534]\n",
      "\n",
      "# 48 Gradient out:  [1.29069542 1.68138545 2.97350738 1.26150802 0.97307943 1.33902097\n",
      " 3.01648711 3.14958989 2.26549403 1.12035718]\n",
      "\n",
      "     Weights  out:  [-0.22887062 -0.10604739  0.23919573 -0.24051849 -0.3962591  -0.21063555\n",
      "  0.25725079  0.32271895  0.0341226  -0.30558745] [ 0.36434674 -0.6476983   0.06247113  0.01230342  0.05622801  0.01512399\n",
      " -0.283053   -0.34169454 -0.08377944  0.49087892]\n",
      "\n",
      "# 49 Gradient out:  [-0.30760097 -0.4255413  -0.7885275  -0.2979432  -0.19548715 -0.32328701\n",
      " -0.80298543 -0.84970459 -0.58261288 -0.24922123]\n",
      "\n",
      "     Weights  out:  [-0.22887062 -0.10604739  0.23919573 -0.24051849 -0.3962591  -0.21063555\n",
      "  0.25725079  0.32271895  0.0341226  -0.30558745] [ 0.62248583 -0.31142121  0.6571726   0.26460502  0.2508439   0.28292819\n",
      "  0.32024442  0.28822343  0.36931936  0.71495036]\n",
      "\n",
      "Lineal error cuadrático medio (ECM): 1.2376370560396794\n",
      "\n",
      "# 0 Gradient out:  [ 0.08532689  0.21771147 -0.30663919  0.05977025 -0.13230412 -0.41881984\n",
      " -0.28468766 -0.47233554 -0.46491672 -0.1343236 ]\n",
      "\n",
      "     Weights  out:  [-0.1968247  -0.3410781   0.17369651 -0.17246529  0.00912579  0.28710621\n",
      "  0.15303022  0.35335097  0.34325777  0.01106103] [-0.438092    0.18772846  0.12554415 -0.48954724  0.43002505  0.46558596\n",
      " -0.23491535  0.41720676 -0.3625889  -0.12336047]\n",
      "\n",
      "# 1 Gradient out:  [0.80652713 0.71195716 1.36147786 0.83159728 1.11225869 1.45289439\n",
      " 1.3377108  1.48499169 1.48071765 1.1157307 ]\n",
      "\n",
      "     Weights  out:  [-0.1968247  -0.3410781   0.17369651 -0.17246529  0.00912579  0.28710621\n",
      "  0.15303022  0.35335097  0.34325777  0.01106103] [-0.42102662  0.23127075  0.06421631 -0.47759319  0.40356423  0.38182199\n",
      " -0.29185288  0.32273965 -0.45557225 -0.15022519]\n",
      "\n",
      "# 2 Gradient out:  [-0.50446874 -0.36842107 -0.99121604 -0.53253754 -0.77396058 -1.10909749\n",
      " -0.96639934 -1.16308081 -1.15554058 -0.77667581]\n",
      "\n",
      "     Weights  out:  [-0.1968247  -0.3410781   0.17369651 -0.17246529  0.00912579  0.28710621\n",
      "  0.15303022  0.35335097  0.34325777  0.01106103] [-0.2597212   0.37366219  0.33651188 -0.31127374  0.62601596  0.67240087\n",
      " -0.02431072  0.61973799 -0.15942872  0.07292095]\n",
      "\n",
      "# 3 Gradient out:  [-0.16511042  0.00379885 -0.68560906 -0.1980284  -0.45390469 -0.82897406\n",
      " -0.65715059 -0.89741298 -0.88785929 -0.45664619]\n",
      "\n",
      "     Weights  out:  [-0.1968247  -0.3410781   0.17369651 -0.17246529  0.00912579  0.28710621\n",
      "  0.15303022  0.35335097  0.34325777  0.01106103] [-0.36061495  0.29997797  0.13826867 -0.41778125  0.47122385  0.45058137\n",
      " -0.21759059  0.38712183 -0.39053683 -0.08241421]\n",
      "\n",
      "# 4 Gradient out:  [1.39657894 1.09479137 2.77448698 1.46677807 2.15740904 3.04988663\n",
      " 2.7102494  3.16128745 3.14612586 2.16559245]\n",
      "\n",
      "     Weights  out:  [-0.1968247  -0.3410781   0.17369651 -0.17246529  0.00912579  0.28710621\n",
      "  0.15303022  0.35335097  0.34325777  0.01106103] [-0.39363703  0.30073774  0.00114686 -0.45738693  0.38044291  0.28478656\n",
      " -0.34902071  0.20763923 -0.56810869 -0.17374345]\n",
      "\n",
      "# 5 Gradient out:  [-0.08969438 -0.07182338 -0.15265736 -0.09334409 -0.12455973 -0.16806157\n",
      " -0.14943628 -0.17521548 -0.17421044 -0.12491017]\n",
      "\n",
      "     Weights  out:  [-0.1968247  -0.3410781   0.17369651 -0.17246529  0.00912579  0.28710621\n",
      "  0.15303022  0.35335097  0.34325777  0.01106103] [-0.11432124  0.51969602  0.55604426 -0.16403131  0.81192472  0.89476389\n",
      "  0.19302917  0.83989672  0.06111648  0.25937504]\n",
      "\n",
      "# 6 Gradient out:  [-0.11830375 -0.09379916 -0.20482995 -0.123315   -0.16621594 -0.22596619\n",
      " -0.20040602 -0.23576501 -0.2343893  -0.16669773]\n",
      "\n",
      "     Weights  out:  [-0.1968247  -0.3410781   0.17369651 -0.17246529  0.00912579  0.28710621\n",
      "  0.15303022  0.35335097  0.34325777  0.01106103] [-0.13226012  0.50533134  0.52551279 -0.18270013  0.78701277  0.86115157\n",
      "  0.16314192  0.80485363  0.02627439  0.23439301]\n",
      "\n",
      "# 7 Gradient out:  [-0.1699458  -0.13277777 -0.30159491 -0.17756137 -0.24284152 -0.33368446\n",
      " -0.29486933 -0.34852402 -0.34644262 -0.24357498]\n",
      "\n",
      "     Weights  out:  [-0.1968247  -0.3410781   0.17369651 -0.17246529  0.00912579  0.28710621\n",
      "  0.15303022  0.35335097  0.34325777  0.01106103] [-0.15592087  0.48657151  0.4845468  -0.20736313  0.75376958  0.81595834\n",
      "  0.12306071  0.75770063 -0.02060347  0.20105346]\n",
      "\n",
      "# 8 Gradient out:  [-0.28136351 -0.21426773 -0.52014432 -0.29515285 -0.41357294 -0.57816084\n",
      " -0.50795958 -0.60488147 -0.60113967 -0.41490427]\n",
      "\n",
      "     Weights  out:  [-0.1968247  -0.3410781   0.17369651 -0.17246529  0.00912579  0.28710621\n",
      "  0.15303022  0.35335097  0.34325777  0.01106103] [-0.18991003  0.46001595  0.42422782 -0.24287541  0.70520128  0.74922145\n",
      "  0.06408685  0.68799582 -0.08989199  0.15233847]\n",
      "\n",
      "# 9 Gradient out:  [-0.57200153 -0.40899786 -1.15575783 -0.60566423 -0.89520137 -1.29707504\n",
      " -1.12599319 -1.36168562 -1.35266836 -0.89845739]\n",
      "\n",
      "     Weights  out:  [-0.1968247  -0.3410781   0.17369651 -0.17246529  0.00912579  0.28710621\n",
      "  0.15303022  0.35335097  0.34325777  0.01106103] [-0.24618273  0.41716241  0.32019895 -0.30190598  0.62248669  0.63358928\n",
      " -0.03750507  0.56701953 -0.21011993  0.06935761]\n",
      "\n",
      "# 10 Gradient out:  [0.53315289 0.498063   0.86407177 0.54570129 0.714931   0.903578\n",
      " 0.85146332 0.91223376 0.91123883 0.71713102]\n",
      "\n",
      "     Weights  out:  [-0.1968247  -0.3410781   0.17369651 -0.17246529  0.00912579  0.28710621\n",
      "  0.15303022  0.35335097  0.34325777  0.01106103] [-0.36058304  0.33536284  0.08904739 -0.42303882  0.44344642  0.37417427\n",
      " -0.26270371  0.2946824  -0.4806536  -0.11033386]\n",
      "\n",
      "# 11 Gradient out:  [-0.72039599 -0.4869772  -1.55187196 -0.76854544 -1.18078555 -1.75422218\n",
      " -1.50933788 -1.84667282 -1.83378872 -1.18541111]\n",
      "\n",
      "     Weights  out:  [-0.1968247  -0.3410781   0.17369651 -0.17246529  0.00912579  0.28710621\n",
      "  0.15303022  0.35335097  0.34325777  0.01106103] [-0.25395246  0.43497544  0.26186174 -0.31389856  0.58643262  0.55488987\n",
      " -0.09241104  0.47712915 -0.29840583  0.03309234]\n",
      "\n",
      "# 12 Gradient out:  [1.4784698  1.14594359 2.91269696 1.55352606 2.27084462 3.21204788\n",
      " 2.84454853 3.33733437 3.32013683 2.27925683]\n",
      "\n",
      "     Weights  out:  [-0.1968247  -0.3410781   0.17369651 -0.17246529  0.00912579  0.28710621\n",
      "  0.15303022  0.35335097  0.34325777  0.01106103] [-0.39803166  0.33758    -0.04851265 -0.46760765  0.35027551  0.20404543\n",
      " -0.39427862  0.10779459 -0.66516358 -0.20398988]\n",
      "\n",
      "# 13 Gradient out:  [-0.11356589 -0.08909144 -0.20017572 -0.11857743 -0.16152323 -0.22129906\n",
      " -0.1957503  -0.23107627 -0.22970437 -0.1620057 ]\n",
      "\n",
      "     Weights  out:  [-0.1968247  -0.3410781   0.17369651 -0.17246529  0.00912579  0.28710621\n",
      "  0.15303022  0.35335097  0.34325777  0.01106103] [-0.1023377   0.56676871  0.53402674 -0.15690244  0.80444443  0.84645501\n",
      "  0.17463109  0.77526146 -0.00113621  0.25186148]\n",
      "\n",
      "# 14 Gradient out:  [-0.16247015 -0.12568086 -0.29302996 -0.17001663 -0.23476117 -0.32480972\n",
      " -0.28636375 -0.33948557 -0.33742813 -0.23548884]\n",
      "\n",
      "     Weights  out:  [-0.1968247  -0.3410781   0.17369651 -0.17246529  0.00912579  0.28710621\n",
      "  0.15303022  0.35335097  0.34325777  0.01106103] [-0.12505088  0.54895043  0.49399159 -0.18061793  0.77213979  0.80219519\n",
      "  0.13548103  0.72904621 -0.04707708  0.21946034]\n",
      "\n",
      "# 15 Gradient out:  [-0.26760077 -0.20217172 -0.50076326 -0.2810579  -0.39669746 -0.55735913\n",
      " -0.48887    -0.58340136 -0.57975568 -0.39799784]\n",
      "\n",
      "     Weights  out:  [-0.1968247  -0.3410781   0.17369651 -0.17246529  0.00912579  0.28710621\n",
      "  0.15303022  0.35335097  0.34325777  0.01106103] [-0.15754491  0.52381425  0.4353856  -0.21462125  0.72518755  0.73723325\n",
      "  0.07820828  0.6611491  -0.11456271  0.17236258]\n",
      "\n",
      "# 16 Gradient out:  [-0.54835874 -0.39255523 -1.10616283 -0.58052487 -0.85719044 -1.24121346\n",
      " -1.077722   -1.30299202 -1.29436747 -0.86030179]\n",
      "\n",
      "     Weights  out:  [-0.1968247  -0.3410781   0.17369651 -0.17246529  0.00912579  0.28710621\n",
      "  0.15303022  0.35335097  0.34325777  0.01106103] [-0.21106506  0.48337991  0.33523295 -0.27083283  0.64584806  0.62576143\n",
      " -0.01956572  0.54446882 -0.23051384  0.09276301]\n",
      "\n",
      "# 17 Gradient out:  [0.24442702 0.2766471  0.31331088 0.24255213 0.28127747 0.29359425\n",
      " 0.31361266 0.27604422 0.27869317 0.28197834]\n",
      "\n",
      "     Weights  out:  [-0.1968247  -0.3410781   0.17369651 -0.17246529  0.00912579  0.28710621\n",
      "  0.15303022  0.35335097  0.34325777  0.01106103] [-0.32073681  0.40486886  0.11400039 -0.38693781  0.47440997  0.37751873\n",
      " -0.23511012  0.28387042 -0.48938734 -0.07929735]\n",
      "\n",
      "# 18 Gradient out:  [-0.48524574 -0.26829751 -1.18269037 -0.52806909 -0.87195487 -1.36747952\n",
      " -1.14542021 -1.45528388 -1.44297471 -0.87570222]\n",
      "\n",
      "     Weights  out:  [-0.1968247  -0.3410781   0.17369651 -0.17246529  0.00912579  0.28710621\n",
      "  0.15303022  0.35335097  0.34325777  0.01106103] [-0.2718514   0.46019828  0.17666256 -0.33842738  0.53066547  0.43623758\n",
      " -0.17238759  0.33907926 -0.4336487  -0.02290168]\n",
      "\n",
      "# 19 Gradient out:  [1.43196892 1.11783868 2.75986763 1.50207274 2.16574399 3.04114368\n",
      " 2.69638329 3.16051589 3.14405786 2.17350131]\n",
      "\n",
      "     Weights  out:  [-0.1968247  -0.3410781   0.17369651 -0.17246529  0.00912579  0.28710621\n",
      "  0.15303022  0.35335097  0.34325777  0.01106103] [-0.36890055  0.40653878 -0.05987551 -0.4440412   0.35627449  0.16274168\n",
      " -0.40147163  0.04802249 -0.72224364 -0.19804213]\n",
      "\n",
      "# 20 Gradient out:  [-0.16721405 -0.12700146 -0.31042415 -0.17547994 -0.2465065  -0.34519614\n",
      " -0.30311922 -0.36121187 -0.3589687  -0.24730523]\n",
      "\n",
      "     Weights  out:  [-0.1968247  -0.3410781   0.17369651 -0.17246529  0.00912579  0.28710621\n",
      "  0.15303022  0.35335097  0.34325777  0.01106103] [-0.08250677  0.63010652  0.49209801 -0.14362665  0.78942329  0.77097042\n",
      "  0.13780503  0.68012567 -0.09343207  0.23665814]\n",
      "\n",
      "# 21 Gradient out:  [-0.28469438 -0.21108619 -0.5477826  -0.29986116 -0.43035563 -0.6115099\n",
      " -0.53437343 -0.64076564 -0.63667362 -0.4318237 ]\n",
      "\n",
      "     Weights  out:  [-0.1968247  -0.3410781   0.17369651 -0.17246529  0.00912579  0.28710621\n",
      "  0.15303022  0.35335097  0.34325777  0.01106103] [-0.11594958  0.60470623  0.43001318 -0.17872264  0.74012199  0.70193119\n",
      "  0.07718119  0.60788329 -0.16522581  0.18719709]\n",
      "\n",
      "# 22 Gradient out:  [-0.61223388 -0.42990531 -1.26408493 -0.64986424 -0.97314297 -1.42212176\n",
      " -1.23082071 -1.49440703 -1.48431888 -0.97677641]\n",
      "\n",
      "     Weights  out:  [-0.1968247  -0.3410781   0.17369651 -0.17246529  0.00912579  0.28710621\n",
      "  0.15303022  0.35335097  0.34325777  0.01106103] [-0.17288845  0.56248899  0.32045666 -0.23869487  0.65405086  0.57962921\n",
      " -0.0296935   0.47973016 -0.29256054  0.10083235]\n",
      "\n",
      "# 23 Gradient out:  [0.91328683 0.7312782  1.8530494  0.95856251 1.43160614 2.02435603\n",
      " 1.81092315 2.08835326 2.07981904 1.43732623]\n",
      "\n",
      "     Weights  out:  [-0.1968247  -0.3410781   0.17369651 -0.17246529  0.00912579  0.28710621\n",
      "  0.15303022  0.35335097  0.34325777  0.01106103] [-0.29533523  0.47650793  0.06763968 -0.36866772  0.45942227  0.29520486\n",
      " -0.27585764  0.18084876 -0.58942431 -0.09452293]\n",
      "\n",
      "# 24 Gradient out:  [-0.28040288 -0.20757817 -0.54075209 -0.2954102  -0.42454727 -0.60380493\n",
      " -0.52748349 -0.63274658 -0.62869869 -0.42600013]\n",
      "\n",
      "     Weights  out:  [-0.1968247  -0.3410781   0.17369651 -0.17246529  0.00912579  0.28710621\n",
      "  0.15303022  0.35335097  0.34325777  0.01106103] [-0.11267786  0.62276357  0.43824956 -0.17695522  0.7457435   0.70007606\n",
      "  0.08632699  0.59851941 -0.1734605   0.19294231]\n",
      "\n",
      "# 25 Gradient out:  [-0.60313782 -0.42381764 -1.24430413 -0.64014743 -0.9581304  -1.39973118\n",
      " -1.21158792 -1.47082745 -1.46090459 -0.96170453]\n",
      "\n",
      "     Weights  out:  [-0.1968247  -0.3410781   0.17369651 -0.17246529  0.00912579  0.28710621\n",
      "  0.15303022  0.35335097  0.34325777  0.01106103] [-0.16875844  0.58124793  0.33009914 -0.23603726  0.66083405  0.57931508\n",
      " -0.01916971  0.47197009 -0.29920024  0.10774229]\n",
      "\n",
      "# 26 Gradient out:  [0.81769691 0.65990057 1.66567071 0.85784611 1.28523312 1.81577739\n",
      " 1.62811541 1.87029078 1.86307715 1.29043213]\n",
      "\n",
      "     Weights  out:  [-0.1968247  -0.3410781   0.17369651 -0.17246529  0.00912579  0.28710621\n",
      "  0.15303022  0.35335097  0.34325777  0.01106103] [-0.289386    0.4964844   0.08123831 -0.36406674  0.46920797  0.29936884\n",
      " -0.26148729  0.1778046  -0.59138116 -0.08459862]\n",
      "\n",
      "# 27 Gradient out:  [-0.35692568 -0.26027446 -0.70296913 -0.37686506 -0.5485128  -0.78670079\n",
      " -0.6853371  -0.82507144 -0.81970866 -0.55044411]\n",
      "\n",
      "     Weights  out:  [-0.1968247  -0.3410781   0.17369651 -0.17246529  0.00912579  0.28710621\n",
      "  0.15303022  0.35335097  0.34325777  0.01106103] [-0.12584662  0.62846452  0.41437246 -0.19249752  0.72625459  0.66252432\n",
      "  0.06413579  0.55186276 -0.21876573  0.17348781]\n",
      "\n",
      "# 28 Gradient out:  [-0.76057159 -0.51250913 -1.63422097 -0.81145591 -1.24437928 -1.84873541\n",
      " -1.58934127 -1.94733316 -1.93356979 -1.24922404]\n",
      "\n",
      "     Weights  out:  [-0.1968247  -0.3410781   0.17369651 -0.17246529  0.00912579  0.28710621\n",
      "  0.15303022  0.35335097  0.34325777  0.01106103] [-0.19723176  0.57640963  0.27377863 -0.26787053  0.61655203  0.50518416\n",
      " -0.07293163  0.38684847 -0.38270746  0.06339898]\n",
      "\n",
      "# 29 Gradient out:  [1.40432428 1.10323061 2.66036779 1.47100946 2.098483   2.92899175\n",
      " 2.60008262 3.04408093 3.02816372 2.10580171]\n",
      "\n",
      "     Weights  out:  [-0.1968247  -0.3410781   0.17369651 -0.17246529  0.00912579  0.28710621\n",
      "  0.15303022  0.35335097  0.34325777  0.01106103] [-0.34934607  0.4739078  -0.05306556 -0.43016171  0.36767618  0.13543708\n",
      " -0.39079988 -0.00261816 -0.76942142 -0.18644582]\n",
      "\n",
      "# 30 Gradient out:  [-0.20911311 -0.15525672 -0.40164628 -0.22020958 -0.31571041 -0.44827012\n",
      " -0.39183523 -0.46967944 -0.46668428 -0.31678496]\n",
      "\n",
      "     Weights  out:  [-0.1968247  -0.3410781   0.17369651 -0.17246529  0.00912579  0.28710621\n",
      "  0.15303022  0.35335097  0.34325777  0.01106103] [-0.06848122  0.69455392  0.47900799 -0.13595982  0.78737278  0.72123543\n",
      "  0.12921664  0.60619803 -0.16378868  0.23471452]\n",
      "\n",
      "# 31 Gradient out:  [-0.40379185 -0.29045584 -0.80998285 -0.42719172 -0.62867772 -0.90821068\n",
      " -0.78928862 -0.95317119 -0.94689073 -0.63094484]\n",
      "\n",
      "     Weights  out:  [-0.1968247  -0.3410781   0.17369651 -0.17246529  0.00912579  0.28710621\n",
      "  0.15303022  0.35335097  0.34325777  0.01106103] [-0.11030384  0.66350258  0.39867874 -0.18000174  0.72423069  0.63158141\n",
      "  0.0508496   0.51226214 -0.25712553  0.17135753]\n",
      "\n",
      "# 32 Gradient out:  [-0.75267591 -0.49284979 -1.63692112 -0.80515019 -1.24256746 -1.86015971\n",
      " -1.59086485 -1.96430879 -1.94972715 -1.24741869]\n",
      "\n",
      "     Weights  out:  [-0.1968247  -0.3410781   0.17369651 -0.17246529  0.00912579  0.28710621\n",
      "  0.15303022  0.35335097  0.34325777  0.01106103] [-0.19106221  0.60541141  0.23668217 -0.26544008  0.59849515  0.44993927\n",
      " -0.10700813  0.3216279  -0.44650368  0.04516856]\n",
      "\n",
      "# 33 Gradient out:  [1.21696664 0.97319105 2.20194097 1.26998693 1.7614913  2.41756555\n",
      " 2.15420823 2.51201968 2.49886303 1.76719399]\n",
      "\n",
      "     Weights  out:  [-0.1968247  -0.3410781   0.17369651 -0.17246529  0.00912579  0.28710621\n",
      "  0.15303022  0.35335097  0.34325777  0.01106103] [-0.34159739  0.50684145 -0.09070206 -0.42647012  0.34998166  0.07790733\n",
      " -0.4251811  -0.07123386 -0.83644911 -0.20431518]\n",
      "\n",
      "# 34 Gradient out:  [-0.55884336 -0.39106363 -1.15892032 -0.59347147 -0.89108424 -1.30433835\n",
      " -1.12830742 -1.37086964 -1.36158197 -0.89442991]\n",
      "\n",
      "     Weights  out:  [-0.1968247  -0.3410781   0.17369651 -0.17246529  0.00912579  0.28710621\n",
      "  0.15303022  0.35335097  0.34325777  0.01106103] [-0.09820406  0.70147966  0.34968614 -0.17247273  0.70227992  0.56142044\n",
      "  0.00566055  0.43117008 -0.3366765   0.14912362]\n",
      "\n",
      "# 35 Gradient out:  [0.32950669 0.28127663 0.7683905  0.34669141 0.57067704 0.82306224\n",
      " 0.75127517 0.83427277 0.83312439 0.57355857]\n",
      "\n",
      "     Weights  out:  [-0.1968247  -0.3410781   0.17369651 -0.17246529  0.00912579  0.28710621\n",
      "  0.15303022  0.35335097  0.34325777  0.01106103] [-0.20997274  0.62326694  0.11790208 -0.29116703  0.52406307  0.30055277\n",
      " -0.22000093  0.15699615 -0.6089929  -0.02976236]\n",
      "\n",
      "# 36 Gradient out:  [-0.78483573 -0.52842523 -1.67532276 -0.83707097 -1.27805239 -1.89638414\n",
      " -1.62934034 -1.99874096 -1.9844242  -1.28297112]\n",
      "\n",
      "     Weights  out:  [-0.1968247  -0.3410781   0.17369651 -0.17246529  0.00912579  0.28710621\n",
      "  0.15303022  0.35335097  0.34325777  0.01106103] [-0.1440714   0.67952226  0.27158018 -0.22182875  0.63819848  0.46516522\n",
      " -0.0697459   0.3238507  -0.44236802  0.08494935]\n",
      "\n",
      "# 37 Gradient out:  [1.3396888  1.06692674 2.44792006 1.3991884  1.95231783 2.68950557\n",
      " 2.39431366 2.79498304 2.78030381 1.95874222]\n",
      "\n",
      "     Weights  out:  [-0.1968247  -0.3410781   0.17369651 -0.17246529  0.00912579  0.28710621\n",
      "  0.15303022  0.35335097  0.34325777  0.01106103] [-0.30103854  0.57383722 -0.06348438 -0.38924294  0.382588    0.08588839\n",
      " -0.39561397 -0.07589749 -0.83925286 -0.17164487]\n",
      "\n",
      "# 38 Gradient out:  [-0.33335081 -0.23759953 -0.67725286 -0.35314144 -0.52374578 -0.76027948\n",
      " -0.65974538 -0.79823873 -0.79293777 -0.52566631]\n",
      "\n",
      "     Weights  out:  [-0.1968247  -0.3410781   0.17369651 -0.17246529  0.00912579  0.28710621\n",
      "  0.15303022  0.35335097  0.34325777  0.01106103] [-0.03310078  0.78722256  0.42609964 -0.10940526  0.77305157  0.6237895\n",
      "  0.08324876  0.48309912 -0.2831921   0.22010357]\n",
      "\n",
      "# 39 Gradient out:  [-0.76060425 -0.51708107 -1.61340604 -0.8103907  -1.23289623 -1.82365054\n",
      " -1.56952569 -1.92070174 -1.90713175 -1.23761982]\n",
      "\n",
      "     Weights  out:  [-0.1968247  -0.3410781   0.17369651 -0.17246529  0.00912579  0.28710621\n",
      "  0.15303022  0.35335097  0.34325777  0.01106103] [-0.09977095  0.73970266  0.29064906 -0.18003355  0.66830241  0.47173361\n",
      " -0.04870031  0.32345137 -0.44177965  0.11497031]\n",
      "\n",
      "# 40 Gradient out:  [1.45344627 1.14363248 2.73611757 1.52173533 2.16236985 3.01185465\n",
      " 2.67443861 3.13076682 3.11427909 2.16983485]\n",
      "\n",
      "     Weights  out:  [-0.1968247  -0.3410781   0.17369651 -0.17246529  0.00912579  0.28710621\n",
      "  0.15303022  0.35335097  0.34325777  0.01106103] [-0.2518918   0.63628644 -0.03203215 -0.34211169  0.42172316  0.1070035\n",
      " -0.36260545 -0.06068897 -0.823206   -0.13255365]\n",
      "\n",
      "# 41 Gradient out:  [-0.15283745 -0.1112015  -0.30229606 -0.16143597 -0.2355824  -0.33837999\n",
      " -0.29468953 -0.35490344 -0.35259395 -0.23641729]\n",
      "\n",
      "     Weights  out:  [-0.1968247  -0.3410781   0.17369651 -0.17246529  0.00912579  0.28710621\n",
      "  0.15303022  0.35335097  0.34325777  0.01106103] [ 0.03879746  0.86501294  0.51519137 -0.03776462  0.85419713  0.70937443\n",
      "  0.17228227  0.56546439 -0.20035018  0.30141332]\n",
      "\n",
      "# 42 Gradient out:  [-0.2620438  -0.18704299 -0.53170793 -0.27755213 -0.41133618 -0.59675164\n",
      " -0.51798646 -0.62648016 -0.62232853 -0.41284269]\n",
      "\n",
      "     Weights  out:  [-0.1968247  -0.3410781   0.17369651 -0.17246529  0.00912579  0.28710621\n",
      "  0.15303022  0.35335097  0.34325777  0.01106103] [ 0.00822997  0.84277264  0.45473216 -0.07005181  0.80708065  0.64169843\n",
      "  0.11334437  0.4944837  -0.27086897  0.25412986]\n",
      "\n",
      "# 43 Gradient out:  [-0.585722   -0.40433105 -1.23289202 -0.62312049 -0.94404783 -1.39004221\n",
      " -1.1998418  -1.46200717 -1.45195962 -0.94765312]\n",
      "\n",
      "     Weights  out:  [-0.1968247  -0.3410781   0.17369651 -0.17246529  0.00912579  0.28710621\n",
      "  0.15303022  0.35335097  0.34325777  0.01106103] [-0.04417879  0.80536404  0.34839057 -0.12556224  0.72481342  0.5223481\n",
      "  0.00974708  0.36918767 -0.39533468  0.17156132]\n",
      "\n",
      "# 44 Gradient out:  [0.59789189 0.44994739 1.44150169 0.63703028 1.06282326 1.5851293\n",
      " 1.4046389  1.63424228 1.62790427 1.06803465]\n",
      "\n",
      "     Weights  out:  [-0.1968247  -0.3410781   0.17369651 -0.17246529  0.00912579  0.28710621\n",
      "  0.15303022  0.35335097  0.34325777  0.01106103] [-0.16132319  0.72449783  0.10181217 -0.25018634  0.53600385  0.24433966\n",
      " -0.23022128  0.07678623 -0.6857266  -0.0179693 ]\n",
      "\n",
      "# 45 Gradient out:  [-0.47379725 -0.33069691 -0.98685887 -0.50336317 -0.75785192 -1.1109387\n",
      " -0.96071178 -1.1676556  -1.15973868 -0.76071466]\n",
      "\n",
      "     Weights  out:  [-0.1968247  -0.3410781   0.17369651 -0.17246529  0.00912579  0.28710621\n",
      "  0.15303022  0.35335097  0.34325777  0.01106103] [-0.04174481  0.81448731  0.39011251 -0.12278028  0.7485685   0.56136552\n",
      "  0.0507065   0.40363469 -0.36014575  0.19563763]\n",
      "\n",
      "# 46 Gradient out:  [-0.4486583  -0.28990962 -0.84190616 -0.47661967 -0.66757167 -0.97085886\n",
      " -0.81843883 -1.03920653 -1.02936458 -0.6694842 ]\n",
      "\n",
      "     Weights  out:  [-0.1968247  -0.3410781   0.17369651 -0.17246529  0.00912579  0.28710621\n",
      "  0.15303022  0.35335097  0.34325777  0.01106103] [-0.13650426  0.74834793  0.19274073 -0.22345292  0.59699812  0.33917778\n",
      " -0.14143586  0.17010357 -0.59209348  0.04349469]\n",
      "\n",
      "# 47 Gradient out:  [1.43587403 1.10668063 2.85575514 1.51011283 2.22031416 3.15189964\n",
      " 2.78833887 3.27614338 3.25906051 2.22864702]\n",
      "\n",
      "     Weights  out:  [-0.1968247  -0.3410781   0.17369651 -0.17246529  0.00912579  0.28710621\n",
      "  0.15303022  0.35335097  0.34325777  0.01106103] [-0.22623592  0.690366    0.0243595  -0.31877685  0.46348379  0.14500601\n",
      " -0.30512362 -0.03773774 -0.7979664  -0.09040215]\n",
      "\n",
      "# 48 Gradient out:  [-0.08615994 -0.0637552  -0.16636654 -0.09077884 -0.13056606 -0.18576668\n",
      " -0.16228184 -0.19467108 -0.19342536 -0.1310139 ]\n",
      "\n",
      "     Weights  out:  [-0.1968247  -0.3410781   0.17369651 -0.17246529  0.00912579  0.28710621\n",
      "  0.15303022  0.35335097  0.34325777  0.01106103] [ 0.06093888  0.91170213  0.59551053 -0.01675428  0.90754662  0.77538594\n",
      "  0.25254415  0.61749094 -0.1461543   0.35532726]\n",
      "\n",
      "# 49 Gradient out:  [-0.11785344 -0.08633249 -0.23088447 -0.12435854 -0.18043159 -0.25819258\n",
      " -0.22513054 -0.27070927 -0.26895913 -0.1810629 ]\n",
      "\n",
      "     Weights  out:  [-0.1968247  -0.3410781   0.17369651 -0.17246529  0.00912579  0.28710621\n",
      "  0.15303022  0.35335097  0.34325777  0.01106103] [ 0.0437069   0.89895109  0.56223722 -0.03491005  0.88143341  0.7382326\n",
      "  0.22008778  0.57855673 -0.18483937  0.32912448]\n",
      "\n",
      "Tanh error cuadrático medio (ECM): 1.102836486712915\n",
      "\n",
      "# 0 Gradient out:  [1.04037991 0.9458115  1.04589562 1.37465432 0.66187633 1.36446953\n",
      " 0.66887171 0.60494256 1.37169545 1.26650901]\n",
      "\n",
      "     Weights  out:  [ 0.02725732 -0.0265906   0.03043406  0.4083257  -0.26878777  0.37180701\n",
      " -0.25655293 -0.42941717  0.3972638   0.1905144 ] [-0.45210365  0.07121779  0.14673902  0.17651087  0.2990357   0.31365279\n",
      " -0.02112758 -0.37293148  0.01566331 -0.47974212]\n",
      "\n",
      "# 1 Gradient out:  [-0.93375776 -0.84270216 -0.93911051 -1.41713885 -0.48806051 -1.38849577\n",
      " -0.50221381 -0.3432134  -1.40883279 -1.1888287 ]\n",
      "\n",
      "     Weights  out:  [ 0.02725732 -0.0265906   0.03043406  0.4083257  -0.26878777  0.37180701\n",
      " -0.25655293 -0.42941717  0.3972638   0.1905144 ] [-0.24402767  0.26038009  0.35591814  0.45144173  0.43141097  0.5865467\n",
      "  0.11264677 -0.25194297  0.2900024  -0.22644032]\n",
      "\n",
      "# 2 Gradient out:  [0.78862094 0.7152179  0.79289384 1.01466944 0.5112686  1.01085983\n",
      " 0.51517544 0.48621974 1.01357021 0.95664798]\n",
      "\n",
      "     Weights  out:  [ 0.02725732 -0.0265906   0.03043406  0.4083257  -0.26878777  0.37180701\n",
      " -0.25655293 -0.42941717  0.3972638   0.1905144 ] [-0.43077922  0.09183966  0.16809604  0.16801396  0.33379887  0.30884754\n",
      "  0.012204   -0.32058565  0.00823584 -0.46420606]\n",
      "\n",
      "# 3 Gradient out:  [-1.18237595 -1.06086283 -1.18952089 -1.83317528 -0.5844601  -1.79441455\n",
      " -0.6036176  -0.38831892 -1.82194214 -1.52425435]\n",
      "\n",
      "     Weights  out:  [ 0.02725732 -0.0265906   0.03043406  0.4083257  -0.26878777  0.37180701\n",
      " -0.25655293 -0.42941717  0.3972638   0.1905144 ] [-0.27305503  0.23488324  0.32667481  0.37094785  0.43605259  0.51101951\n",
      "  0.11523909 -0.2233417   0.21094988 -0.27287646]\n",
      "\n",
      "# 4 Gradient out:  [2.36683376 2.13109272 2.38064502 3.43779689 1.30385959 3.38460137\n",
      " 1.3322048  1.0292561  3.4223586  2.98541493]\n",
      "\n",
      "     Weights  out:  [ 0.02725732 -0.0265906   0.03043406  0.4083257  -0.26878777  0.37180701\n",
      " -0.25655293 -0.42941717  0.3972638   0.1905144 ] [-0.50953022  0.02271067  0.08877063  0.00431279  0.31916057  0.1521366\n",
      " -0.00548443 -0.30100549 -0.15343855 -0.57772733]\n",
      "\n",
      "# 5 Gradient out:  [-0.14459402 -0.13301762 -0.1452745  -0.20625651 -0.08793056 -0.20253497\n",
      " -0.08973638 -0.06925296 -0.20517437 -0.17700397]\n",
      "\n",
      "     Weights  out:  [ 0.02725732 -0.0265906   0.03043406  0.4083257  -0.26878777  0.37180701\n",
      " -0.25655293 -0.42941717  0.3972638   0.1905144 ] [-0.03616347  0.44892922  0.56489963  0.69187217  0.57993248  0.82905687\n",
      "  0.26095653 -0.09515427  0.53103317  0.01935565]\n",
      "\n",
      "# 6 Gradient out:  [-0.19953019 -0.18305753 -0.20049847 -0.28714202 -0.11894728 -0.28187196\n",
      " -0.12151143 -0.09246997 -0.28561012 -0.24563008]\n",
      "\n",
      "     Weights  out:  [ 0.02725732 -0.0265906   0.03043406  0.4083257  -0.26878777  0.37180701\n",
      " -0.25655293 -0.42941717  0.3972638   0.1905144 ] [-0.06508227  0.42232569  0.53584473  0.65062087  0.56234637  0.78854988\n",
      "  0.24300925 -0.10900486  0.4899983  -0.01604514]\n",
      "\n",
      "# 7 Gradient out:  [-0.30822393 -0.28165759 -0.3097855  -0.44925244 -0.17835187 -0.44080889\n",
      " -0.18247625 -0.13586444 -0.44679931 -0.38253935]\n",
      "\n",
      "     Weights  out:  [ 0.02725732 -0.0265906   0.03043406  0.4083257  -0.26878777  0.37180701\n",
      " -0.25655293 -0.42941717  0.3972638   0.1905144 ] [-0.10498831  0.38571419  0.49574504  0.59319247  0.53855692  0.73217549\n",
      "  0.21870697 -0.12749885  0.43287627 -0.06517116]\n",
      "\n",
      "# 8 Gradient out:  [-0.5784857  -0.52511411 -0.5816229  -0.86132178 -0.31766054 -0.84450003\n",
      " -0.32592855 -0.23279837 -0.85643884 -0.72777318]\n",
      "\n",
      "     Weights  out:  [ 0.02725732 -0.0265906   0.03043406  0.4083257  -0.26878777  0.37180701\n",
      " -0.25655293 -0.42941717  0.3972638   0.1905144 ] [-0.1666331   0.32938267  0.43378794  0.50334198  0.50288654  0.64401371\n",
      "  0.18221172 -0.15467174  0.34351641 -0.14167903]\n",
      "\n",
      "# 9 Gradient out:  [-1.25023353 -1.11989772 -1.257899   -1.95493732 -0.60553551 -1.91259652\n",
      " -0.62638967 -0.39147513 -1.94266705 -1.61848901]\n",
      "\n",
      "     Weights  out:  [ 0.02725732 -0.0265906   0.03043406  0.4083257  -0.26878777  0.37180701\n",
      " -0.25655293 -0.42941717  0.3972638   0.1905144 ] [-0.28233024  0.22435985  0.31746336  0.33107762  0.43935443  0.4751137\n",
      "  0.11702601 -0.20123141  0.17222864 -0.28723366]\n",
      "\n",
      "# 10 Gradient out:  [2.2305603  2.01823923 2.24300519 3.22137009 1.26120691 3.16974054\n",
      " 1.28790327 0.99754044 3.20635227 2.79298823]\n",
      "\n",
      "     Weights  out:  [ 0.02725732 -0.0265906   0.03043406  0.4083257  -0.26878777  0.37180701\n",
      " -0.25655293 -0.42941717  0.3972638   0.1905144 ] [-5.32376943e-01  3.80302246e-04  6.58835562e-02 -5.99098428e-02\n",
      "  3.18247331e-01  9.25944013e-02 -8.25192611e-03 -2.79526441e-01\n",
      " -2.16304767e-01 -6.10931464e-01]\n",
      "\n",
      "# 11 Gradient out:  [-0.28299131 -0.25821888 -0.28444742 -0.41436656 -0.16194323 -0.40651408\n",
      " -0.16578342 -0.1224128  -0.41208542 -0.35226575]\n",
      "\n",
      "     Weights  out:  [ 0.02725732 -0.0265906   0.03043406  0.4083257  -0.26878777  0.37180701\n",
      " -0.25655293 -0.42941717  0.3972638   0.1905144 ] [-0.08626488  0.40402815  0.51448459  0.58436418  0.57048871  0.72654251\n",
      "  0.24932873 -0.08001835  0.42496569 -0.05233382]\n",
      "\n",
      "# 12 Gradient out:  [-0.51326584 -0.46573563 -0.51605964 -0.7649768  -0.28107683 -0.7500131\n",
      " -0.28843191 -0.20559044 -0.760633   -0.64617272]\n",
      "\n",
      "     Weights  out:  [ 0.02725732 -0.0265906   0.03043406  0.4083257  -0.26878777  0.37180701\n",
      " -0.25655293 -0.42941717  0.3972638   0.1905144 ] [-0.14286315  0.35238437  0.45759511  0.50149086  0.53810007  0.64523969\n",
      "  0.21617205 -0.10450091  0.3425486  -0.12278697]\n",
      "\n",
      "# 13 Gradient out:  [-1.16350904 -1.04531957 -1.17045855 -1.79700924 -0.58185789 -1.75916022\n",
      " -0.60050893 -0.39057036 -1.78603524 -1.49605165]\n",
      "\n",
      "     Weights  out:  [ 0.02725732 -0.0265906   0.03043406  0.4083257  -0.26878777  0.37180701\n",
      " -0.25655293 -0.42941717  0.3972638   0.1905144 ] [-0.24551632  0.25923725  0.35438318  0.3484955   0.4818847   0.49523707\n",
      "  0.15848566 -0.145619    0.190422   -0.25202151]\n",
      "\n",
      "# 14 Gradient out:  [2.32248352 2.08832818 2.33619995 3.37819977 1.27053898 3.32638748\n",
      " 1.29832885 1.00249707 3.36316696 2.93515095]\n",
      "\n",
      "     Weights  out:  [ 0.02725732 -0.0265906   0.03043406  0.4083257  -0.26878777  0.37180701\n",
      " -0.25655293 -0.42941717  0.3972638   0.1905144 ] [-0.47821812  0.05017333  0.12029147 -0.01090634  0.36551312  0.14340503\n",
      "  0.03838388 -0.22373307 -0.16678505 -0.55123184]\n",
      "\n",
      "# 15 Gradient out:  [-0.13685704 -0.12545605 -0.1375272  -0.19745113 -0.08110485 -0.19380931\n",
      " -0.08287756 -0.06280544 -0.19639254 -0.1687544 ]\n",
      "\n",
      "     Weights  out:  [ 0.02725732 -0.0265906   0.03043406  0.4083257  -0.26878777  0.37180701\n",
      " -0.25655293 -0.42941717  0.3972638   0.1905144 ] [-0.01372142  0.46783897  0.58753146  0.66473361  0.61962092  0.80868253\n",
      "  0.29804965 -0.02323366  0.50584835  0.03579835]\n",
      "\n",
      "# 16 Gradient out:  [-0.18665097 -0.17068833 -0.18758924 -0.27137933 -0.1086307  -0.26630187\n",
      " -0.11110811 -0.08309371 -0.26990388 -0.23129581]\n",
      "\n",
      "     Weights  out:  [ 0.02725732 -0.0265906   0.03043406  0.4083257  -0.26878777  0.37180701\n",
      " -0.25655293 -0.42941717  0.3972638   0.1905144 ] [-0.04109283  0.44274776  0.56002602  0.62524338  0.60339995  0.76992066\n",
      "  0.28147414 -0.03579475  0.46656984  0.00204747]\n",
      "\n",
      "# 17 Gradient out:  [-0.28287222 -0.25780216 -0.2843458  -0.41573623 -0.16040463 -0.40780519\n",
      " -0.16428711 -0.12046343 -0.41343253 -0.35296486]\n",
      "\n",
      "     Weights  out:  [ 0.02725732 -0.0265906   0.03043406  0.4083257  -0.26878777  0.37180701\n",
      " -0.25655293 -0.42941717  0.3972638   0.1905144 ] [-0.07842302  0.40861009  0.52250818  0.57096752  0.58167381  0.71666029\n",
      "  0.25925251 -0.05241349  0.41258906 -0.0442117 ]\n",
      "\n",
      "# 18 Gradient out:  [-0.51470048 -0.46659143 -0.51752828 -0.76937415 -0.27972189 -0.7542471\n",
      " -0.28716217 -0.20339206 -0.76498326 -0.64921105]\n",
      "\n",
      "     Weights  out:  [ 0.02725732 -0.0265906   0.03043406  0.4083257  -0.26878777  0.37180701\n",
      " -0.25655293 -0.42941717  0.3972638   0.1905144 ] [-0.13499747  0.35704966  0.46563901  0.48782027  0.54959288  0.63509925\n",
      "  0.22639509 -0.07650618  0.32990256 -0.11480467]\n",
      "\n",
      "# 19 Gradient out:  [-1.172607   -1.05350133 -1.17961057 -1.81189489 -0.58604415 -1.77362859\n",
      " -0.60487865 -0.39272999 -1.80079867 -1.50790684]\n",
      "\n",
      "     Weights  out:  [ 0.02725732 -0.0265906   0.03043406  0.4083257  -0.26878777  0.37180701\n",
      " -0.25655293 -0.42941717  0.3972638   0.1905144 ] [-0.23793756  0.26373137  0.36213336  0.33394544  0.4936485   0.48424983\n",
      "  0.16896266 -0.11718459  0.1769059  -0.24464688]\n",
      "\n",
      "# 20 Gradient out:  [2.32568446 2.09204395 2.3393717  3.38348241 1.27402974 3.33116637\n",
      " 1.30195459 1.00385236 3.36829797 2.93790275]\n",
      "\n",
      "     Weights  out:  [ 0.02725732 -0.0265906   0.03043406  0.4083257  -0.26878777  0.37180701\n",
      " -0.25655293 -0.42941717  0.3972638   0.1905144 ] [-0.47245896  0.05303111  0.12621124 -0.02843354  0.37643968  0.12952412\n",
      "  0.04798693 -0.19573059 -0.18325383 -0.54622824]\n",
      "\n",
      "# 21 Gradient out:  [-0.13916381 -0.12736413 -0.13985739 -0.2018151  -0.08148595 -0.19805684\n",
      " -0.08331801 -0.0625909  -0.20072285 -0.17216689]\n",
      "\n",
      "     Weights  out:  [ 0.02725732 -0.0265906   0.03043406  0.4083257  -0.26878777  0.37180701\n",
      " -0.25655293 -0.42941717  0.3972638   0.1905144 ] [-0.00732207  0.4714399   0.59408558  0.64826295  0.63124562  0.79575739\n",
      "  0.30837784  0.00503988  0.49040576  0.0413523 ]\n",
      "\n",
      "# 22 Gradient out:  [-0.19108918 -0.17446579 -0.19206628 -0.27924099 -0.10987163 -0.27396828\n",
      " -0.11244801 -0.08333795 -0.27770905 -0.23756902]\n",
      "\n",
      "     Weights  out:  [ 0.02725732 -0.0265906   0.03043406  0.4083257  -0.26878777  0.37180701\n",
      " -0.25655293 -0.42941717  0.3972638   0.1905144 ] [-0.03515483  0.44596707  0.56611411  0.60789993  0.61494843  0.75614602\n",
      "  0.29171424 -0.00747829  0.45026119  0.00691893]\n",
      "\n",
      "# 23 Gradient out:  [-0.29296534 -0.2665767  -0.29451641 -0.43269882 -0.16410091 -0.4243724\n",
      " -0.16818256 -0.12214617 -0.43028067 -0.36672694]\n",
      "\n",
      "     Weights  out:  [ 0.02725732 -0.0265906   0.03043406  0.4083257  -0.26878777  0.37180701\n",
      " -0.25655293 -0.42941717  0.3972638   0.1905144 ] [-0.07337267  0.41107391  0.52770085  0.55205173  0.59297411  0.70135237\n",
      "  0.26922464 -0.02414589  0.39471938 -0.04059488]\n",
      "\n",
      "# 24 Gradient out:  [-0.54427909 -0.49266093 -0.54731315 -0.81742885 -0.29218494 -0.80122367\n",
      " -0.30016401 -0.21037961 -0.81272569 -0.68859491]\n",
      "\n",
      "     Weights  out:  [ 0.02725732 -0.0265906   0.03043406  0.4083257  -0.26878777  0.37180701\n",
      " -0.25655293 -0.42941717  0.3972638   0.1905144 ] [-0.13196574  0.35775857  0.46879757  0.46551196  0.56015393  0.61647789\n",
      "  0.23558813 -0.04857512  0.30866325 -0.11394026]\n",
      "\n",
      "# 25 Gradient out:  [-1.23512585 -1.10915185 -1.24253449 -1.91605603 -0.61239167 -1.87499361\n",
      " -0.63252954 -0.40518553 -1.90414725 -1.59083144]\n",
      "\n",
      "     Weights  out:  [ 0.02725732 -0.0265906   0.03043406  0.4083257  -0.26878777  0.37180701\n",
      " -0.25655293 -0.42941717  0.3972638   0.1905144 ] [-0.24082155  0.25922639  0.35933494  0.30202619  0.50171694  0.45623315\n",
      "  0.17555533 -0.09065104  0.14611811 -0.25165925]\n",
      "\n",
      "# 26 Gradient out:  [2.31281486 2.09086375 2.32582301 3.34406687 1.30178719 3.2906017\n",
      " 1.32948646 1.02862394 3.32851332 2.89969603]\n",
      "\n",
      "     Weights  out:  [ 0.02725732 -0.0265906   0.03043406  0.4083257  -0.26878777  0.37180701\n",
      " -0.25655293 -0.42941717  0.3972638   0.1905144 ] [-0.48784672  0.03739602  0.11082804 -0.08118501  0.3792386   0.08123443\n",
      "  0.04904942 -0.17168815 -0.23471134 -0.56982554]\n",
      "\n",
      "# 27 Gradient out:  [-0.19229207 -0.1751633  -0.19329886 -0.28299463 -0.10865572 -0.27758432\n",
      " -0.11130489 -0.08140771 -0.28142308 -0.24016454]\n",
      "\n",
      "     Weights  out:  [ 0.02725732 -0.0265906   0.03043406  0.4083257  -0.26878777  0.37180701\n",
      " -0.25655293 -0.42941717  0.3972638   0.1905144 ] [-0.02528375  0.45556877  0.57599264  0.58762836  0.63959604  0.73935477\n",
      "  0.31494671  0.03403664  0.43099133  0.01011367]\n",
      "\n",
      "# 28 Gradient out:  [-0.29665965 -0.26936435 -0.29826398 -0.44101612 -0.16343612 -0.43243532\n",
      " -0.1676504  -0.12016826 -0.43852463 -0.37292802]\n",
      "\n",
      "     Weights  out:  [ 0.02725732 -0.0265906   0.03043406  0.4083257  -0.26878777  0.37180701\n",
      " -0.25655293 -0.42941717  0.3972638   0.1905144 ] [-0.06374217  0.42053611  0.53733287  0.53102944  0.6178649   0.68383791\n",
      "  0.29268573  0.0177551   0.37470671 -0.03791924]\n",
      "\n",
      "# 29 Gradient out:  [-0.55764582 -0.50388244 -0.56080596 -0.84198478 -0.29512735 -0.82514\n",
      " -0.30343128 -0.21005311 -0.83709671 -0.70794083]\n",
      "\n",
      "     Weights  out:  [ 0.02725732 -0.0265906   0.03043406  0.4083257  -0.26878777  0.37180701\n",
      " -0.25655293 -0.42941717  0.3972638   0.1905144 ] [-0.1230741   0.36666324  0.47768007  0.44282621  0.58517767  0.59735084\n",
      "  0.25915565 -0.00627855  0.28700178 -0.11250484]\n",
      "\n",
      "# 30 Gradient out:  [-1.2629511  -1.13455453 -1.27050327 -1.96154842 -0.6260805  -1.91908586\n",
      " -0.64681085 -0.4121426  -1.94922937 -1.62646828]\n",
      "\n",
      "     Weights  out:  [ 0.02725732 -0.0265906   0.03043406  0.4083257  -0.26878777  0.37180701\n",
      " -0.25655293 -0.42941717  0.3972638   0.1905144 ] [-0.23460326  0.26588675  0.36551888  0.27442925  0.5261522   0.43232284\n",
      "  0.1984694  -0.04828918  0.11958244 -0.25409301]\n",
      "\n",
      "# 31 Gradient out:  [2.24586364 2.03660663 2.25813042 3.23107959 1.28699254 3.17870688\n",
      " 1.31367573 1.02104888 3.21582135 2.80164114]\n",
      "\n",
      "     Weights  out:  [ 0.02725732 -0.0265906   0.03043406  0.4083257  -0.26878777  0.37180701\n",
      " -0.25655293 -0.42941717  0.3972638   0.1905144 ] [-0.48719348  0.03897584  0.11141823 -0.11788043  0.4009361   0.04850567\n",
      "  0.06910723 -0.13071769 -0.27026343 -0.57938666]\n",
      "\n",
      "# 32 Gradient out:  [-0.25542363 -0.23159926 -0.25682393 -0.38127329 -0.1392055  -0.37380684\n",
      " -0.14287728 -0.10153863 -0.37910563 -0.32196607]\n",
      "\n",
      "     Weights  out:  [ 0.02725732 -0.0265906   0.03043406  0.4083257  -0.26878777  0.37180701\n",
      " -0.25655293 -0.42941717  0.3972638   0.1905144 ] [-0.03802075  0.44629717  0.56304431  0.52833549  0.65833461  0.68424705\n",
      "  0.33184237  0.07349208  0.37290084 -0.01905844]\n",
      "\n",
      "# 33 Gradient out:  [-0.4487396  -0.40532007 -0.45129167 -0.67803452 -0.23689803 -0.66447012\n",
      " -0.24358872 -0.16838103 -0.67409831 -0.57004105]\n",
      "\n",
      "     Weights  out:  [ 0.02725732 -0.0265906   0.03043406  0.4083257  -0.26878777  0.37180701\n",
      " -0.25655293 -0.42941717  0.3972638   0.1905144 ] [-0.08910548  0.39997732  0.51167953  0.45208083  0.63049351  0.60948568\n",
      "  0.30326692  0.05318435  0.29707971 -0.08345165]\n",
      "\n",
      "# 34 Gradient out:  [-1.02203186 -0.91858862 -1.02811371 -1.57467683 -0.51400545 -1.54168235\n",
      " -0.530241   -0.34739392 -1.56510569 -1.31257884]\n",
      "\n",
      "     Weights  out:  [ 0.02725732 -0.0265906   0.03043406  0.4083257  -0.26878777  0.37180701\n",
      " -0.25655293 -0.42941717  0.3972638   0.1905144 ] [-0.1788534   0.3189133   0.42142119  0.31647392  0.58311391  0.47659165\n",
      "  0.25454917  0.01950815  0.16226005 -0.19745986]\n",
      "\n",
      "# 35 Gradient out:  [1.45818265 1.28405341 1.4683644  2.16242766 0.71374122 2.13484493\n",
      " 0.73078157 0.56344252 2.15450241 1.89688713]\n",
      "\n",
      "     Weights  out:  [ 0.02725732 -0.0265906   0.03043406  0.4083257  -0.26878777  0.37180701\n",
      " -0.25655293 -0.42941717  0.3972638   0.1905144 ] [-0.38325977  0.13519558  0.21579845  0.00153856  0.48031281  0.16825518\n",
      "  0.14850097 -0.04997064 -0.15076109 -0.45997563]\n",
      "\n",
      "# 36 Gradient out:  [-0.48509728 -0.43785438 -0.48787408 -0.73461672 -0.25457363 -0.71985978\n",
      " -0.26185534 -0.18001995 -0.73033475 -0.61709504]\n",
      "\n",
      "     Weights  out:  [ 0.02725732 -0.0265906   0.03043406  0.4083257  -0.26878777  0.37180701\n",
      " -0.25655293 -0.42941717  0.3972638   0.1905144 ] [-0.09162324  0.39200626  0.50947133  0.43402409  0.62306106  0.59522417\n",
      "  0.29465729  0.06271787  0.28013939 -0.0805982 ]\n",
      "\n",
      "# 37 Gradient out:  [-1.12348466 -1.00944838 -1.13019018 -1.7361764  -0.56173426 -1.69937932\n",
      " -0.57978964 -0.37608239 -1.72550112 -1.44456053]\n",
      "\n",
      "     Weights  out:  [ 0.02725732 -0.0265906   0.03043406  0.4083257  -0.26878777  0.37180701\n",
      " -0.25655293 -0.42941717  0.3972638   0.1905144 ] [-0.1886427   0.30443538  0.41189651  0.28710075  0.57214633  0.45125221\n",
      "  0.24228622  0.02671388  0.13407244 -0.20401721]\n",
      "\n",
      "# 38 Gradient out:  [2.09466976 1.87098084 2.10776879 3.08386313 1.09866615 3.03704999\n",
      " 1.12434978 0.85450782 3.07030246 2.67599336]\n",
      "\n",
      "     Weights  out:  [ 0.02725732 -0.0265906   0.03043406  0.4083257  -0.26878777  0.37180701\n",
      " -0.25655293 -0.42941717  0.3972638   0.1905144 ] [-0.41333963  0.10254571  0.18585848 -0.06013453  0.45979948  0.11137635\n",
      "  0.12632829 -0.0485026  -0.21102778 -0.49292932]\n",
      "\n",
      "# 39 Gradient out:  [-0.17746643 -0.16112512 -0.1784269  -0.26381536 -0.0977481  -0.25868531\n",
      " -0.10026757 -0.07188207 -0.26232572 -0.22310809]\n",
      "\n",
      "     Weights  out:  [ 0.02725732 -0.0265906   0.03043406  0.4083257  -0.26878777  0.37180701\n",
      " -0.25655293 -0.42941717  0.3972638   0.1905144 ] [0.00559432 0.47674188 0.60741224 0.55663809 0.67953271 0.71878635\n",
      " 0.35119825 0.12239897 0.40303271 0.04226936]\n",
      "\n",
      "# 40 Gradient out:  [-0.26696378 -0.24172772 -0.26844704 -0.40018164 -0.14389178 -0.39228948\n",
      " -0.14777737 -0.10405943 -0.3978907  -0.33743643]\n",
      "\n",
      "     Weights  out:  [ 0.02725732 -0.0265906   0.03043406  0.4083257  -0.26878777  0.37180701\n",
      " -0.25655293 -0.42941717  0.3972638   0.1905144 ] [-0.02989896  0.44451685  0.57172686  0.50387502  0.65998309  0.66704928\n",
      "  0.33114473  0.10802255  0.35056757 -0.00235226]\n",
      "\n",
      "# 41 Gradient out:  [-0.48030231 -0.43324853 -0.48306799 -0.72873737 -0.25073524 -0.71405419\n",
      " -0.2579841  -0.17654037 -0.72447703 -0.61175795]\n",
      "\n",
      "     Weights  out:  [ 0.02725732 -0.0265906   0.03043406  0.4083257  -0.26878777  0.37180701\n",
      " -0.25655293 -0.42941717  0.3972638   0.1905144 ] [-0.08329172  0.39617131  0.51803745  0.42383869  0.63120473  0.58859139\n",
      "  0.30158926  0.08721067  0.27098943 -0.06983955]\n",
      "\n",
      "# 42 Gradient out:  [-1.11311761 -0.99999499 -1.11976938 -1.72084814 -0.55590632 -1.68434493\n",
      " -0.57381416 -0.37175346 -1.71025769 -1.43160178]\n",
      "\n",
      "     Weights  out:  [ 0.02725732 -0.0265906   0.03043406  0.4083257  -0.26878777  0.37180701\n",
      " -0.25655293 -0.42941717  0.3972638   0.1905144 ] [-0.17935218  0.3095216   0.42142385  0.27809122  0.58105769  0.44578055\n",
      "  0.24999244  0.05190259  0.12609402 -0.19219114]\n",
      "\n",
      "# 43 Gradient out:  [2.03310052 1.81243636 2.04602107 3.00270046 1.05340042 2.95740215\n",
      " 1.07846013 0.81641924 2.9895871  2.60531051]\n",
      "\n",
      "     Weights  out:  [ 0.02725732 -0.0265906   0.03043406  0.4083257  -0.26878777  0.37180701\n",
      " -0.25655293 -0.42941717  0.3972638   0.1905144 ] [-0.4019757   0.1095226   0.19746998 -0.06607841  0.46987642  0.10891156\n",
      "  0.13522961 -0.0224481  -0.21595751 -0.47851149]\n",
      "\n",
      "# 44 Gradient out:  [-0.1939562  -0.17585288 -0.19502022 -0.28954594 -0.10566772 -0.28387589\n",
      " -0.10845591 -0.07706489 -0.28789974 -0.24450921]\n",
      "\n",
      "     Weights  out:  [ 0.02725732 -0.0265906   0.03043406  0.4083257  -0.26878777  0.37180701\n",
      " -0.25655293 -0.42941717  0.3972638   0.1905144 ] [0.0046444  0.47200988 0.60667419 0.53446168 0.68055651 0.70039199\n",
      " 0.35092163 0.14083575 0.38195991 0.04255061]\n",
      "\n",
      "# 45 Gradient out:  [-0.30252582 -0.27350658 -0.30423143 -0.45562705 -0.16103092 -0.44657083\n",
      " -0.16549545 -0.1152996  -0.45299865 -0.38355386]\n",
      "\n",
      "     Weights  out:  [ 0.02725732 -0.0265906   0.03043406  0.4083257  -0.26878777  0.37180701\n",
      " -0.25655293 -0.42941717  0.3972638   0.1905144 ] [-0.03414684  0.4368393   0.56767015  0.47655249  0.65942296  0.64361681\n",
      "  0.32923045  0.12542277  0.32437996 -0.00635123]\n",
      "\n",
      "# 46 Gradient out:  [-0.58030856 -0.52256479 -0.58370265 -0.88540902 -0.29844149 -0.86738062\n",
      " -0.30734833 -0.20731044 -0.8801789  -0.74170217]\n",
      "\n",
      "     Weights  out:  [ 0.02725732 -0.0265906   0.03043406  0.4083257  -0.26878777  0.37180701\n",
      " -0.25655293 -0.42941717  0.3972638   0.1905144 ] [-0.094652    0.38213798  0.50682386  0.38542708  0.62721678  0.55430265\n",
      "  0.29613136  0.10236285  0.23378023 -0.083062  ]\n",
      "\n",
      "# 47 Gradient out:  [-1.29645396 -1.166522   -1.30409911 -2.01506945 -0.64651521 -1.97051084\n",
      " -0.6680154  -0.4229286  -2.00213019 -1.66675678]\n",
      "\n",
      "     Weights  out:  [ 0.02725732 -0.0265906   0.03043406  0.4083257  -0.26878777  0.37180701\n",
      " -0.25655293 -0.42941717  0.3972638   0.1905144 ] [-0.21071372  0.27762503  0.39008333  0.20834528  0.56752848  0.38082652\n",
      "  0.23466169  0.06090076  0.05774445 -0.23140244]\n",
      "\n",
      "# 48 Gradient out:  [2.11086281 1.92477165 2.12177536 3.00660437 1.24977226 2.95694316\n",
      " 1.27435511 1.00025842 2.99209746 2.60870892]\n",
      "\n",
      "     Weights  out:  [ 0.02725732 -0.0265906   0.03043406  0.4083257  -0.26878777  0.37180701\n",
      " -0.25655293 -0.42941717  0.3972638   0.1905144 ] [-0.47000451  0.04432063  0.12926351 -0.19466861  0.43822544 -0.01327564\n",
      "  0.10105861 -0.02368496 -0.34268159 -0.5647538 ]\n",
      "\n",
      "# 49 Gradient out:  [-0.41685743 -0.37516886 -0.41930767 -0.63652077 -0.2136569  -0.62358044\n",
      " -0.22005965 -0.14821486 -0.63276692 -0.53324282]\n",
      "\n",
      "     Weights  out:  [ 0.02725732 -0.0265906   0.03043406  0.4083257  -0.26878777  0.37180701\n",
      " -0.25655293 -0.42941717  0.3972638   0.1905144 ] [-0.04783195  0.42927496  0.55361858  0.40665226  0.68817989  0.57811299\n",
      "  0.35592964  0.17636673  0.2557379  -0.04301201]\n",
      "\n",
      "Lineal error cuadrático medio (ECM): 1.5791508804109293\n",
      "\n",
      "# 0 Gradient out:  [3.52570571 3.4638575  3.4515844  1.04998466 1.33377726 0.96972953\n",
      " 3.5255327  0.98406836 1.12212992 1.53095685]\n",
      "\n",
      "     Weights  out:  [ 0.47787269  0.42753074  0.41836463 -0.41283824 -0.25720245 -0.47662144\n",
      "  0.47772156 -0.46434097 -0.36456446 -0.18478548] [-0.1533876  -0.03966069 -0.32016236 -0.46354224  0.28901216 -0.06332587\n",
      " -0.30769291  0.07611275 -0.11910732  0.26120614]\n",
      "\n",
      "# 1 Gradient out:  [-0.40520531 -0.39686283 -0.39518354 -0.11709585 -0.15515265 -0.10622659\n",
      " -0.40518238 -0.10813849 -0.12701192 -0.17925431]\n",
      "\n",
      "     Weights  out:  [ 0.47787269  0.42753074  0.41836463 -0.41283824 -0.25720245 -0.47662144\n",
      "  0.47772156 -0.46434097 -0.36456446 -0.18478548] [ 0.55175354  0.65311081  0.37015452 -0.2535453   0.55576761  0.13062004\n",
      "  0.39741363  0.27292642  0.10531866  0.56739751]\n",
      "\n",
      "# 2 Gradient out:  [-0.69215879 -0.67755478 -0.6746113  -0.18520896 -0.25210526 -0.16617461\n",
      " -0.69211869 -0.16951861 -0.20261245 -0.29454301]\n",
      "\n",
      "     Weights  out:  [ 0.47787269  0.42753074  0.41836463 -0.41283824 -0.25720245 -0.47662144\n",
      "  0.47772156 -0.46434097 -0.36456446 -0.18478548] [ 0.47071248  0.57373825  0.29111781 -0.27696447  0.52473708  0.10937472\n",
      "  0.31637715  0.25129873  0.07991628  0.53154665]\n",
      "\n",
      "# 3 Gradient out:  [-1.50110026 -1.46791654 -1.46121256 -0.3488477  -0.50167465 -0.30556797\n",
      " -1.50100937 -0.3131536  -0.38856301 -0.59845282]\n",
      "\n",
      "     Weights  out:  [ 0.47787269  0.42753074  0.41836463 -0.41283824 -0.25720245 -0.47662144\n",
      "  0.47772156 -0.46434097 -0.36456446 -0.18478548] [ 0.33228072  0.43822729  0.15619555 -0.31400626  0.47431603  0.0761398\n",
      "  0.17795341  0.21739501  0.03939379  0.47263804]\n",
      "\n",
      "# 4 Gradient out:  [0.42251217 0.42894123 0.43027102 0.18788065 0.17089447 0.19631961\n",
      " 0.42252872 0.19490291 0.18055472 0.18111152]\n",
      "\n",
      "     Weights  out:  [ 0.47787269  0.42753074  0.41836463 -0.41283824 -0.25720245 -0.47662144\n",
      "  0.47772156 -0.46434097 -0.36456446 -0.18478548] [ 0.03206067  0.14464398 -0.13604696 -0.3837758   0.3739811   0.0150262\n",
      " -0.12224846  0.15476429 -0.03831881  0.35294748]\n",
      "\n",
      "# 5 Gradient out:  [-1.51732209 -1.4745303  -1.46588073 -0.27828582 -0.46796693 -0.22246841\n",
      " -1.51720527 -0.23223188 -0.32917306 -0.57693199]\n",
      "\n",
      "     Weights  out:  [ 0.47787269  0.42753074  0.41836463 -0.41283824 -0.25720245 -0.47662144\n",
      "  0.47772156 -0.46434097 -0.36456446 -0.18478548] [ 0.1165631   0.23043223 -0.04999276 -0.34619967  0.40815999  0.05429013\n",
      " -0.03774271  0.19374487 -0.00220787  0.38916978]\n",
      "\n",
      "# 6 Gradient out:  [3.44106424 3.37891652 3.36662168 1.0354621  1.31618288 0.95488912\n",
      " 3.44088998 0.96932315 1.10742721 1.50826986]\n",
      "\n",
      "     Weights  out:  [ 0.47787269  0.42753074  0.41836463 -0.41283824 -0.25720245 -0.47662144\n",
      "  0.47772156 -0.46434097 -0.36456446 -0.18478548] [-0.18690131 -0.06447383 -0.3431689  -0.40185684  0.31456661  0.00979644\n",
      " -0.34118377  0.14729849 -0.06804248  0.27378339]\n",
      "\n",
      "# 7 Gradient out:  [-0.48733324 -0.47693793 -0.47484192 -0.12449669 -0.17222153 -0.11094629\n",
      " -0.4873047  -0.11332607 -0.13689682 -0.20257783]\n",
      "\n",
      "     Weights  out:  [ 0.47787269  0.42753074  0.41836463 -0.41283824 -0.25720245 -0.47662144\n",
      "  0.47772156 -0.46434097 -0.36456446 -0.18478548] [ 0.50131154  0.61130948  0.33015543 -0.19476442  0.57780318  0.20077427\n",
      "  0.34699423  0.34116312  0.15344296  0.57543736]\n",
      "\n",
      "# 8 Gradient out:  [-0.90928549 -0.88941837 -0.88540835 -0.21508385 -0.30653719 -0.18917901\n",
      " -0.90923101 -0.19372379 -0.23882936 -0.36469201]\n",
      "\n",
      "     Weights  out:  [ 0.47787269  0.42753074  0.41836463 -0.41283824 -0.25720245 -0.47662144\n",
      "  0.47772156 -0.46434097 -0.36456446 -0.18478548] [ 0.40384489  0.51592189  0.23518705 -0.21966376  0.54335888  0.17858501\n",
      "  0.24953329  0.31849791  0.1260636   0.53492179]\n",
      "\n",
      "# 9 Gradient out:  [-2.02480163 -1.97724191 -1.96764151 -0.4312965  -0.64817448 -0.3692821\n",
      " -2.02467132 -0.3801571  -0.48804705 -0.78301428]\n",
      "\n",
      "     Weights  out:  [ 0.47787269  0.42753074  0.41836463 -0.41283824 -0.25720245 -0.47662144\n",
      "  0.47772156 -0.46434097 -0.36456446 -0.18478548] [ 0.22198779  0.33803822  0.05810538 -0.26268053  0.48205144  0.14074921\n",
      "  0.06768708  0.27975315  0.07829773  0.46198339]\n",
      "\n",
      "# 10 Gradient out:  [3.39050036 3.32891926 3.31673165 0.97735833 1.25668927 0.89751067\n",
      " 3.39032771 0.91181095 1.04876535 1.44903572]\n",
      "\n",
      "     Weights  out:  [ 0.47787269  0.42753074  0.41836463 -0.41283824 -0.25720245 -0.47662144\n",
      "  0.47772156 -0.46434097 -0.36456446 -0.18478548] [-0.18297254 -0.05741017 -0.33542292 -0.34893983  0.35241654  0.06689279\n",
      " -0.33724718  0.20372173 -0.01931168  0.30538054]\n",
      "\n",
      "# 11 Gradient out:  [-0.4648462  -0.454809   -0.45278378 -0.11271214 -0.1589238  -0.09962589\n",
      " -0.46481867 -0.10192266 -0.12470329 -0.18838188]\n",
      "\n",
      "     Weights  out:  [ 0.47787269  0.42753074  0.41836463 -0.41283824 -0.25720245 -0.47662144\n",
      "  0.47772156 -0.46434097 -0.36456446 -0.18478548] [ 0.49512754  0.60837369  0.32792341 -0.15346816  0.6037544   0.24639492\n",
      "  0.34081836  0.38608392  0.19044139  0.59518768]\n",
      "\n",
      "# 12 Gradient out:  [-0.84854514 -0.82984565 -0.82606981 -0.19281979 -0.27905255 -0.16843462\n",
      " -0.84849388 -0.17271125 -0.21518987 -0.33397534]\n",
      "\n",
      "     Weights  out:  [ 0.47787269  0.42753074  0.41836463 -0.41283824 -0.25720245 -0.47662144\n",
      "  0.47772156 -0.46434097 -0.36456446 -0.18478548] [ 0.40215829  0.51741189  0.23736665 -0.17601059  0.57196964  0.22646974\n",
      "  0.24785463  0.36569939  0.16550073  0.55751131]\n",
      "\n",
      "# 13 Gradient out:  [-1.91234414 -1.86759656 -1.85856837 -0.40513943 -0.60922077 -0.34680026\n",
      " -1.91222145 -0.35703648 -0.45849865 -0.73651998]\n",
      "\n",
      "     Weights  out:  [ 0.47787269  0.42753074  0.41836463 -0.41283824 -0.25720245 -0.47662144\n",
      "  0.47772156 -0.46434097 -0.36456446 -0.18478548] [ 0.23244927  0.35144276  0.07215269 -0.21457455  0.51615913  0.19278282\n",
      "  0.07815585  0.33115714  0.12246275  0.49071624]\n",
      "\n",
      "# 14 Gradient out:  [3.08963187 3.03554082 3.02477136 0.78168399 1.03615122 0.71142711\n",
      " 3.08948088 0.72394726 0.8453846  1.21856992]\n",
      "\n",
      "     Weights  out:  [ 0.47787269  0.42753074  0.41836463 -0.41283824 -0.25720245 -0.47662144\n",
      "  0.47772156 -0.46434097 -0.36456446 -0.18478548] [-0.15001956 -0.02207656 -0.29956098 -0.29560243  0.39431497  0.12342277\n",
      " -0.30428844  0.25974984  0.03076302  0.34341224]\n",
      "\n",
      "# 15 Gradient out:  [-0.53548652 -0.52381037 -0.52145326 -0.12480765 -0.17866097 -0.10958235\n",
      " -0.5354545  -0.11225325 -0.13877149 -0.21302312]\n",
      "\n",
      "     Weights  out:  [ 0.47787269  0.42753074  0.41836463 -0.41283824 -0.25720245 -0.47662144\n",
      "  0.47772156 -0.46434097 -0.36456446 -0.18478548] [ 0.46790681  0.58503161  0.30539329 -0.13926563  0.60154522  0.26570819\n",
      "  0.31360774  0.40453929  0.19983994  0.58712623]\n",
      "\n",
      "# 16 Gradient out:  [-1.04534885 -1.02200533 -1.0172903  -0.22809331 -0.33578114 -0.19764938\n",
      " -1.04528488 -0.20298688 -0.25603311 -0.40429352]\n",
      "\n",
      "     Weights  out:  [ 0.47787269  0.42753074  0.41836463 -0.41283824 -0.25720245 -0.47662144\n",
      "  0.47772156 -0.46434097 -0.36456446 -0.18478548] [ 0.36080951  0.48026953  0.20110264 -0.16422716  0.56581302  0.24379172\n",
      "  0.20651684  0.38208864  0.17208565  0.5445216 ]\n",
      "\n",
      "# 17 Gradient out:  [-2.07318995 -2.02117008 -2.01071236 -0.44996758 -0.68131259 -0.38221846\n",
      " -2.07304697 -0.39414117 -0.51139691 -0.82022237]\n",
      "\n",
      "     Weights  out:  [ 0.47787269  0.42753074  0.41836463 -0.41283824 -0.25720245 -0.47662144\n",
      "  0.47772156 -0.46434097 -0.36456446 -0.18478548] [ 0.15173974  0.27586847 -0.00235542 -0.20984583  0.4986568   0.20426184\n",
      " -0.00254014  0.34149127  0.12087902  0.4636629 ]\n",
      "\n",
      "# 18 Gradient out:  [3.1189905  3.05886992 3.0470763  0.9905968  1.25092615 0.91283997\n",
      " 3.11882077 0.92687273 1.05883547 1.42191225]\n",
      "\n",
      "     Weights  out:  [ 0.47787269  0.42753074  0.41836463 -0.41283824 -0.25720245 -0.47662144\n",
      "  0.47772156 -0.46434097 -0.36456446 -0.18478548] [-0.26289825 -0.12836555 -0.4044979  -0.29983934  0.36239428  0.12781815\n",
      " -0.41714953  0.26266303  0.01859964  0.29961842]\n",
      "\n",
      "# 19 Gradient out:  [-0.93829    -0.91714891 -0.91287602 -0.19386861 -0.29167702 -0.16629193\n",
      " -0.9382321  -0.17112393 -0.21920888 -0.35406531]\n",
      "\n",
      "     Weights  out:  [ 0.47787269  0.42753074  0.41836463 -0.41283824 -0.25720245 -0.47662144\n",
      "  0.47772156 -0.46434097 -0.36456446 -0.18478548] [ 0.36089985  0.48340844  0.20491736 -0.10171998  0.61257951  0.31038614\n",
      "  0.20661462  0.44803758  0.23036674  0.58400087]\n",
      "\n",
      "# 20 Gradient out:  [-2.0242493  -1.97449678 -1.96449078 -0.42503162 -0.64797157 -0.36022743\n",
      " -2.02411254 -0.37163005 -0.48390073 -0.78394896]\n",
      "\n",
      "     Weights  out:  [ 0.47787269  0.42753074  0.41836463 -0.41283824 -0.25720245 -0.47662144\n",
      "  0.47772156 -0.46434097 -0.36456446 -0.18478548] [ 0.17324185  0.29997865  0.02234216 -0.14049371  0.55424411  0.27712776\n",
      "  0.0189682   0.41381279  0.18652496  0.51318781]\n",
      "\n",
      "# 21 Gradient out:  [3.10845161 3.04834406 3.03648997 0.84215606 1.1099209  0.76429768\n",
      " 3.10828262 0.77828448 0.91126857 1.29107754]\n",
      "\n",
      "     Weights  out:  [ 0.47787269  0.42753074  0.41836463 -0.41283824 -0.25720245 -0.47662144\n",
      "  0.47772156 -0.46434097 -0.36456446 -0.18478548] [-0.23160801 -0.0949207  -0.370556   -0.22550003  0.42464979  0.20508227\n",
      " -0.3858543   0.33948678  0.08974481  0.35639802]\n",
      "\n",
      "# 22 Gradient out:  [-0.733684   -0.71717914 -0.7138414  -0.1482041  -0.2247962  -0.1266714\n",
      " -0.73363882 -0.13044249 -0.16801445 -0.2738222 ]\n",
      "\n",
      "     Weights  out:  [ 0.47787269  0.42753074  0.41836463 -0.41283824 -0.25720245 -0.47662144\n",
      "  0.47772156 -0.46434097 -0.36456446 -0.18478548] [ 0.39008232  0.51474811  0.236742   -0.05706882  0.64663397  0.35794181\n",
      "  0.23580222  0.49514368  0.27199853  0.61461353]\n",
      "\n",
      "# 23 Gradient out:  [-1.60758089 -1.56976415 -1.56213286 -0.31831952 -0.49137751 -0.26901352\n",
      " -1.60747721 -0.27766417 -0.36345591 -0.60002008]\n",
      "\n",
      "     Weights  out:  [ 0.47787269  0.42753074  0.41836463 -0.41283824 -0.25720245 -0.47662144\n",
      "  0.47772156 -0.46434097 -0.36456446 -0.18478548] [ 0.24334552  0.37131228  0.09397372 -0.08670964  0.60167473  0.33260753\n",
      "  0.08907446  0.46905518  0.23839564  0.55984909]\n",
      "\n",
      "# 24 Gradient out:  [1.06123875 1.0552291  1.05381166 0.04152261 0.10473575 0.03329964\n",
      " 1.06122407 0.03455737 0.05214553 0.17880686]\n",
      "\n",
      "     Weights  out:  [ 0.47787269  0.42753074  0.41836463 -0.41283824 -0.25720245 -0.47662144\n",
      "  0.47772156 -0.46434097 -0.36456446 -0.18478548] [-0.07817066  0.05735945 -0.21845286 -0.15037354  0.50339923  0.27880482\n",
      " -0.23242098  0.41352235  0.16570446  0.43984507]\n",
      "\n",
      "# 25 Gradient out:  [-2.04397894 -1.991638   -1.98114128 -0.44723917 -0.67760089 -0.37911972\n",
      " -2.04383476 -0.39113466 -0.50871881 -0.81451431]\n",
      "\n",
      "     Weights  out:  [ 0.47787269  0.42753074  0.41836463 -0.41283824 -0.25720245 -0.47662144\n",
      "  0.47772156 -0.46434097 -0.36456446 -0.18478548] [ 0.13407709  0.26840527 -0.00769052 -0.14206902  0.52434638  0.28546475\n",
      " -0.02017617  0.42043382  0.17613356  0.47560644]\n",
      "\n",
      "# 26 Gradient out:  [3.05341846 2.99292623 2.98107199 0.92926134 1.19003702 0.85104682\n",
      " 3.05324753 0.86517526 0.99776324 1.36071138]\n",
      "\n",
      "     Weights  out:  [ 0.47787269  0.42753074  0.41836463 -0.41283824 -0.25720245 -0.47662144\n",
      "  0.47772156 -0.46434097 -0.36456446 -0.18478548] [-0.2747187  -0.12992233 -0.40391878 -0.23151686  0.3888262   0.20964081\n",
      " -0.42894312  0.34220689  0.0743898   0.31270358]\n",
      "\n",
      "# 27 Gradient out:  [-0.98031411 -0.9579615  -0.9534415  -0.19184245 -0.29542394 -0.16268123\n",
      " -0.98025292 -0.16778844 -0.21866204 -0.36153207]\n",
      "\n",
      "     Weights  out:  [ 0.47787269  0.42753074  0.41836463 -0.41283824 -0.25720245 -0.47662144\n",
      "  0.47772156 -0.46434097 -0.36456446 -0.18478548] [ 0.33596499  0.46866292  0.19229562 -0.04566459  0.62683361  0.37985017\n",
      "  0.18170638  0.51524194  0.27394245  0.58484586]\n",
      "\n",
      "# 28 Gradient out:  [-2.01522215 -1.96391202 -1.95362083 -0.42696813 -0.65358477 -0.36018782\n",
      " -2.01508079 -0.37196664 -0.48728649 -0.78932728]\n",
      "\n",
      "     Weights  out:  [ 0.47787269  0.42753074  0.41836463 -0.41283824 -0.25720245 -0.47662144\n",
      "  0.47772156 -0.46434097 -0.36456446 -0.18478548] [ 0.13990217  0.27707062  0.00160732 -0.08403308  0.56774882  0.34731393\n",
      " -0.0143442   0.48168425  0.23021004  0.51253944]\n",
      "\n",
      "# 29 Gradient out:  [2.97113068 2.91155619 2.89984302 0.80022544 1.06145891 0.72312476\n",
      " 2.97096278 0.73701205 0.86822487 1.23542357]\n",
      "\n",
      "     Weights  out:  [ 0.47787269  0.42753074  0.41836463 -0.41283824 -0.25720245 -0.47662144\n",
      "  0.47772156 -0.46434097 -0.36456446 -0.18478548] [-0.26314226 -0.11571179 -0.38911685 -0.1694267   0.43703187  0.27527636\n",
      " -0.41736036  0.40729092  0.13275274  0.35467399]\n",
      "\n",
      "# 30 Gradient out:  [-0.94820924 -0.9264933  -0.92210015 -0.1798564  -0.28066271 -0.15152224\n",
      " -0.94814982 -0.1564826  -0.20593652 -0.3450829 ]\n",
      "\n",
      "     Weights  out:  [ 0.47787269  0.42753074  0.41836463 -0.41283824 -0.25720245 -0.47662144\n",
      "  0.47772156 -0.46434097 -0.36456446 -0.18478548] [ 0.33108388  0.46659945  0.19085176 -0.00938162  0.64932365  0.41990131\n",
      "  0.1768322   0.55469334  0.30639772  0.6017587 ]\n",
      "\n",
      "# 31 Gradient out:  [-1.97418354 -1.9239484  -1.91387106 -0.40864534 -0.63094377 -0.34326077\n",
      " -1.97404515 -0.35479208 -0.46773605 -0.76457196]\n",
      "\n",
      "     Weights  out:  [ 0.47787269  0.42753074  0.41836463 -0.41283824 -0.25720245 -0.47662144\n",
      "  0.47772156 -0.46434097 -0.36456446 -0.18478548] [ 0.14144203  0.28130079  0.00643173 -0.0453529   0.59319111  0.38959687\n",
      " -0.01279777  0.52339682  0.26521041  0.53274212]\n",
      "\n",
      "# 32 Gradient out:  [2.84115476 2.78445444 2.77326092 0.68322101 0.9370055  0.60975474\n",
      " 2.84099548 0.6229406  0.74856834 1.10942051]\n",
      "\n",
      "     Weights  out:  [ 0.47787269  0.42753074  0.41836463 -0.41283824 -0.25720245 -0.47662144\n",
      "  0.47772156 -0.46434097 -0.36456446 -0.18478548] [-0.25339468 -0.10348889 -0.37634249 -0.12708197  0.46700235  0.32094471\n",
      " -0.4076068   0.4524384   0.1716632   0.37982773]\n",
      "\n",
      "# 33 Gradient out:  [-1.00506923 -0.98192093 -0.97723775 -0.18719897 -0.29463585 -0.15699545\n",
      " -1.00500589 -0.16228273 -0.21500063 -0.36323934]\n",
      "\n",
      "     Weights  out:  [ 0.47787269  0.42753074  0.41836463 -0.41283824 -0.25720245 -0.47662144\n",
      "  0.47772156 -0.46434097 -0.36456446 -0.18478548] [0.31483627 0.453402   0.1783097  0.00956224 0.65440345 0.44289566\n",
      " 0.1605923  0.57702652 0.32137687 0.60171183]\n",
      "\n",
      "# 34 Gradient out:  [-1.96458371 -1.9127877  -1.90242806 -0.42107468 -0.64652517 -0.35371642\n",
      " -1.96444069 -0.36562681 -0.48156235 -0.77912248]\n",
      "\n",
      "     Weights  out:  [ 0.47787269  0.42753074  0.41836463 -0.41283824 -0.25720245 -0.47662144\n",
      "  0.47772156 -0.46434097 -0.36456446 -0.18478548] [ 0.11382243  0.25701781 -0.01713785 -0.02787756  0.59547628  0.41149657\n",
      " -0.04040888  0.54456997  0.27837674  0.52906396]\n",
      "\n",
      "# 35 Gradient out:  [2.81852569 2.76061355 2.74923071 0.71694772 0.97043811 0.64200498\n",
      " 2.81836245 0.65550703 0.78299835 1.13891002]\n",
      "\n",
      "     Weights  out:  [ 0.47787269  0.42753074  0.41836463 -0.41283824 -0.25720245 -0.47662144\n",
      "  0.47772156 -0.46434097 -0.36456446 -0.18478548] [-0.27909431 -0.12553973 -0.39762346 -0.11209249  0.46617125  0.34075329\n",
      " -0.43329702  0.47144461  0.18206427  0.37323946]\n",
      "\n",
      "# 36 Gradient out:  [-1.13729935 -1.11082786 -1.10547367 -0.20771522 -0.3303394  -0.17317814\n",
      " -1.13722691 -0.17922523 -0.23948583 -0.40840358]\n",
      "\n",
      "     Weights  out:  [ 0.47787269  0.42753074  0.41836463 -0.41283824 -0.25720245 -0.47662144\n",
      "  0.47772156 -0.46434097 -0.36456446 -0.18478548] [0.28461082 0.42658298 0.15222268 0.03129705 0.66025887 0.46915428\n",
      " 0.13037547 0.60254602 0.33866394 0.60102147]\n",
      "\n",
      "# 37 Gradient out:  [-1.73152852 -1.68048113 -1.67036539 -0.42784188 -0.63849061 -0.36163414\n",
      " -1.73138652 -0.37343575 -0.48610958 -0.75321428]\n",
      "\n",
      "     Weights  out:  [ 0.47787269  0.42753074  0.41836463 -0.41283824 -0.25720245 -0.47662144\n",
      "  0.47772156 -0.46434097 -0.36456446 -0.18478548] [ 0.05715095  0.20441741 -0.06887206 -0.01024599  0.59419099  0.43451865\n",
      " -0.09706991  0.56670097  0.29076677  0.51934075]\n",
      "\n",
      "# 38 Gradient out:  [2.78005427 2.72229085 2.7109498  0.70927869 0.96068936 0.634552\n",
      " 2.7798913  0.64802804 0.77498561 1.12683274]\n",
      "\n",
      "     Weights  out:  [ 0.47787269  0.42753074  0.41836463 -0.41283824 -0.25720245 -0.47662144\n",
      "  0.47772156 -0.46434097 -0.36456446 -0.18478548] [-0.28915475 -0.13167882 -0.40294513 -0.09581437  0.46649287  0.36219183\n",
      " -0.44334721  0.49201382  0.19354486  0.36869789]\n",
      "\n",
      "# 39 Gradient out:  [-1.21492543 -1.18645529 -1.18069835 -0.21998399 -0.35164459 -0.18284213\n",
      " -1.2148475  -0.18934673 -0.25413012 -0.43526449]\n",
      "\n",
      "     Weights  out:  [ 0.47787269  0.42753074  0.41836463 -0.41283824 -0.25720245 -0.47662144\n",
      "  0.47772156 -0.46434097 -0.36456446 -0.18478548] [0.2668561  0.41277936 0.13924483 0.04604137 0.65863074 0.48910223\n",
      " 0.11263105 0.62161943 0.34854198 0.59406444]\n",
      "\n",
      "# 40 Gradient out:  [-1.44204476 -1.39472446 -1.38542312 -0.41040397 -0.59631724 -0.34917274\n",
      " -1.4419123  -0.36016357 -0.46333055 -0.6896978 ]\n",
      "\n",
      "     Weights  out:  [ 0.47787269  0.42753074  0.41836463 -0.41283824 -0.25720245 -0.47662144\n",
      "  0.47772156 -0.46434097 -0.36456446 -0.18478548] [ 0.02387102  0.1754883  -0.09689484  0.00204457  0.58830182  0.4525338\n",
      " -0.13033845  0.58375008  0.29771596  0.50701154]\n",
      "\n",
      "# 41 Gradient out:  [2.72581281 2.67083982 2.65997841 0.61940729 0.86634969 0.54816244\n",
      " 2.72565849 0.56094019 0.68288084 1.03460716]\n",
      "\n",
      "     Weights  out:  [ 0.47787269  0.42753074  0.41836463 -0.41283824 -0.25720245 -0.47662144\n",
      "  0.47772156 -0.46434097 -0.36456446 -0.18478548] [-0.26453793 -0.1034566  -0.37397947 -0.08003622  0.46903837  0.38269925\n",
      " -0.41872091  0.51171737  0.20504985  0.36907198]\n",
      "\n",
      "# 42 Gradient out:  [-1.14167716 -1.11507786 -1.10969776 -0.20767326 -0.33089101 -0.17296932\n",
      " -1.14160436 -0.17904555 -0.23959786 -0.40932934]\n",
      "\n",
      "     Weights  out:  [ 0.47787269  0.42753074  0.41836463 -0.41283824 -0.25720245 -0.47662144\n",
      "  0.47772156 -0.46434097 -0.36456446 -0.18478548] [0.28062463 0.43071137 0.15801621 0.04384523 0.64230831 0.49233174\n",
      " 0.12641078 0.62390541 0.34162602 0.57599342]\n",
      "\n",
      "# 43 Gradient out:  [-1.71834966 -1.66739948 -1.65730753 -0.42658799 -0.63631106 -0.36051484\n",
      " -1.71820789 -0.37229709 -0.48468143 -0.75011398]\n",
      "\n",
      "     Weights  out:  [ 0.47787269  0.42753074  0.41836463 -0.41283824 -0.25720245 -0.47662144\n",
      "  0.47772156 -0.46434097 -0.36456446 -0.18478548] [ 0.0522892   0.2076958  -0.06392334  0.00231058  0.57613011  0.45773788\n",
      " -0.10191009  0.5880963   0.29370644  0.49412755]\n",
      "\n",
      "# 44 Gradient out:  [2.77042043 2.71281859 2.70150582 0.69947636 0.9505421  0.62495224\n",
      " 2.77025796 0.63838813 0.76504547 1.1166738 ]\n",
      "\n",
      "     Weights  out:  [ 0.47787269  0.42753074  0.41836463 -0.41283824 -0.25720245 -0.47662144\n",
      "  0.47772156 -0.46434097 -0.36456446 -0.18478548] [-0.29138073 -0.1257841  -0.39538484 -0.08300702  0.4488679   0.38563491\n",
      " -0.44555167  0.51363688  0.19677016  0.34410475]\n",
      "\n",
      "# 45 Gradient out:  [-1.22356482 -1.19486518 -1.18906197 -0.22101345 -0.35371569 -0.18357244\n",
      " -1.22348626 -0.19012957 -0.25543273 -0.43798037]\n",
      "\n",
      "     Weights  out:  [ 0.47787269  0.42753074  0.41836463 -0.41283824 -0.25720245 -0.47662144\n",
      "  0.47772156 -0.46434097 -0.36456446 -0.18478548] [0.26270335 0.41677962 0.14491632 0.05688826 0.63897632 0.51062536\n",
      " 0.10849993 0.64131451 0.34977925 0.56743951]\n",
      "\n",
      "# 46 Gradient out:  [-1.40752944 -1.36070699 -1.35151233 -0.40724641 -0.5901161  -0.34667598\n",
      " -1.40739828 -0.35755711 -0.45948966 -0.68100916]\n",
      "\n",
      "     Weights  out:  [ 0.47787269  0.42753074  0.41836463 -0.41283824 -0.25720245 -0.47662144\n",
      "  0.47772156 -0.46434097 -0.36456446 -0.18478548] [ 0.01799039  0.17780659 -0.09289607  0.01268557  0.56823318  0.47391087\n",
      " -0.13619733  0.60328859  0.29869271  0.47984344]\n",
      "\n",
      "# 47 Gradient out:  [2.71264454 2.65820524 2.64743784 0.6039089  0.84974045 0.53333427\n",
      " 2.71249185 0.54598005 0.66692378 1.0180656 ]\n",
      "\n",
      "     Weights  out:  [ 0.47787269  0.42753074  0.41836463 -0.41283824 -0.25720245 -0.47662144\n",
      "  0.47772156 -0.46434097 -0.36456446 -0.18478548] [-0.2635155  -0.09433481 -0.36319854 -0.06876372  0.45020996  0.40457567\n",
      " -0.41767698  0.53177717  0.20679477  0.34364161]\n",
      "\n",
      "# 48 Gradient out:  [-1.14034233 -1.11377723 -1.10840404 -0.20734512 -0.3304098  -0.17268579\n",
      " -1.14026964 -0.17875421 -0.23922895 -0.40875909]\n",
      "\n",
      "     Weights  out:  [ 0.47787269  0.42753074  0.41836463 -0.41283824 -0.25720245 -0.47662144\n",
      "  0.47772156 -0.46434097 -0.36456446 -0.18478548] [0.27901341 0.43730623 0.16628903 0.05201806 0.62015805 0.51124252\n",
      " 0.12482139 0.64097318 0.34017953 0.54725472]\n",
      "\n",
      "# 49 Gradient out:  [-1.72879061 -1.67771581 -1.66759599 -0.42678007 -0.63739696 -0.36053933\n",
      " -1.72864852 -0.37234823 -0.48506059 -0.75199947]\n",
      "\n",
      "     Weights  out:  [ 0.47787269  0.42753074  0.41836463 -0.41283824 -0.25720245 -0.47662144\n",
      "  0.47772156 -0.46434097 -0.36456446 -0.18478548] [ 0.05094494  0.21455079 -0.05539178  0.01054904  0.55407609  0.47670537\n",
      " -0.10323254  0.60522234  0.29233374  0.46550291]\n",
      "\n",
      "Tanh error cuadrático medio (ECM): 0.9128616509592062\n",
      "\n",
      "# 0 Gradient out:  [-0.68111952 -0.20640712 -0.9840386  -1.05797301 -1.2380705  -0.31154183\n",
      " -0.40327749 -0.47535043 -1.26589629 -0.11523704]\n",
      "\n",
      "     Weights  out:  [ 0.00972266 -0.32331137  0.21355346  0.26756238  0.43721346 -0.24051704\n",
      " -0.17619059 -0.12773167  0.47494742 -0.41313706] [-0.07372945 -0.30176844  0.08498808 -0.18476651  0.45044345  0.20624566\n",
      "  0.24528926  0.29304745 -0.0548175  -0.11210964]\n",
      "\n",
      "# 1 Gradient out:  [2.26522186 1.17062223 3.02713375 3.16177655 3.4431515  1.34692179\n",
      " 1.52797031 1.69315109 3.48678898 1.03212958]\n",
      "\n",
      "     Weights  out:  [ 0.00972266 -0.32331137  0.21355346  0.26756238  0.43721346 -0.24051704\n",
      " -0.17619059 -0.12773167  0.47494742 -0.41313706] [-0.20995335 -0.34304987 -0.11181964 -0.39636111  0.20282935  0.1439373\n",
      "  0.16463376  0.19797736 -0.30799676 -0.13515705]\n",
      "\n",
      "# 2 Gradient out:  [-0.19648944 -0.10489207 -0.25755556 -0.2703209  -0.29972859 -0.12244988\n",
      " -0.13885908 -0.15270546 -0.30435734 -0.09014222]\n",
      "\n",
      "     Weights  out:  [ 0.00972266 -0.32331137  0.21355346  0.26756238  0.43721346 -0.24051704\n",
      " -0.17619059 -0.12773167  0.47494742 -0.41313706] [ 0.24309102 -0.10892542  0.49360711  0.2359942   0.89145965  0.41332165\n",
      "  0.47022782  0.53660758  0.38936104  0.07126886]\n",
      "\n",
      "# 3 Gradient out:  [-0.29742209 -0.15436943 -0.39281974 -0.41274655 -0.45856748 -0.18176207\n",
      " -0.20738518 -0.22901436 -0.46575857 -0.13139639]\n",
      "\n",
      "     Weights  out:  [ 0.00972266 -0.32331137  0.21355346  0.26756238  0.43721346 -0.24051704\n",
      " -0.17619059 -0.12773167  0.47494742 -0.41313706] [ 0.20379313 -0.12990384  0.44209599  0.18193002  0.83151394  0.38883168\n",
      "  0.44245601  0.50606648  0.32848957  0.05324042]\n",
      "\n",
      "# 4 Gradient out:  [-0.53691973 -0.26577735 -0.71775258 -0.75553102 -0.84219648 -0.31768129\n",
      " -0.36626756 -0.40727851 -0.85573241 -0.22234935]\n",
      "\n",
      "     Weights  out:  [ 0.00972266 -0.32331137  0.21355346  0.26756238  0.43721346 -0.24051704\n",
      " -0.17619059 -0.12773167  0.47494742 -0.41313706] [ 0.14430871 -0.16077772  0.36353205  0.09938071  0.73980044  0.35247926\n",
      "  0.40097897  0.46026361  0.23533785  0.02696114]\n",
      "\n",
      "# 5 Gradient out:  [-1.16366752 -0.51927797 -1.59161834 -1.68248856 -1.8917604  -0.64452156\n",
      " -0.76095872 -0.85850673 -1.92421029 -0.4143561 ]\n",
      "\n",
      "     Weights  out:  [ 0.00972266 -0.32331137  0.21355346  0.26756238  0.43721346 -0.24051704\n",
      " -0.17619059 -0.12773167  0.47494742 -0.41313706] [ 0.03692477 -0.21393319  0.21998153 -0.0517255   0.57136114  0.28894301\n",
      "  0.32772546  0.37880791  0.06419137 -0.01750873]\n",
      "\n",
      "# 6 Gradient out:  [2.23303148 1.14936349 2.98799041 3.12093117 3.39800343 1.32321882\n",
      " 1.5021645  1.66569652 3.44092958 1.01306572]\n",
      "\n",
      "     Weights  out:  [ 0.00972266 -0.32331137  0.21355346  0.26756238  0.43721346 -0.24051704\n",
      " -0.17619059 -0.12773167  0.47494742 -0.41313706] [-0.19580874 -0.31778878 -0.09834214 -0.38822321  0.19300906  0.16003869\n",
      "  0.17553371  0.20710656 -0.32065069 -0.10037995]\n",
      "\n",
      "# 7 Gradient out:  [-0.19835651 -0.10419251 -0.26115232 -0.27426695 -0.30444269 -0.12222301\n",
      " -0.13908631 -0.15332216 -0.30918532 -0.08906125]\n",
      "\n",
      "     Weights  out:  [ 0.00972266 -0.32331137  0.21355346  0.26756238  0.43721346 -0.24051704\n",
      " -0.17619059 -0.12773167  0.47494742 -0.41313706] [ 0.25079756 -0.08791609  0.49925594  0.23596302  0.87260975  0.42468246\n",
      "  0.47596662  0.54024587  0.36753523  0.10223319]\n",
      "\n",
      "# 8 Gradient out:  [-0.30217378 -0.15439372 -0.40074915 -0.42132326 -0.46858139 -0.18266547\n",
      " -0.20912789 -0.23147401 -0.47598807 -0.1307055 ]\n",
      "\n",
      "     Weights  out:  [ 0.00972266 -0.32331137  0.21355346  0.26756238  0.43721346 -0.24051704\n",
      " -0.17619059 -0.12773167  0.47494742 -0.41313706] [ 0.21112626 -0.10875459  0.44702548  0.18110963  0.81172121  0.40023786\n",
      "  0.44814935  0.50958144  0.30569816  0.08442094]\n",
      "\n",
      "# 9 Gradient out:  [-0.5518736  -0.26934131 -0.74032395 -0.77968159 -0.86991043 -0.32340364\n",
      " -0.37402724 -0.41676428 -0.8839896  -0.22413414]\n",
      "\n",
      "     Weights  out:  [ 0.00972266 -0.32331137  0.21355346  0.26756238  0.43721346 -0.24051704\n",
      " -0.17619059 -0.12773167  0.47494742 -0.41313706] [ 0.1506915  -0.13963333  0.36687565  0.09684498  0.71800493  0.36370476\n",
      "  0.40632378  0.46328664  0.21050055  0.05827985]\n",
      "\n",
      "# 10 Gradient out:  [-1.19546824 -0.53093985 -1.6362216  -1.73024095 -1.94739198 -0.66069404\n",
      " -0.78102393 -0.88161861 -1.98112281 -0.42200779]\n",
      "\n",
      "     Weights  out:  [ 0.00972266 -0.32331137  0.21355346  0.26756238  0.43721346 -0.24051704\n",
      " -0.17619059 -0.12773167  0.47494742 -0.41313706] [ 0.04031678 -0.19350159  0.21881086 -0.05909134  0.54402285  0.29902404\n",
      "  0.33151833  0.37993378  0.03370263  0.01345302]\n",
      "\n",
      "# 11 Gradient out:  [2.27488756 1.1790965  3.0353016  3.1713132  3.45872187 1.35801292\n",
      " 1.54023572 1.70555777 3.5036545  1.03729629]\n",
      "\n",
      "     Weights  out:  [ 0.00972266 -0.32331137  0.21355346  0.26756238  0.43721346 -0.24051704\n",
      " -0.17619059 -0.12773167  0.47494742 -0.41313706] [-0.19877687 -0.29968957 -0.10843346 -0.40513953  0.15454445  0.16688523\n",
      "  0.17531354  0.20361006 -0.36252193 -0.07094854]\n",
      "\n",
      "# 12 Gradient out:  [-0.20763213 -0.10670506 -0.27496429 -0.28900912 -0.32127204 -0.12600331\n",
      " -0.14406992 -0.15933042 -0.32633234 -0.09053276]\n",
      "\n",
      "     Weights  out:  [ 0.00972266 -0.32331137  0.21355346  0.26756238  0.43721346 -0.24051704\n",
      " -0.17619059 -0.12773167  0.47494742 -0.41313706] [ 0.25620064 -0.06387027  0.49862686  0.22912311  0.84628883  0.43848781\n",
      "  0.48336069  0.54472161  0.33820897  0.13651072]\n",
      "\n",
      "# 13 Gradient out:  [-0.32307501 -0.16143708 -0.430929   -0.45341747 -0.50499584 -0.19232381\n",
      " -0.22125828 -0.24570336 -0.51306378 -0.13559146]\n",
      "\n",
      "     Weights  out:  [ 0.00972266 -0.32331137  0.21355346  0.26756238  0.43721346 -0.24051704\n",
      " -0.17619059 -0.12773167  0.47494742 -0.41313706] [ 0.21467422 -0.08521128  0.443634    0.17132129  0.78203442  0.41328715\n",
      "  0.4545467   0.51285553  0.2729425   0.11840416]\n",
      "\n",
      "# 14 Gradient out:  [-0.61188374 -0.29200918 -0.82524145 -0.86980855 -0.97190542 -0.35321736\n",
      " -0.41054358 -0.45893565 -0.98781232 -0.24086449]\n",
      "\n",
      "     Weights  out:  [ 0.00972266 -0.32331137  0.21355346  0.26756238  0.43721346 -0.24051704\n",
      " -0.17619059 -0.12773167  0.47494742 -0.41313706] [ 0.15005922 -0.11749869  0.3574482   0.08063779  0.68103525  0.37482239\n",
      "  0.41029505  0.46371485  0.17032974  0.09128587]\n",
      "\n",
      "# 15 Gradient out:  [-1.25255063 -0.54916458 -1.7164959  -1.81742204 -2.05315931 -0.68919593\n",
      " -0.81772775 -0.92420616 -2.08996771 -0.43066091]\n",
      "\n",
      "     Weights  out:  [ 0.00972266 -0.32331137  0.21355346  0.26756238  0.43721346 -0.24051704\n",
      " -0.17619059 -0.12773167  0.47494742 -0.41313706] [ 0.02768247 -0.17590053  0.19239991 -0.09332392  0.48665417  0.30417892\n",
      "  0.32818633  0.37192772 -0.02723272  0.04311297]\n",
      "\n",
      "# 16 Gradient out:  [2.17683104 1.17359209 2.86704255 2.99465377 3.27291063 1.34362326\n",
      " 1.51288515 1.66410079 3.31749439 1.03539255]\n",
      "\n",
      "     Weights  out:  [ 0.00972266 -0.32331137  0.21355346  0.26756238  0.43721346 -0.24051704\n",
      " -0.17619059 -0.12773167  0.47494742 -0.41313706] [-0.22282766 -0.28573345 -0.15089927 -0.45680833  0.07602231  0.16633973\n",
      "  0.16464078  0.18708649 -0.44522626 -0.04301921]\n",
      "\n",
      "# 17 Gradient out:  [-0.37991594 -0.18286687 -0.51145538 -0.53884784 -0.60152969 -0.22046098\n",
      " -0.25572164 -0.28552934 -0.61130272 -0.1514728 ]\n",
      "\n",
      "     Weights  out:  [ 0.00972266 -0.32331137  0.21355346  0.26756238  0.43721346 -0.24051704\n",
      " -0.17619059 -0.12773167  0.47494742 -0.41313706] [ 0.21253855 -0.05101503  0.42250924  0.14212243  0.73060443  0.43506438\n",
      "  0.46721781  0.51990665  0.21827261  0.1640593 ]\n",
      "\n",
      "# 18 Gradient out:  [-0.78043236 -0.35881769 -1.06144298 -1.12031443 -1.25524084 -0.43971159\n",
      " -0.51538933 -0.57918672 -1.27622841 -0.29122489]\n",
      "\n",
      "     Weights  out:  [ 0.00972266 -0.32331137  0.21355346  0.26756238  0.43721346 -0.24051704\n",
      " -0.17619059 -0.12773167  0.47494742 -0.41313706] [ 0.13655536 -0.0875884   0.32021817  0.03435286  0.61029849  0.39097219\n",
      "  0.41607348  0.46280078  0.09601207  0.13376474]\n",
      "\n",
      "# 19 Gradient out:  [-0.77390444 -0.35193963 -1.03557904 -1.10515428 -1.28498757 -0.45329419\n",
      " -0.53782565 -0.60165788 -1.31443176 -0.25991287]\n",
      "\n",
      "     Weights  out:  [ 0.00972266 -0.32331137  0.21355346  0.26756238  0.43721346 -0.24051704\n",
      " -0.17619059 -0.12773167  0.47494742 -0.41313706] [-0.01953111 -0.15935194  0.10792957 -0.18971002  0.35925033  0.30302987\n",
      "  0.31299562  0.34696344 -0.15923361  0.07551976]\n",
      "\n",
      "# 20 Gradient out:  [2.23471473 1.15723405 2.98079316 3.11535813 3.40224063 1.33485856\n",
      " 1.51467563 1.6771843  3.44745846 1.01542168]\n",
      "\n",
      "     Weights  out:  [ 0.00972266 -0.32331137  0.21355346  0.26756238  0.43721346 -0.24051704\n",
      " -0.17619059 -0.12773167  0.47494742 -0.41313706] [-0.174312   -0.22973986 -0.09918624 -0.41074088  0.10225281  0.21237103\n",
      "  0.20543049  0.22663186 -0.42211997  0.02353719]\n",
      "\n",
      "# 21 Gradient out:  [-0.22805151 -0.11097822 -0.30623057 -0.32248865 -0.35968368 -0.13328574\n",
      " -0.15422018 -0.17192838 -0.36548798 -0.09234956]\n",
      "\n",
      "     Weights  out:  [ 0.00972266 -0.32331137  0.21355346  0.26756238  0.43721346 -0.24051704\n",
      " -0.17619059 -0.12773167  0.47494742 -0.41313706] [0.27263095 0.00170695 0.49697239 0.21233075 0.78270094 0.47934274\n",
      " 0.50836561 0.56206872 0.26737173 0.22662152]\n",
      "\n",
      "# 22 Gradient out:  [-0.37201413 -0.17619097 -0.50277095 -0.52997669 -0.59216276 -0.2135141\n",
      " -0.24854425 -0.27816929 -0.60184589 -0.1450524 ]\n",
      "\n",
      "     Weights  out:  [ 0.00972266 -0.32331137  0.21355346  0.26756238  0.43721346 -0.24051704\n",
      " -0.17619059 -0.12773167  0.47494742 -0.41313706] [ 0.22702065 -0.0204887   0.43572628  0.14783301  0.7107642   0.45268559\n",
      "  0.47752158  0.52768305  0.19427413  0.20815161]\n",
      "\n",
      "# 23 Gradient out:  [-0.76045322 -0.34612347 -1.03663841 -1.09447838 -1.22699327 -0.42558856\n",
      " -0.49994858 -0.56264569 -1.24759996 -0.27974371]\n",
      "\n",
      "     Weights  out:  [ 0.00972266 -0.32331137  0.21355346  0.26756238  0.43721346 -0.24051704\n",
      " -0.17619059 -0.12773167  0.47494742 -0.41313706] [ 0.15261782 -0.05572689  0.33517209  0.04183768  0.59233165  0.40998277\n",
      "  0.42781273  0.47204919  0.07390495  0.17914113]\n",
      "\n",
      "# 24 Gradient out:  [-0.88565782 -0.41672328 -1.17920298 -1.25497222 -1.44904529 -0.52650074\n",
      " -0.61912233 -0.69001572 -1.48088765 -0.31754692]\n",
      "\n",
      "     Weights  out:  [ 0.00972266 -0.32331137  0.21355346  0.26756238  0.43721346 -0.24051704\n",
      " -0.17619059 -0.12773167  0.47494742 -0.41313706] [ 0.00052718 -0.12495159  0.12784441 -0.177058    0.346933    0.32486506\n",
      "  0.32782301  0.35952005 -0.17561504  0.12319239]\n",
      "\n",
      "# 25 Gradient out:  [2.21845776 1.17358671 2.93867331 3.0708455  3.35749188 1.3492553\n",
      " 1.52494042 1.682439   3.40331809 1.0313755 ]\n",
      "\n",
      "     Weights  out:  [ 0.00972266 -0.32331137  0.21355346  0.26756238  0.43721346 -0.24051704\n",
      " -0.17619059 -0.12773167  0.47494742 -0.41313706] [-0.17660439 -0.20829624 -0.10799619 -0.42805245  0.05712394  0.21956492\n",
      "  0.20399855  0.22151691 -0.47179257  0.05968301]\n",
      "\n",
      "# 26 Gradient out:  [-0.27935181 -0.13157444 -0.37807329 -0.39857956 -0.44539771 -0.1596921\n",
      " -0.18610785 -0.20846498 -0.45268273 -0.10813642]\n",
      "\n",
      "     Weights  out:  [ 0.00972266 -0.32331137  0.21355346  0.26756238  0.43721346 -0.24051704\n",
      " -0.17619059 -0.12773167  0.47494742 -0.41313706] [0.26708716 0.0264211  0.47973847 0.18611665 0.72862231 0.48941598\n",
      " 0.50898663 0.55800471 0.20887105 0.26595811]\n",
      "\n",
      "# 27 Gradient out:  [-0.50133788 -0.22931044 -0.68295477 -0.72077058 -0.80713282 -0.28118296\n",
      " -0.32987132 -0.37103356 -0.82054962 -0.18607544]\n",
      "\n",
      "     Weights  out:  [ 0.00972266 -0.32331137  0.21355346  0.26756238  0.43721346 -0.24051704\n",
      " -0.17619059 -0.12773167  0.47494742 -0.41313706] [2.11216802e-01 1.06209408e-04 4.04123817e-01 1.06400742e-01\n",
      " 6.39542773e-01 4.57477555e-01 4.71765058e-01 5.16311713e-01\n",
      " 1.18334503e-01 2.44330822e-01]\n",
      "\n",
      "# 28 Gradient out:  [-1.11998672 -0.49802867 -1.53228711 -1.62035758 -1.82452722 -0.61969853\n",
      " -0.73235089 -0.82646644 -1.85646117 -0.39552781]\n",
      "\n",
      "     Weights  out:  [ 0.00972266 -0.32331137  0.21355346  0.26756238  0.43721346 -0.24051704\n",
      " -0.17619059 -0.12773167  0.47494742 -0.41313706] [ 0.11094923 -0.04575588  0.26753286 -0.03775337  0.47811621  0.40124096\n",
      "  0.40579079  0.442105   -0.04577542  0.20711573]\n",
      "\n",
      "# 29 Gradient out:  [1.8866071  0.89449399 2.58233685 2.70167939 2.94400146 1.0489184\n",
      " 1.21087424 1.36069033 2.98079725 0.77597918]\n",
      "\n",
      "     Weights  out:  [ 0.00972266 -0.32331137  0.21355346  0.26756238  0.43721346 -0.24051704\n",
      " -0.17619059 -0.12773167  0.47494742 -0.41313706] [-0.11304812 -0.14536161 -0.03892456 -0.36182489  0.11321076  0.27730126\n",
      "  0.25932062  0.27681171 -0.41706766  0.12801017]\n",
      "\n",
      "# 30 Gradient out:  [-0.30947161 -0.1440786  -0.41996741 -0.44291664 -0.49528562 -0.17554146\n",
      " -0.2051063  -0.2301302  -0.50342729 -0.11786469]\n",
      "\n",
      "     Weights  out:  [ 0.00972266 -0.32331137  0.21355346  0.26756238  0.43721346 -0.24051704\n",
      " -0.17619059 -0.12773167  0.47494742 -0.41313706] [0.2642733  0.03353718 0.47754281 0.17851099 0.70201106 0.48708494\n",
      " 0.50149546 0.54894978 0.17909179 0.28320601]\n",
      "\n",
      "# 31 Gradient out:  [-0.5837816  -0.2637425  -0.79737415 -0.84191082 -0.94366933 -0.3248527\n",
      " -0.38217533 -0.43060555 -0.95947283 -0.21279597]\n",
      "\n",
      "     Weights  out:  [ 0.00972266 -0.32331137  0.21355346  0.26756238  0.43721346 -0.24051704\n",
      " -0.17619059 -0.12773167  0.47494742 -0.41313706] [0.20237898 0.00472147 0.39354933 0.08992766 0.60295393 0.45197665\n",
      " 0.4604742  0.50292374 0.07840634 0.25963307]\n",
      "\n",
      "# 32 Gradient out:  [-1.24933131 -0.5601072  -1.70270961 -1.80218914 -2.03673742 -0.69859693\n",
      " -0.82496724 -0.92923489 -2.07379537 -0.4419797 ]\n",
      "\n",
      "     Weights  out:  [ 0.00972266 -0.32331137  0.21355346  0.26756238  0.43721346 -0.24051704\n",
      " -0.17619059 -0.12773167  0.47494742 -0.41313706] [ 0.08562266 -0.04802704  0.2340745  -0.0784545   0.41422007  0.38700611\n",
      "  0.38403914  0.41680263 -0.11348823  0.21707387]\n",
      "\n",
      "# 33 Gradient out:  [2.16200968 1.16077126 2.84962746 2.97754033 3.258976   1.33173142\n",
      " 1.50104019 1.65186636 3.30457112 1.02072075]\n",
      "\n",
      "     Weights  out:  [ 0.00972266 -0.32331137  0.21355346  0.26756238  0.43721346 -0.24051704\n",
      " -0.17619059 -0.12773167  0.47494742 -0.41313706] [-0.1642436  -0.16004847 -0.10646742 -0.43889233  0.00687258  0.24728672\n",
      "  0.21904569  0.23095565 -0.5282473   0.12867793]\n",
      "\n",
      "# 34 Gradient out:  [-0.34568023 -0.15599621 -0.47245028 -0.49875172 -0.55865566 -0.19203244\n",
      " -0.22592918 -0.25463414 -0.56794385 -0.12602295]\n",
      "\n",
      "     Weights  out:  [ 0.00972266 -0.32331137  0.21355346  0.26756238  0.43721346 -0.24051704\n",
      " -0.17619059 -0.12773167  0.47494742 -0.41313706] [0.26815834 0.07210578 0.46345807 0.15661574 0.65866778 0.513633\n",
      " 0.51925373 0.56132892 0.13266692 0.33282208]\n",
      "\n",
      "# 35 Gradient out:  [-0.68969652 -0.30363984 -0.9471698  -1.00099926 -1.12410117 -0.37754303\n",
      " -0.44678359 -0.50521124 -1.14321052 -0.24199865]\n",
      "\n",
      "     Weights  out:  [ 0.00972266 -0.32331137  0.21355346  0.26756238  0.43721346 -0.24051704\n",
      " -0.17619059 -0.12773167  0.47494742 -0.41313706] [0.19902229 0.04090654 0.36896801 0.05686539 0.54693665 0.47522651\n",
      " 0.47406789 0.5104021  0.01907815 0.30761749]\n",
      "\n",
      "# 36 Gradient out:  [-1.14108454 -0.54699813 -1.5210814  -1.6125673  -1.84068734 -0.67762327\n",
      " -0.79118478 -0.88093333 -1.87802805 -0.43093132]\n",
      "\n",
      "     Weights  out:  [ 0.00972266 -0.32331137  0.21355346  0.26756238  0.43721346 -0.24051704\n",
      " -0.17619059 -0.12773167  0.47494742 -0.41313706] [ 0.06108299 -0.01982143  0.17953405 -0.14333446  0.32211642  0.39971791\n",
      "  0.38471117  0.40935985 -0.20956395  0.25921776]\n",
      "\n",
      "# 37 Gradient out:  [2.06744225 1.14885089 2.69474058 2.81386363 3.08168363 1.30940693\n",
      " 1.46608009 1.60431316 3.12593367 1.01497977]\n",
      "\n",
      "     Weights  out:  [ 0.00972266 -0.32331137  0.21355346  0.26756238  0.43721346 -0.24051704\n",
      " -0.17619059 -0.12773167  0.47494742 -0.41313706] [-0.16713392 -0.12922106 -0.12468223 -0.46584792 -0.04602105  0.26419325\n",
      "  0.22647422  0.23317318 -0.58516956  0.1730315 ]\n",
      "\n",
      "# 38 Gradient out:  [-0.50913907 -0.22129433 -0.70141228 -0.74138964 -0.83244319 -0.27608376\n",
      " -0.32758339 -0.37115232 -0.84653649 -0.17573729]\n",
      "\n",
      "     Weights  out:  [ 0.00972266 -0.32331137  0.21355346  0.26756238  0.43721346 -0.24051704\n",
      " -0.17619059 -0.12773167  0.47494742 -0.41313706] [0.24635453 0.10054912 0.41426589 0.0969248  0.57031567 0.52607464\n",
      " 0.51969024 0.55403581 0.04001717 0.37602745]\n",
      "\n",
      "# 39 Gradient out:  [-1.13248191 -0.49772058 -1.55182358 -1.64246829 -1.85441552 -0.62340296\n",
      " -0.73898504 -0.83501687 -1.8877984  -0.39113953]\n",
      "\n",
      "     Weights  out:  [ 0.00972266 -0.32331137  0.21355346  0.26756238  0.43721346 -0.24051704\n",
      " -0.17619059 -0.12773167  0.47494742 -0.41313706] [ 0.14452671  0.05629025  0.27398343 -0.05135312  0.40382704  0.47085789\n",
      "  0.45417356  0.47980535 -0.12929013  0.34088   ]\n",
      "\n",
      "# 40 Gradient out:  [1.82136857 0.83101318 2.51327662 2.63380096 2.88178848 0.98786285\n",
      " 1.15067038 1.30020642 2.91974129 0.70937558]\n",
      "\n",
      "     Weights  out:  [ 0.00972266 -0.32331137  0.21355346  0.26756238  0.43721346 -0.24051704\n",
      " -0.17619059 -0.12773167  0.47494742 -0.41313706] [-0.08196967 -0.04325386 -0.03638128 -0.37984678  0.03294393  0.3461773\n",
      "  0.30637655  0.31280198 -0.50684981  0.26265209]\n",
      "\n",
      "# 41 Gradient out:  [-0.3728187  -0.16293863 -0.51313506 -0.54221765 -0.6083319  -0.20276115\n",
      " -0.24025646 -0.27202415 -0.61855601 -0.12987142]\n",
      "\n",
      "     Weights  out:  [ 0.00972266 -0.32331137  0.21355346  0.26756238  0.43721346 -0.24051704\n",
      " -0.17619059 -0.12773167  0.47494742 -0.41313706] [0.28230405 0.12294878 0.46627404 0.14691341 0.60930163 0.54374987\n",
      " 0.53651062 0.57284326 0.07709845 0.4045272 ]\n",
      "\n",
      "# 42 Gradient out:  [-0.7712269  -0.33210604 -1.0637914  -1.12518782 -1.26586596 -0.41647948\n",
      " -0.49538    -0.56184342 -1.28771736 -0.2616391 ]\n",
      "\n",
      "     Weights  out:  [ 0.00972266 -0.32331137  0.21355346  0.26756238  0.43721346 -0.24051704\n",
      " -0.17619059 -0.12773167  0.47494742 -0.41313706] [ 0.20774031  0.09036105  0.36364703  0.03846988  0.48763525  0.50319764\n",
      "  0.48845933  0.51843843 -0.04661275  0.37855292]\n",
      "\n",
      "# 43 Gradient out:  [-0.77273726 -0.44229554 -0.96552326 -1.02624902 -1.19799724 -0.53430463\n",
      " -0.60539458 -0.65507032 -1.22818424 -0.35308345]\n",
      "\n",
      "     Weights  out:  [ 0.00972266 -0.32331137  0.21355346  0.26756238  0.43721346 -0.24051704\n",
      " -0.17619059 -0.12773167  0.47494742 -0.41313706] [ 0.05349493  0.02393984  0.15088875 -0.18656768  0.23446206  0.41990174\n",
      "  0.38938333  0.40606975 -0.30415622  0.3262251 ]\n",
      "\n",
      "# 44 Gradient out:  [2.0280999  1.00204011 2.73811126 2.86647396 3.14103563 1.17166995\n",
      " 1.34306537 1.49779378 3.18448993 0.86621981]\n",
      "\n",
      "     Weights  out:  [ 0.00972266 -0.32331137  0.21355346  0.26756238  0.43721346 -0.24051704\n",
      " -0.17619059 -0.12773167  0.47494742 -0.41313706] [-0.10105253 -0.06451927 -0.0422159  -0.39181749 -0.00513739  0.31304081\n",
      "  0.26830442  0.27505568 -0.54979307  0.25560841]\n",
      "\n",
      "# 45 Gradient out:  [-0.31591859 -0.13858788 -0.4345045  -0.45905938 -0.51485563 -0.17220257\n",
      " -0.20386797 -0.2307083  -0.52348393 -0.11068349]\n",
      "\n",
      "     Weights  out:  [ 0.00972266 -0.32331137  0.21355346  0.26756238  0.43721346 -0.24051704\n",
      " -0.17619059 -0.12773167  0.47494742 -0.41313706] [0.30456745 0.13588875 0.50540635 0.1814773  0.62306973 0.5473748\n",
      " 0.53691749 0.57461444 0.08710491 0.42885237]\n",
      "\n",
      "# 46 Gradient out:  [-0.60667838 -0.2612839  -0.83721797 -0.88528446 -0.99494073 -0.32721048\n",
      " -0.38908676 -0.44136723 -1.01192615 -0.20640238]\n",
      "\n",
      "     Weights  out:  [ 0.00972266 -0.32331137  0.21355346  0.26756238  0.43721346 -0.24051704\n",
      " -0.17619059 -0.12773167  0.47494742 -0.41313706] [ 0.24138374  0.10817118  0.41850545  0.08966543  0.52009861  0.51293429\n",
      "  0.4961439   0.52847278 -0.01759187  0.40671568]\n",
      "\n",
      "# 47 Gradient out:  [-1.23474887 -0.56565317 -1.67057033 -1.76939809 -2.00784456 -0.70459609\n",
      " -0.82906802 -0.93020852 -2.04621187 -0.44504018]\n",
      "\n",
      "     Weights  out:  [ 0.00972266 -0.32331137  0.21355346  0.26756238  0.43721346 -0.24051704\n",
      " -0.17619059 -0.12773167  0.47494742 -0.41313706] [ 0.12004806  0.0559144   0.25106186 -0.08739147  0.32111046  0.44749219\n",
      "  0.41832655  0.44019933 -0.2199771   0.3654352 ]\n",
      "\n",
      "# 48 Gradient out:  [2.04893818 1.1106359  2.69011238 2.81156701 3.08417789 1.27420457\n",
      " 1.43405577 1.57525728 3.12919653 0.97441329]\n",
      "\n",
      "     Weights  out:  [ 0.00972266 -0.32331137  0.21355346  0.26756238  0.43721346 -0.24051704\n",
      " -0.17619059 -0.12773167  0.47494742 -0.41313706] [-0.12690171 -0.05721624 -0.08305221 -0.44127108 -0.08045845  0.30657297\n",
      "  0.25251294  0.25415763 -0.62921947  0.27642716]\n",
      "\n",
      "# 49 Gradient out:  [-0.46654048 -0.19779648 -0.64616122 -0.68343526 -0.76814748 -0.24883971\n",
      " -0.29688568 -0.33757003 -0.78122799 -0.15543139]\n",
      "\n",
      "     Weights  out:  [ 0.00972266 -0.32331137  0.21355346  0.26756238  0.43721346 -0.24051704\n",
      " -0.17619059 -0.12773167  0.47494742 -0.41313706] [ 0.28288592  0.16491094  0.45497027  0.12104232  0.53637713  0.56141389\n",
      "  0.53932409  0.56920908 -0.00338017  0.47130982]\n",
      "\n",
      "Lineal error cuadrático medio (ECM): 1.3710626394177992\n",
      "\n",
      "# 0 Gradient out:  [0.53118363 0.51065896 0.58603241 0.81845318 1.0329249  0.97822289\n",
      " 0.72388291 0.60841068 1.0410778  1.04245064]\n",
      "\n",
      "     Weights  out:  [-0.3265561  -0.47790261 -0.18064072  0.03085034  0.37908949  0.19373024\n",
      " -0.04158434 -0.1500573   0.44461009  0.45543694] [ 0.40722463 -0.17086019 -0.12748887 -0.39625112 -0.37857663  0.34317062\n",
      " -0.15161863 -0.13448024  0.08178122  0.36859677]\n",
      "\n",
      "# 1 Gradient out:  [-0.50622628 -0.3477676  -0.74541351 -1.21139364 -1.83835962 -1.55965476\n",
      " -1.04396478 -0.80632534 -1.90430919 -1.9137896 ]\n",
      "\n",
      "     Weights  out:  [-0.3265561  -0.47790261 -0.18064072  0.03085034  0.37908949  0.19373024\n",
      " -0.04158434 -0.1500573   0.44461009  0.45543694] [ 0.51346136 -0.0687284  -0.01028239 -0.23256048 -0.17199165  0.53881519\n",
      " -0.00684205 -0.0127981   0.28999678  0.5770869 ]\n",
      "\n",
      "# 2 Gradient out:  [1.1897514  0.97106033 1.55080465 2.41329527 3.44920902 3.04029565\n",
      " 2.08943308 1.65343456 3.53970826 3.55296107]\n",
      "\n",
      "     Weights  out:  [-0.3265561  -0.47790261 -0.18064072  0.03085034  0.37908949  0.19373024\n",
      " -0.04158434 -0.1500573   0.44461009  0.45543694] [ 0.4122161  -0.13828192 -0.15936509 -0.47483921 -0.53966357  0.22688424\n",
      " -0.21563501 -0.17406317 -0.09086506  0.19432898]\n",
      "\n",
      "# 3 Gradient out:  [-0.08676992 -0.07015399 -0.1113224  -0.15943552 -0.2240087  -0.19533204\n",
      " -0.14209642 -0.11757781 -0.2309432  -0.23195262]\n",
      "\n",
      "     Weights  out:  [-0.3265561  -0.47790261 -0.18064072  0.03085034  0.37908949  0.19373024\n",
      " -0.04158434 -0.1500573   0.44461009  0.45543694] [0.65016638 0.05593015 0.15079584 0.00781984 0.15017823 0.83494337\n",
      " 0.20225161 0.15662374 0.6170766  0.90492119]\n",
      "\n",
      "# 4 Gradient out:  [-0.11925244 -0.09516812 -0.15494909 -0.22501111 -0.31894703 -0.27727605\n",
      " -0.19975662 -0.16405367 -0.32899505 -0.33045606]\n",
      "\n",
      "     Weights  out:  [-0.3265561  -0.47790261 -0.18064072  0.03085034  0.37908949  0.19373024\n",
      " -0.04158434 -0.1500573   0.44461009  0.45543694] [ 0.6328124   0.04189935  0.12853136 -0.02406726  0.10537649  0.79587697\n",
      "  0.17383233  0.13310818  0.57088795  0.85853067]\n",
      "\n",
      "# 5 Gradient out:  [-0.18354948 -0.14339196 -0.24334424 -0.36093879 -0.51839808 -0.4486489\n",
      " -0.31854139 -0.2586174  -0.53514327 -0.53757369]\n",
      "\n",
      "     Weights  out:  [-0.3265561  -0.47790261 -0.18064072  0.03085034  0.37908949  0.19373024\n",
      " -0.04158434 -0.1500573   0.44461009  0.45543694] [ 0.60896191  0.02286573  0.09754154 -0.06906948  0.04158708  0.74042176\n",
      "  0.133881    0.10029744  0.50508894  0.79243946]\n",
      "\n",
      "# 6 Gradient out:  [-0.33837718 -0.25276636 -0.46683568 -0.71974866 -1.05808769 -0.90840792\n",
      " -0.62858343 -0.49969221 -1.09375368 -1.09891159]\n",
      "\n",
      "     Weights  out:  [-0.3265561  -0.47790261 -0.18064072  0.03085034  0.37908949  0.19373024\n",
      " -0.04158434 -0.1500573   0.44461009  0.45543694] [ 0.57225201 -0.00581266  0.04887269 -0.14125724 -0.06209253  0.65069198\n",
      "  0.07017272  0.04857397  0.39806029  0.68492472]\n",
      "\n",
      "# 7 Gradient out:  [-0.40027571 -0.22809048 -0.6548946  -1.12073844 -1.77110546 -1.4722787\n",
      " -0.95604479 -0.71772846 -1.84284571 -1.85310199]\n",
      "\n",
      "     Weights  out:  [-0.3265561  -0.47790261 -0.18064072  0.03085034  0.37908949  0.19373024\n",
      " -0.04158434 -0.1500573   0.44461009  0.45543694] [ 0.50457658 -0.05636594 -0.04449445 -0.28520697 -0.27371007  0.46901039\n",
      " -0.05554396 -0.05136448  0.17930956  0.4651424 ]\n",
      "\n",
      "# 8 Gradient out:  [1.08966755 0.89172233 1.40263433 2.11858419 2.99935318 2.64158915\n",
      " 1.85173126 1.48930066 3.08161115 3.09373778]\n",
      "\n",
      "     Weights  out:  [-0.3265561  -0.47790261 -0.18064072  0.03085034  0.37908949  0.19373024\n",
      " -0.04158434 -0.1500573   0.44461009  0.45543694] [ 0.42452143 -0.10198403 -0.17547337 -0.50935466 -0.62793116  0.17455465\n",
      " -0.24675292 -0.19491017 -0.18925959  0.094522  ]\n",
      "\n",
      "# 9 Gradient out:  [-0.24082981 -0.1815491  -0.32962414 -0.50470299 -0.73873852 -0.63526419\n",
      " -0.44156292 -0.3523477  -0.76344135 -0.7670187 ]\n",
      "\n",
      "     Weights  out:  [-0.3265561  -0.47790261 -0.18064072  0.03085034  0.37908949  0.19373024\n",
      " -0.04158434 -0.1500573   0.44461009  0.45543694] [ 0.64245494  0.07636044  0.1050535  -0.08563782 -0.02806053  0.70287248\n",
      "  0.12359333  0.10294996  0.42706264  0.71326956]\n",
      "\n",
      "# 10 Gradient out:  [-0.49164191 -0.34455165 -0.71266257 -1.14367497 -1.72336817 -1.46570378\n",
      " -0.98872794 -0.76894964 -1.78462173 -1.7934517 ]\n",
      "\n",
      "     Weights  out:  [-0.3265561  -0.47790261 -0.18064072  0.03085034  0.37908949  0.19373024\n",
      " -0.04158434 -0.1500573   0.44461009  0.45543694] [ 0.59428898  0.04005062  0.03912867 -0.18657842 -0.17580823  0.57581965\n",
      "  0.03528075  0.03248042  0.27437437  0.55986582]\n",
      "\n",
      "# 11 Gradient out:  [1.06034224 0.86836297 1.3870737  2.19726277 3.15109229 2.78372407\n",
      " 1.89105583 1.48202477 3.23031893 3.24190621]\n",
      "\n",
      "     Weights  out:  [-0.3265561  -0.47790261 -0.18064072  0.03085034  0.37908949  0.19373024\n",
      " -0.04158434 -0.1500573   0.44461009  0.45543694] [ 0.4959606  -0.02885971 -0.10340384 -0.41531342 -0.52048186  0.28267889\n",
      " -0.16246484 -0.12130951 -0.08254997  0.20117548]\n",
      "\n",
      "# 12 Gradient out:  [-0.08986355 -0.0706469  -0.11842776 -0.17462546 -0.24986234 -0.21653653\n",
      " -0.15435979 -0.12572394 -0.25787719 -0.25904169]\n",
      "\n",
      "     Weights  out:  [-0.3265561  -0.47790261 -0.18064072  0.03085034  0.37908949  0.19373024\n",
      " -0.04158434 -0.1500573   0.44461009  0.45543694] [0.70802905 0.14481288 0.1740109  0.02413914 0.10973659 0.8394237\n",
      " 0.21574632 0.17509545 0.56351381 0.84955672]\n",
      "\n",
      "# 13 Gradient out:  [-0.12814087 -0.09923921 -0.17122927 -0.25611952 -0.36966796 -0.31942186\n",
      " -0.22550186 -0.18224626 -0.38171823 -0.38346707]\n",
      "\n",
      "     Weights  out:  [-0.3265561  -0.47790261 -0.18064072  0.03085034  0.37908949  0.19373024\n",
      " -0.04158434 -0.1500573   0.44461009  0.45543694] [ 0.69005634  0.1306835   0.15032535 -0.01078595  0.05976412  0.7961164\n",
      "  0.18487437  0.14995066  0.51193837  0.79774838]\n",
      "\n",
      "# 14 Gradient out:  [-0.21054559 -0.15903399 -0.28767104 -0.43980245 -0.64312064 -0.55324235\n",
      " -0.38493098 -0.30741134 -0.66458729 -0.66769702]\n",
      "\n",
      "     Weights  out:  [-0.3265561  -0.47790261 -0.18064072  0.03085034  0.37908949  0.19373024\n",
      " -0.04158434 -0.1500573   0.44461009  0.45543694] [ 0.66442816  0.11083566  0.11607949 -0.06200986 -0.01416947  0.73223202\n",
      "  0.13977399  0.11350141  0.43559473  0.72105497]\n",
      "\n",
      "# 15 Gradient out:  [-0.42726236 -0.30541411 -0.6104551  -0.96982777 -1.45153424 -1.23807968\n",
      " -0.84043384 -0.65724292 -1.5022799  -1.50960487]\n",
      "\n",
      "     Weights  out:  [-0.3265561  -0.47790261 -0.18064072  0.03085034  0.37908949  0.19373024\n",
      " -0.04158434 -0.1500573   0.44461009  0.45543694] [ 0.62231904  0.07902886  0.05854529 -0.14997035 -0.14279359  0.62158355\n",
      "  0.0627878   0.05201914  0.30267727  0.58751557]\n",
      "\n",
      "# 16 Gradient out:  [0.40697722 0.37548175 0.4928735  0.81293245 1.12315399 1.03566691\n",
      " 0.68490326 0.52522596 1.13547043 1.13728812]\n",
      "\n",
      "     Weights  out:  [-0.3265561  -0.47790261 -0.18064072  0.03085034  0.37908949  0.19373024\n",
      " -0.04158434 -0.1500573   0.44461009  0.45543694] [ 0.53686657  0.01794604 -0.06354573 -0.3439359  -0.43310044  0.37396762\n",
      " -0.10529897 -0.07942944  0.00222129  0.28559459]\n",
      "\n",
      "# 17 Gradient out:  [-0.52078794 -0.35902345 -0.76292343 -1.2315287  -1.86460987 -1.58203155\n",
      " -1.06337337 -0.8243424  -1.93199384 -1.94170781]\n",
      "\n",
      "     Weights  out:  [-0.3265561  -0.47790261 -0.18064072  0.03085034  0.37908949  0.19373024\n",
      " -0.04158434 -0.1500573   0.44461009  0.45543694] [ 0.61826202  0.09304239  0.03502897 -0.18134941 -0.20846965  0.581101\n",
      "  0.03168168  0.02561575  0.22931537  0.51305222]\n",
      "\n",
      "# 18 Gradient out:  [1.17523472 0.95489574 1.53382064 2.38079698 3.40450297 2.99724813\n",
      " 2.06332733 1.63503068 3.49582048 3.50923699]\n",
      "\n",
      "     Weights  out:  [-0.3265561  -0.47790261 -0.18064072  0.03085034  0.37908949  0.19373024\n",
      " -0.04158434 -0.1500573   0.44461009  0.45543694] [ 0.51410443  0.02123769 -0.11755572 -0.42765515 -0.58139162  0.26469469\n",
      " -0.18099299 -0.13925273 -0.15708339  0.12471065]\n",
      "\n",
      "# 19 Gradient out:  [-0.0830618  -0.0643802  -0.11090496 -0.16578842 -0.23917937 -0.20671107\n",
      " -0.14599039 -0.11802557 -0.24696886 -0.2480997 ]\n",
      "\n",
      "     Weights  out:  [-0.3265561  -0.47790261 -0.18064072  0.03085034  0.37908949  0.19373024\n",
      " -0.04158434 -0.1500573   0.44461009  0.45543694] [0.74915137 0.21221684 0.18920841 0.04850424 0.09950897 0.86414431\n",
      " 0.23167247 0.18775341 0.5420807  0.82655805]\n",
      "\n",
      "# 20 Gradient out:  [-0.11710991 -0.0894999  -0.15836691 -0.23978707 -0.34858019 -0.30049066\n",
      " -0.21041252 -0.16892692 -0.36008916 -0.36175832]\n",
      "\n",
      "     Weights  out:  [-0.3265561  -0.47790261 -0.18064072  0.03085034  0.37908949  0.19373024\n",
      " -0.04158434 -0.1500573   0.44461009  0.45543694] [0.73253901 0.1993408  0.16702742 0.01534656 0.0516731  0.8228021\n",
      " 0.20247439 0.16414829 0.49268693 0.77693811]\n",
      "\n",
      "# 21 Gradient out:  [-0.18899539 -0.14124335 -0.26059875 -0.40203028 -0.59089251 -0.50747493\n",
      " -0.35100527 -0.27894079 -0.61078938 -0.6136707 ]\n",
      "\n",
      "     Weights  out:  [-0.3265561  -0.47790261 -0.18064072  0.03085034  0.37908949  0.19373024\n",
      " -0.04158434 -0.1500573   0.44461009  0.45543694] [ 0.70911703  0.18144082  0.13535403 -0.03261086 -0.01804294  0.76270397\n",
      "  0.16039189  0.13036291  0.4206691   0.70458645]\n",
      "\n",
      "# 22 Gradient out:  [-0.38045154 -0.27206248 -0.54340575 -0.86374243 -1.29261956 -1.10277757\n",
      " -0.74834088 -0.58506645 -1.3377622  -1.34428251]\n",
      "\n",
      "     Weights  out:  [-0.3265561  -0.47790261 -0.18064072  0.03085034  0.37908949  0.19373024\n",
      " -0.04158434 -0.1500573   0.44461009  0.45543694] [ 0.67131795  0.15319215  0.08323428 -0.11301691 -0.13622144  0.66120898\n",
      "  0.09019084  0.07457475  0.29851122  0.58185231]\n",
      "\n",
      "# 23 Gradient out:  [-0.08630234 -0.00451996 -0.17747691 -0.24115285 -0.41703167 -0.30085377\n",
      " -0.22784126 -0.19278519 -0.45178377 -0.45682379]\n",
      "\n",
      "     Weights  out:  [-0.3265561  -0.47790261 -0.18064072  0.03085034  0.37908949  0.19373024\n",
      " -0.04158434 -0.1500573   0.44461009  0.45543694] [ 0.59522764  0.09877966 -0.02544687 -0.2857654  -0.39474535  0.44065347\n",
      " -0.05947734 -0.04243854  0.03095878  0.31299581]\n",
      "\n",
      "# 24 Gradient out:  [0.57181928 0.48283161 0.75098301 1.27450948 1.84123047 1.64702181\n",
      " 1.07156837 0.80861072 1.87732683 1.88254945]\n",
      "\n",
      "     Weights  out:  [-0.3265561  -0.47790261 -0.18064072  0.03085034  0.37908949  0.19373024\n",
      " -0.04158434 -0.1500573   0.44461009  0.45543694] [ 0.57796718  0.09787566 -0.06094225 -0.33399597 -0.47815169  0.38048272\n",
      " -0.10504559 -0.08099558 -0.05939797  0.22163105]\n",
      "\n",
      "# 25 Gradient out:  [-0.3230601  -0.23289262 -0.45865348 -0.72589648 -1.08316251 -0.92523176\n",
      " -0.62955694 -0.49336277 -1.1207173  -1.12614475]\n",
      "\n",
      "     Weights  out:  [-0.3265561  -0.47790261 -0.18064072  0.03085034  0.37908949  0.19373024\n",
      " -0.04158434 -0.1500573   0.44461009  0.45543694] [ 0.69233103  0.19444198  0.08925435 -0.07909407 -0.10990559  0.70988708\n",
      "  0.10926808  0.08072657  0.3160674   0.59814094]\n",
      "\n",
      "# 26 Gradient out:  [-0.47033833 -0.29707768 -0.71594147 -1.1431736  -1.75871302 -1.46786181\n",
      " -0.99391723 -0.77489758 -1.83119191 -1.84166326]\n",
      "\n",
      "     Weights  out:  [-0.3265561  -0.47790261 -0.18064072  0.03085034  0.37908949  0.19373024\n",
      " -0.04158434 -0.1500573   0.44461009  0.45543694] [ 0.62771901  0.14786346 -0.00247634 -0.22427337 -0.32653809  0.52484073\n",
      " -0.0166433  -0.01794599  0.09192393  0.37291199]\n",
      "\n",
      "# 27 Gradient out:  [1.10737628 0.91273723 1.40548185 2.07159332 2.90223036 2.55937199\n",
      " 1.82422714 1.48681052 2.98337797 2.99543775]\n",
      "\n",
      "     Weights  out:  [-0.3265561  -0.47790261 -0.18064072  0.03085034  0.37908949  0.19373024\n",
      " -0.04158434 -0.1500573   0.44461009  0.45543694] [ 0.53365135  0.08844792 -0.14566464 -0.45290809 -0.6782807   0.23126836\n",
      " -0.21542675 -0.1729255  -0.27431445  0.00457934]\n",
      "\n",
      "# 28 Gradient out:  [-0.2585553  -0.18541936 -0.36869211 -0.58636181 -0.87688738 -0.74865831\n",
      " -0.50784406 -0.39692684 -0.90734519 -0.91174733]\n",
      "\n",
      "     Weights  out:  [-0.3265561  -0.47790261 -0.18064072  0.03085034  0.37908949  0.19373024\n",
      " -0.04158434 -0.1500573   0.44461009  0.45543694] [ 0.7551266   0.27099537  0.13543174 -0.03858943 -0.09783463  0.74314276\n",
      "  0.14941868  0.1244366   0.32236115  0.60366689]\n",
      "\n",
      "# 29 Gradient out:  [-0.5542751  -0.37462956 -0.81878297 -1.31874175 -2.0038055  -1.69392688\n",
      " -1.14030391 -0.88502352 -2.078747   -2.08957834]\n",
      "\n",
      "     Weights  out:  [-0.3265561  -0.47790261 -0.18064072  0.03085034  0.37908949  0.19373024\n",
      " -0.04158434 -0.1500573   0.44461009  0.45543694] [ 0.70341554  0.2339115   0.06169331 -0.15586179 -0.2732121   0.5934111\n",
      "  0.04784986  0.04505123  0.14089211  0.42131742]\n",
      "\n",
      "# 30 Gradient out:  [1.15380412 0.94474705 1.47520171 2.19710185 3.09469981 2.7254005\n",
      " 1.92875648 1.56314904 3.18183106 3.19477786]\n",
      "\n",
      "     Weights  out:  [-0.3265561  -0.47790261 -0.18064072  0.03085034  0.37908949  0.19373024\n",
      " -0.04158434 -0.1500573   0.44461009  0.45543694] [ 0.59256052  0.15898559 -0.10206328 -0.41961014 -0.6739732   0.25462572\n",
      " -0.18021092 -0.13195347 -0.27485729  0.00340175]\n",
      "\n",
      "# 31 Gradient out:  [-0.16640128 -0.11950529 -0.23713902 -0.3774384  -0.56431199 -0.48199414\n",
      " -0.32678794 -0.25530738 -0.58383975 -0.58666257]\n",
      "\n",
      "     Weights  out:  [-0.3265561  -0.47790261 -0.18064072  0.03085034  0.37908949  0.19373024\n",
      " -0.04158434 -0.1500573   0.44461009  0.45543694] [ 0.82332135  0.347935    0.19297706  0.01981023 -0.05503324  0.79970582\n",
      "  0.20554038  0.18067633  0.36150892  0.64235733]\n",
      "\n",
      "# 32 Gradient out:  [-0.33928941 -0.23593918 -0.49486777 -0.80104848 -1.21069335 -1.02949471\n",
      " -0.69072746 -0.53467178 -1.25373141 -1.25994652]\n",
      "\n",
      "     Weights  out:  [-0.3265561  -0.47790261 -0.18064072  0.03085034  0.37908949  0.19373024\n",
      " -0.04158434 -0.1500573   0.44461009  0.45543694] [ 0.79004109  0.32403394  0.14554926 -0.05567745 -0.16789564  0.703307\n",
      "  0.14018279  0.12961486  0.24474097  0.52502481]\n",
      "\n",
      "# 33 Gradient out:  [-0.36321452 -0.23220236 -0.52368154 -0.72832258 -1.0898029  -0.89240813\n",
      " -0.66356691 -0.55687648 -1.14523496 -1.15338944]\n",
      "\n",
      "     Weights  out:  [-0.3265561  -0.47790261 -0.18064072  0.03085034  0.37908949  0.19373024\n",
      " -0.04158434 -0.1500573   0.44461009  0.45543694] [ 0.72218321  0.2768461   0.04657571 -0.21588714 -0.41003431  0.49740805\n",
      "  0.0020373   0.0226805  -0.00600531  0.27303551]\n",
      "\n",
      "# 34 Gradient out:  [1.08712    0.86993572 1.43717604 2.25840754 3.25481712 2.85653276\n",
      " 1.95090641 1.53555138 3.34491913 3.35819048]\n",
      "\n",
      "     Weights  out:  [-0.3265561  -0.47790261 -0.18064072  0.03085034  0.37908949  0.19373024\n",
      " -0.04158434 -0.1500573   0.44461009  0.45543694] [ 0.64954031  0.23040563 -0.0581606  -0.36155166 -0.62799489  0.31892643\n",
      " -0.13067608 -0.08869479 -0.2350523   0.04235762]\n",
      "\n",
      "# 35 Gradient out:  [-0.090059   -0.06576831 -0.12664585 -0.19926714 -0.29595934 -0.25337724\n",
      " -0.1730416  -0.13604474 -0.30607615 -0.30753993]\n",
      "\n",
      "     Weights  out:  [-0.3265561  -0.47790261 -0.18064072  0.03085034  0.37908949  0.19373024\n",
      " -0.04158434 -0.1500573   0.44461009  0.45543694] [0.8669643  0.40439277 0.2292746  0.09012985 0.02296854 0.89023298\n",
      " 0.2595052  0.21841548 0.43393152 0.71399572]\n",
      "\n",
      "# 36 Gradient out:  [-0.13783281 -0.09898279 -0.19645905 -0.31284753 -0.46778716 -0.39957227\n",
      " -0.27081995 -0.21152422 -0.48396406 -0.4863026 ]\n",
      "\n",
      "     Weights  out:  [-0.3265561  -0.47790261 -0.18064072  0.03085034  0.37908949  0.19373024\n",
      " -0.04158434 -0.1500573   0.44461009  0.45543694] [ 0.8489525   0.39123911  0.20394543  0.05027642 -0.03622333  0.83955753\n",
      "  0.22489688  0.19120653  0.37271629  0.65248773]\n",
      "\n",
      "# 37 Gradient out:  [-0.25804672 -0.18052183 -0.37505206 -0.60649739 -0.91523248 -0.77905701\n",
      " -0.52300534 -0.40506814 -0.9475098  -0.95217112]\n",
      "\n",
      "     Weights  out:  [-0.3265561  -0.47790261 -0.18064072  0.03085034  0.37908949  0.19373024\n",
      " -0.04158434 -0.1500573   0.44461009  0.45543694] [ 0.82138594  0.37144255  0.16465362 -0.01229309 -0.12978076  0.75964308\n",
      "  0.17073289  0.14890169  0.27592348  0.55522721]\n",
      "\n",
      "# 38 Gradient out:  [-0.56402285 -0.37862807 -0.83271364 -1.32995756 -2.02010888 -1.70420113\n",
      " -1.15334853 -0.8992301  -2.09755939 -2.10878677]\n",
      "\n",
      "     Weights  out:  [-0.3265561  -0.47790261 -0.18064072  0.03085034  0.37908949  0.19373024\n",
      " -0.04158434 -0.1500573   0.44461009  0.45543694] [ 0.7697766   0.33533819  0.08964321 -0.13359257 -0.31282726  0.60383168\n",
      "  0.06613182  0.06788806  0.08642152  0.36479299]\n",
      "\n",
      "# 39 Gradient out:  [1.12671942 0.9259646  1.4272434  2.08785403 2.91956165 2.57241134\n",
      " 1.84315015 1.5083772  3.00345217 3.01599425]\n",
      "\n",
      "     Weights  out:  [-0.3265561  -0.47790261 -0.18064072  0.03085034  0.37908949  0.19373024\n",
      " -0.04158434 -0.1500573   0.44461009  0.45543694] [ 0.65697203  0.25961257 -0.07689952 -0.39958408 -0.71684904  0.26299145\n",
      " -0.16453788 -0.11195796 -0.33309036 -0.05696437]\n",
      "\n",
      "# 40 Gradient out:  [-0.22204767 -0.1531844  -0.32625572 -0.53292003 -0.80817059 -0.68695575\n",
      " -0.45833054 -0.35302986 -0.83683401 -0.84097096]\n",
      "\n",
      "     Weights  out:  [-0.3265561  -0.47790261 -0.18064072  0.03085034  0.37908949  0.19373024\n",
      " -0.04158434 -0.1500573   0.44461009  0.45543694] [ 0.88231591  0.44480549  0.20854916  0.01798673 -0.13293671  0.77747372\n",
      "  0.20409215  0.18971748  0.26760008  0.54623448]\n",
      "\n",
      "# 41 Gradient out:  [-0.51397554 -0.34449587 -0.76317791 -1.23604846 -1.8825774  -1.59066815\n",
      " -1.06708807 -0.82569417 -1.9532954  -1.96353555]\n",
      "\n",
      "     Weights  out:  [-0.3265561  -0.47790261 -0.18064072  0.03085034  0.37908949  0.19373024\n",
      " -0.04158434 -0.1500573   0.44461009  0.45543694] [ 0.83790638  0.41416861  0.14329802 -0.08859728 -0.29457083  0.64008257\n",
      "  0.11242604  0.11911151  0.10023328  0.37804029]\n",
      "\n",
      "# 42 Gradient out:  [1.03289447 0.81778834 1.37351109 2.16066836 3.1238323  2.73490152\n",
      " 1.86664565 1.46834534 3.2132313  3.2264474 ]\n",
      "\n",
      "     Weights  out:  [-0.3265561  -0.47790261 -0.18064072  0.03085034  0.37908949  0.19373024\n",
      " -0.04158434 -0.1500573   0.44461009  0.45543694] [ 0.73511127  0.34526944 -0.00933756 -0.33580697 -0.67108631  0.32194894\n",
      " -0.10099158 -0.04602732 -0.2904258  -0.01466682]\n",
      "\n",
      "# 43 Gradient out:  [-0.10969302 -0.07669478 -0.15973479 -0.25946206 -0.39191184 -0.33374056\n",
      " -0.22342675 -0.17262469 -0.40564494 -0.40762743]\n",
      "\n",
      "     Weights  out:  [-0.3265561  -0.47790261 -0.18064072  0.03085034  0.37908949  0.19373024\n",
      " -0.04158434 -0.1500573   0.44461009  0.45543694] [ 0.94169016  0.50882711  0.26536466  0.0963267  -0.04631985  0.86892925\n",
      "  0.27233755  0.24764175  0.35222046  0.63062266]\n",
      "\n",
      "# 44 Gradient out:  [-0.19044039 -0.13056337 -0.28122778 -0.46167648 -0.70169359 -0.59613474\n",
      " -0.39651923 -0.30458351 -0.72661187 -0.73020692]\n",
      "\n",
      "     Weights  out:  [-0.3265561  -0.47790261 -0.18064072  0.03085034  0.37908949  0.19373024\n",
      " -0.04158434 -0.1500573   0.44461009  0.45543694] [ 0.91975156  0.49348815  0.2334177   0.04443429 -0.12470221  0.80218113\n",
      "  0.2276522   0.21311681  0.27109147  0.54909718]\n",
      "\n",
      "# 45 Gradient out:  [-0.43324438 -0.28957354 -0.64727567 -1.06134072 -1.62095598 -1.37102912\n",
      " -0.91273839 -0.70153607 -1.6808364  -1.68949247]\n",
      "\n",
      "     Weights  out:  [-0.3265561  -0.47790261 -0.18064072  0.03085034  0.37908949  0.19373024\n",
      " -0.04158434 -0.1500573   0.44461009  0.45543694] [ 0.88166348  0.46737547  0.17717214 -0.047901   -0.26504093  0.68295419\n",
      "  0.14834836  0.15220011  0.12576909  0.40305579]\n",
      "\n",
      "# 46 Gradient out:  [0.4915564  0.36494731 0.73738129 1.4074919  2.15769976 1.88805912\n",
      " 1.15072437 0.8132893  2.20917331 2.21649386]\n",
      "\n",
      "     Weights  out:  [-0.3265561  -0.47790261 -0.18064072  0.03085034  0.37908949  0.19373024\n",
      " -0.04158434 -0.1500573   0.44461009  0.45543694] [ 0.79501461  0.40946077  0.04771701 -0.26016915 -0.58923213  0.40874836\n",
      " -0.03419932  0.01189289 -0.21039819  0.0651573 ]\n",
      "\n",
      "# 47 Gradient out:  [-0.24949306 -0.17031134 -0.36926739 -0.60643821 -0.92259763 -0.78325337\n",
      " -0.52087154 -0.40001754 -0.95555632 -0.96031233]\n",
      "\n",
      "     Weights  out:  [-0.3265561  -0.47790261 -0.18064072  0.03085034  0.37908949  0.19373024\n",
      " -0.04158434 -0.1500573   0.44461009  0.45543694] [ 0.89332589  0.48245023  0.19519327  0.02132923 -0.15769218  0.78636018\n",
      "  0.19594555  0.17455075  0.23143647  0.50845607]\n",
      "\n",
      "# 48 Gradient out:  [-0.56190797 -0.3752185  -0.82922491 -1.31672403 -1.99949051 -1.68437275\n",
      " -1.14415459 -0.89486782 -2.07756918 -2.08891864]\n",
      "\n",
      "     Weights  out:  [-0.3265561  -0.47790261 -0.18064072  0.03085034  0.37908949  0.19373024\n",
      " -0.04158434 -0.1500573   0.44461009  0.45543694] [ 0.84342728  0.44838796  0.12133979 -0.09995841 -0.3422117   0.62970951\n",
      "  0.09177124  0.09454724  0.04032521  0.31639361]\n",
      "\n",
      "# 49 Gradient out:  [1.105338   0.90342004 1.40375222 2.05333728 2.87587316 2.53029354\n",
      " 1.81309145 1.48381882 2.96035618 2.97302681]\n",
      "\n",
      "     Weights  out:  [-0.3265561  -0.47790261 -0.18064072  0.03085034  0.37908949  0.19373024\n",
      " -0.04158434 -0.1500573   0.44461009  0.45543694] [ 0.73104568  0.37334426 -0.04450519 -0.36330322 -0.74210981  0.29283496\n",
      " -0.13705967 -0.08442632 -0.37518862 -0.10139012]\n",
      "\n",
      "Tanh error cuadrático medio (ECM): 1.039175360963628\n",
      "\n",
      "# 0 Gradient out:  [0.39978287 0.34305841 0.43222188 0.27677154 0.36097717 0.330232\n",
      " 0.3627716  0.33728213 0.33714175 0.3953313 ]\n",
      "\n",
      "     Weights  out:  [-0.27262965  0.23003023 -0.37886635  0.4959198  -0.05497603  0.27040312\n",
      " -0.12083829  0.24859314  0.24903432 -0.2589605 ] [-0.37440463  0.44705809 -0.26706551 -0.08857553  0.31072695 -0.21827884\n",
      " -0.03957596 -0.39149078  0.4648707  -0.02946665]\n",
      "\n",
      "# 1 Gradient out:  [-0.32332897 -1.33616448 -0.17856157 -1.65870237 -0.74534808 -1.40382922\n",
      " -0.60526465 -1.36807021 -1.36881232 -0.34539462]\n",
      "\n",
      "     Weights  out:  [-0.27262965  0.23003023 -0.37886635  0.4959198  -0.05497603  0.27040312\n",
      " -0.12083829  0.24859314  0.24903432 -0.2589605 ] [-0.29444806  0.51566977 -0.18062113 -0.03322122  0.38292238 -0.15223244\n",
      "  0.03297835 -0.32403436  0.53229905  0.04959961]\n",
      "\n",
      "# 2 Gradient out:  [1.24218431 2.97633064 1.0498695  3.41511714 1.93144603 3.07213899\n",
      " 1.67826881 3.02213737 3.02318895 1.27280036]\n",
      "\n",
      "     Weights  out:  [-0.27262965  0.23003023 -0.37886635  0.4959198  -0.05497603  0.27040312\n",
      " -0.12083829  0.24859314  0.24903432 -0.2589605 ] [-0.35911385  0.24843687 -0.21633345 -0.36496169  0.23385276 -0.43299828\n",
      " -0.08807457 -0.5976484   0.25853659 -0.01947931]\n",
      "\n",
      "# 3 Gradient out:  [-0.15560109 -0.33730341 -0.13062636 -0.39398721 -0.23038442 -0.34893655\n",
      " -0.20489385 -0.3427976  -0.34292518 -0.1593867 ]\n",
      "\n",
      "     Weights  out:  [-0.27262965  0.23003023 -0.37886635  0.4959198  -0.05497603  0.27040312\n",
      " -0.12083829  0.24859314  0.24903432 -0.2589605 ] [-0.11067699  0.843703   -0.00635955  0.31806173  0.62014197  0.18142951\n",
      "  0.24757919  0.00677907  0.86317438  0.23508076]\n",
      "\n",
      "# 4 Gradient out:  [-0.25651908 -0.58947513 -0.21103427 -0.69253713 -0.39347736 -0.61073153\n",
      " -0.34673397 -0.59951818 -0.59975129 -0.26343217]\n",
      "\n",
      "     Weights  out:  [-0.27262965  0.23003023 -0.37886635  0.4959198  -0.05497603  0.27040312\n",
      " -0.12083829  0.24859314  0.24903432 -0.2589605 ] [-0.14179721  0.77624232 -0.03248482  0.23926429  0.57406508  0.1116422\n",
      "  0.20660042 -0.06178045  0.79458934  0.20320342]\n",
      "\n",
      "# 5 Gradient out:  [-0.50529074 -1.32242122 -0.39448854 -1.57245405 -0.84138234 -1.37449795\n",
      " -0.72663924 -1.34703894 -1.3476101  -0.52221327]\n",
      "\n",
      "     Weights  out:  [-0.27262965  0.23003023 -0.37886635  0.4959198  -0.05497603  0.27040312\n",
      " -0.12083829  0.24859314  0.24903432 -0.2589605 ] [-0.19310102  0.6583473  -0.07469167  0.10075687  0.49536961 -0.0105041\n",
      "  0.13725362 -0.18168408  0.67463908  0.15051699]\n",
      "\n",
      "# 6 Gradient out:  [0.73733656 1.60189318 0.66812255 1.76253229 1.06630129 1.64063225\n",
      " 0.9341695  1.62078587 1.62121164 0.74934312]\n",
      "\n",
      "     Weights  out:  [-0.27262965  0.23003023 -0.37886635  0.4959198  -0.05497603  0.27040312\n",
      " -0.12083829  0.24859314  0.24903432 -0.2589605 ] [-0.29415917  0.39386305 -0.15358938 -0.21373394  0.32709315 -0.28540369\n",
      " -0.00807423 -0.45109187  0.40511706  0.04607433]\n",
      "\n",
      "# 7 Gradient out:  [-0.38223796 -0.94588078 -0.30567527 -1.11900879 -0.61400173 -0.98178518\n",
      " -0.53483212 -0.96285095 -0.96324473 -0.39390815]\n",
      "\n",
      "     Weights  out:  [-0.27262965  0.23003023 -0.37886635  0.4959198  -0.05497603  0.27040312\n",
      " -0.12083829  0.24859314  0.24903432 -0.2589605 ] [-0.14669186  0.71424169 -0.01996487  0.13877251  0.5403534   0.04272276\n",
      "  0.17875967 -0.1269347   0.72935939  0.19594296]\n",
      "\n",
      "# 8 Gradient out:  [-0.45691326 -1.51511773 -0.30379225 -1.8581165  -0.89815267 -1.58612655\n",
      " -0.75194639 -1.5485734  -1.54935209 -0.48009823]\n",
      "\n",
      "     Weights  out:  [-0.27262965  0.23003023 -0.37886635  0.4959198  -0.05497603  0.27040312\n",
      " -0.12083829  0.24859314  0.24903432 -0.2589605 ] [-0.22313945  0.52506553 -0.08109992 -0.08502925  0.41755306 -0.15363428\n",
      "  0.07179325 -0.31950489  0.53671044  0.11716133]\n",
      "\n",
      "# 9 Gradient out:  [1.17757915 2.70756376 1.00197106 3.1096156  1.78822702 2.79377333\n",
      " 1.56590266 2.74869559 2.74964162 1.2052157 ]\n",
      "\n",
      "     Weights  out:  [-0.27262965  0.23003023 -0.37886635  0.4959198  -0.05497603  0.27040312\n",
      " -0.12083829  0.24859314  0.24903432 -0.2589605 ] [-0.3145221   0.22204198 -0.14185837 -0.45665254  0.23792252 -0.47085959\n",
      " -0.07859603 -0.62921957  0.22684003  0.02114168]\n",
      "\n",
      "# 10 Gradient out:  [-0.26300976 -0.63469408 -0.21246347 -0.74912711 -0.4158233  -0.65836742\n",
      " -0.3636104  -0.64588221 -0.64614184 -0.2707056 ]\n",
      "\n",
      "     Weights  out:  [-0.27262965  0.23003023 -0.37886635  0.4959198  -0.05497603  0.27040312\n",
      " -0.12083829  0.24859314  0.24903432 -0.2589605 ] [-0.07900627  0.76355474  0.05853584  0.16527058  0.59556793  0.08789508\n",
      "  0.23458451 -0.07948045  0.77676835  0.26218482]\n",
      "\n",
      "# 11 Gradient out:  [-0.5464655  -1.47245295 -0.42037187 -1.75693276 -0.92764838 -1.53165653\n",
      " -0.79774549 -1.50043358 -1.5010829  -0.56571   ]\n",
      "\n",
      "     Weights  out:  [-0.27262965  0.23003023 -0.37886635  0.4959198  -0.05497603  0.27040312\n",
      " -0.12083829  0.24859314  0.24903432 -0.2589605 ] [-0.13160823  0.63661592  0.01604314  0.01544515  0.51240327 -0.04377841\n",
      "  0.16186242 -0.20865689  0.64753998  0.2080437 ]\n",
      "\n",
      "# 12 Gradient out:  [1.08389305 2.68500318 0.92401645 3.04977242 1.7112381  2.76778139\n",
      " 1.47381548 2.72482934 2.72573834 1.1100933 ]\n",
      "\n",
      "     Weights  out:  [-0.27262965  0.23003023 -0.37886635  0.4959198  -0.05497603  0.27040312\n",
      " -0.12083829  0.24859314  0.24903432 -0.2589605 ] [-0.24090133  0.34212533 -0.06803123 -0.3359414   0.32687359 -0.35010971\n",
      "  0.00231333 -0.50874361  0.3473234   0.0949017 ]\n",
      "\n",
      "# 13 Gradient out:  [-0.11194211 -0.2505368  -0.09295779 -0.2936076  -0.168958   -0.25939239\n",
      " -0.14950411 -0.25472006 -0.25481717 -0.11482297]\n",
      "\n",
      "     Weights  out:  [-0.27262965  0.23003023 -0.37886635  0.4959198  -0.05497603  0.27040312\n",
      " -0.12083829  0.24859314  0.24903432 -0.2589605 ] [-0.02412272  0.87912597  0.11677206  0.27401309  0.66912121  0.20344657\n",
      "  0.29707642  0.03622226  0.89247107  0.31692036]\n",
      "\n",
      "# 14 Gradient out:  [-0.16712823 -0.38747106 -0.13705753 -0.45563077 -0.25774203 -0.40152516\n",
      " -0.22679947 -0.39411157 -0.39426569 -0.17169863]\n",
      "\n",
      "     Weights  out:  [-0.27262965  0.23003023 -0.37886635  0.4959198  -0.05497603  0.27040312\n",
      " -0.12083829  0.24859314  0.24903432 -0.2589605 ] [-0.04651114  0.82901861  0.0981805   0.21529157  0.63532961  0.15156809\n",
      "  0.2671756  -0.01472175  0.84150764  0.29395577]\n",
      "\n",
      "# 15 Gradient out:  [-0.29668655 -0.72958314 -0.23788823 -0.86263571 -0.47465447 -0.75714282\n",
      " -0.41383666 -0.74260905 -0.7429113  -0.30564463]\n",
      "\n",
      "     Weights  out:  [-0.27262965  0.23003023 -0.37886635  0.4959198  -0.05497603  0.27040312\n",
      " -0.12083829  0.24859314  0.24903432 -0.2589605 ] [-0.07993678  0.75152439  0.070769    0.12416541  0.58378121  0.07126306\n",
      "  0.2218157  -0.09354406  0.7626545   0.25961604]\n",
      "\n",
      "# 16 Gradient out:  [-0.60657822 -1.69234689 -0.45731886 -2.02872336 -1.0544146  -1.76228901\n",
      " -0.902447   -1.72538651 -1.7261536  -0.62932937]\n",
      "\n",
      "     Weights  out:  [-0.27262965  0.23003023 -0.37886635  0.4959198  -0.05497603  0.27040312\n",
      " -0.12083829  0.24859314  0.24903432 -0.2589605 ] [-0.13927409  0.60560777  0.02319135 -0.04836173  0.48885031 -0.08016551\n",
      "  0.13904837 -0.24206587  0.61407224  0.19848712]\n",
      "\n",
      "# 17 Gradient out:  [1.29286825 3.04719106 1.0962176  3.49700571 1.99081678 3.14460491\n",
      " 1.73497915 3.09373424 3.09480336 1.32402985]\n",
      "\n",
      "     Weights  out:  [-0.27262965  0.23003023 -0.37886635  0.4959198  -0.05497603  0.27040312\n",
      " -0.12083829  0.24859314  0.24903432 -0.2589605 ] [-0.26058974  0.26713839 -0.06827242 -0.4541064   0.27796739 -0.43262331\n",
      " -0.04144103 -0.58714318  0.26884152  0.07262124]\n",
      "\n",
      "# 18 Gradient out:  [-0.11209948 -0.25603232 -0.09242864 -0.30064277 -0.1712959  -0.26521783\n",
      " -0.15108611 -0.26037203 -0.26047276 -0.11508704]\n",
      "\n",
      "     Weights  out:  [-0.27262965  0.23003023 -0.37886635  0.4959198  -0.05497603  0.27040312\n",
      " -0.12083829  0.24859314  0.24903432 -0.2589605 ] [-0.00201609  0.8765766   0.1509711   0.24529474  0.67613075  0.19629767\n",
      "  0.3055548   0.03160367  0.88780219  0.33742721]\n",
      "\n",
      "# 19 Gradient out:  [-0.1691055  -0.3998902  -0.13767513 -0.47110474 -0.26399122 -0.41459419\n",
      " -0.23157281 -0.40683867 -0.40699993 -0.17388632]\n",
      "\n",
      "     Weights  out:  [-0.27262965  0.23003023 -0.37886635  0.4959198  -0.05497603  0.27040312\n",
      " -0.12083829  0.24859314  0.24903432 -0.2589605 ] [-0.02443598  0.82537014  0.13248537  0.18516619  0.64187157  0.14325411\n",
      "  0.27533758 -0.02047073  0.83570764  0.3144098 ]\n",
      "\n",
      "# 20 Gradient out:  [-0.30611859 -0.76663385 -0.24365023 -0.90794548 -0.49541903 -0.79593423\n",
      " -0.43071129 -0.78048365 -0.780805   -0.31564126]\n",
      "\n",
      "     Weights  out:  [-0.27262965  0.23003023 -0.37886635  0.4959198  -0.05497603  0.27040312\n",
      " -0.12083829  0.24859314  0.24903432 -0.2589605 ] [-0.05825708  0.7453921   0.10495034  0.09094524  0.58907332  0.06033527\n",
      "  0.22902302 -0.10183847  0.75430765  0.27963254]\n",
      "\n",
      "# 21 Gradient out:  [-0.62374869 -1.75224541 -0.46709382 -2.10531243 -1.09000336 -1.82543528\n",
      " -0.93237672 -1.78680077 -1.78760344 -0.64757495]\n",
      "\n",
      "     Weights  out:  [-0.27262965  0.23003023 -0.37886635  0.4959198  -0.05497603  0.27040312\n",
      " -0.12083829  0.24859314  0.24903432 -0.2589605 ] [-0.1194808   0.59206533  0.0562203  -0.09064386  0.48998952 -0.09885158\n",
      "  0.14288076 -0.2579352   0.59814665  0.21650429]\n",
      "\n",
      "# 22 Gradient out:  [1.25015688 2.85056022 1.06532564 3.27443955 1.88923484 2.94098731\n",
      " 1.65681882 2.89368744 2.89467971 1.27916312]\n",
      "\n",
      "     Weights  out:  [-0.27262965  0.23003023 -0.37886635  0.4959198  -0.05497603  0.27040312\n",
      " -0.12083829  0.24859314  0.24903432 -0.2589605 ] [-0.24423054  0.24161625 -0.03719847 -0.51170634  0.27198885 -0.46393863\n",
      " -0.04359458 -0.61529535  0.24062596  0.0869893 ]\n",
      "\n",
      "# 23 Gradient out:  [-0.17922261 -0.43704566 -0.14422186 -0.51630025 -0.28518825 -0.45344529\n",
      " -0.24895592 -0.44479695 -0.44497681 -0.18455313]\n",
      "\n",
      "     Weights  out:  [-0.27262965  0.23003023 -0.37886635  0.4959198  -0.05497603  0.27040312\n",
      " -0.12083829  0.24859314  0.24903432 -0.2589605 ] [ 0.00580084  0.81172829  0.17586666  0.14318157  0.64983582  0.12425883\n",
      "  0.28776918 -0.03655786  0.8195619   0.34282192]\n",
      "\n",
      "# 24 Gradient out:  [-0.34038029 -0.87872304 -0.26745906 -1.04358162 -0.5616618  -0.91296006\n",
      " -0.48601204 -0.89490793 -0.89528343 -0.35150563]\n",
      "\n",
      "     Weights  out:  [-0.27262965  0.23003023 -0.37886635  0.4959198  -0.05497603  0.27040312\n",
      " -0.12083829  0.24859314  0.24903432 -0.2589605 ] [-0.03004369  0.72431916  0.14702229  0.03992152  0.59279817  0.03356977\n",
      "  0.237978   -0.12551725  0.73056654  0.3059113 ]\n",
      "\n",
      "# 25 Gradient out:  [-0.61606072 -1.74149736 -0.45247303 -2.11028742 -1.08487097 -1.8168765\n",
      " -0.92921855 -1.77699732 -1.77782384 -0.64068967]\n",
      "\n",
      "     Weights  out:  [-0.27262965  0.23003023 -0.37886635  0.4959198  -0.05497603  0.27040312\n",
      " -0.12083829  0.24859314  0.24903432 -0.2589605 ] [-0.09811974  0.54857455  0.09353048 -0.16879481  0.4804658  -0.14902224\n",
      "  0.14077559 -0.30449884  0.55150986  0.23561017]\n",
      "\n",
      "# 26 Gradient out:  [1.13656709 2.44105416 0.97913822 2.80379679 1.66028759 2.51664318\n",
      " 1.47200959 2.47700877 2.47783799 1.1609123 ]\n",
      "\n",
      "     Weights  out:  [-0.27262965  0.23003023 -0.37886635  0.4959198  -0.05497603  0.27040312\n",
      " -0.12083829  0.24859314  0.24903432 -0.2589605 ] [-0.22133189  0.20027508  0.00303587 -0.59085229  0.26349161 -0.51239754\n",
      " -0.04506812 -0.6598983   0.19594509  0.10747224]\n",
      "\n",
      "# 27 Gradient out:  [-0.40499629 -1.08513875 -0.31278811 -1.2934578  -0.68466939 -1.12844742\n",
      " -0.58913335 -1.10561163 -1.10608663 -0.41906978]\n",
      "\n",
      "     Weights  out:  [-0.27262965  0.23003023 -0.37886635  0.4959198  -0.05497603  0.27040312\n",
      " -0.12083829  0.24859314  0.24903432 -0.2589605 ] [ 0.00598153  0.68848591  0.19886351 -0.03009293  0.59554913 -0.0090689\n",
      "  0.24933379 -0.16449655  0.69151268  0.3396547 ]\n",
      "\n",
      "# 28 Gradient out:  [-0.31630747 -0.77294549 -0.21765359 -0.9963765  -0.5229897  -0.81388083\n",
      " -0.4665166  -0.79184321 -0.79229137 -0.33006812]\n",
      "\n",
      "     Weights  out:  [-0.27262965  0.23003023 -0.37886635  0.4959198  -0.05497603  0.27040312\n",
      " -0.12083829  0.24859314  0.24903432 -0.2589605 ] [-0.07501773  0.47145816  0.13630589 -0.28878449  0.45861525 -0.23475839\n",
      "  0.13150713 -0.38561888  0.47029536  0.25584074]\n",
      "\n",
      "# 29 Gradient out:  [1.14583046 2.90309264 0.95976467 3.327737   1.83976069 2.99733776\n",
      " 1.58137363 2.94827638 2.94931102 1.17582128]\n",
      "\n",
      "     Weights  out:  [-0.27262965  0.23003023 -0.37886635  0.4959198  -0.05497603  0.27040312\n",
      " -0.12083829  0.24859314  0.24903432 -0.2589605 ] [-0.13827922  0.31686906  0.09277517 -0.48805979  0.35401731 -0.39753455\n",
      "  0.0382038  -0.54398752  0.31183709  0.18982712]\n",
      "\n",
      "# 30 Gradient out:  [-0.08822798 -0.21241571 -0.07136002 -0.25063256 -0.13926579 -0.2203143\n",
      " -0.12181235 -0.21614882 -0.21623544 -0.09079552]\n",
      "\n",
      "     Weights  out:  [-0.27262965  0.23003023 -0.37886635  0.4959198  -0.05497603  0.27040312\n",
      " -0.12083829  0.24859314  0.24903432 -0.2589605 ] [0.09088687 0.89748759 0.28472811 0.17748761 0.72196945 0.201933\n",
      " 0.35447853 0.04566776 0.90169929 0.42499137]\n",
      "\n",
      "# 31 Gradient out:  [-0.12566973 -0.31015687 -0.10067187 -0.36675856 -0.2014719  -0.32187729\n",
      " -0.17553643 -0.31569714 -0.31582568 -0.12947871]\n",
      "\n",
      "     Weights  out:  [-0.27262965  0.23003023 -0.37886635  0.4959198  -0.05497603  0.27040312\n",
      " -0.12083829  0.24859314  0.24903432 -0.2589605 ] [0.07324127 0.85500445 0.2704561  0.1273611  0.69411629 0.15787014\n",
      " 0.33011606 0.00243799 0.8584522  0.40683227]\n",
      "\n",
      "# 32 Gradient out:  [-0.20653985 -0.52827013 -0.16305218 -0.62664748 -0.33871426 -0.54869121\n",
      " -0.29347596 -0.53792479 -0.53814876 -0.21317474]\n",
      "\n",
      "     Weights  out:  [-0.27262965  0.23003023 -0.37886635  0.4959198  -0.05497603  0.27040312\n",
      " -0.12083829  0.24859314  0.24903432 -0.2589605 ] [ 0.04810733  0.79297307  0.25032173  0.05400939  0.65382191  0.09349468\n",
      "  0.29500878 -0.06070143  0.79528707  0.38093653]\n",
      "\n",
      "# 33 Gradient out:  [-0.4322796  -1.16752891 -0.33245877 -1.39301515 -0.73470003 -1.21439862\n",
      " -0.63145824 -1.18968354 -1.19019759 -0.44751189]\n",
      "\n",
      "     Weights  out:  [-0.27262965  0.23003023 -0.37886635  0.4959198  -0.05497603  0.27040312\n",
      " -0.12083829  0.24859314  0.24903432 -0.2589605 ] [ 0.00679936  0.68731905  0.21771129 -0.07132011  0.58607906 -0.01624356\n",
      "  0.23631358 -0.16828639  0.68765731  0.33830158]\n",
      "\n",
      "# 34 Gradient out:  [-0.08422505 -0.07740127 -0.0377493  -0.18323936 -0.10575    -0.09223558\n",
      " -0.11657686 -0.08382851 -0.08399015 -0.08963763]\n",
      "\n",
      "     Weights  out:  [-0.27262965  0.23003023 -0.37886635  0.4959198  -0.05497603  0.27040312\n",
      " -0.12083829  0.24859314  0.24903432 -0.2589605 ] [-0.07965656  0.45381327  0.15121954 -0.34992314  0.43913905 -0.25912329\n",
      "  0.11002194 -0.4062231   0.4496178   0.2487992 ]\n",
      "\n",
      "# 35 Gradient out:  [0.15742087 0.57969009 0.15404767 0.58683555 0.3032386  0.58914923\n",
      " 0.23264726 0.5848258  0.58493073 0.1598232 ]\n",
      "\n",
      "     Weights  out:  [-0.27262965  0.23003023 -0.37886635  0.4959198  -0.05497603  0.27040312\n",
      " -0.12083829  0.24859314  0.24903432 -0.2589605 ] [-0.09650157  0.43833301  0.14366968 -0.38657101  0.41798905 -0.2775704\n",
      "  0.08670656 -0.4229888   0.43281976  0.23087168]\n",
      "\n",
      "# 36 Gradient out:  [-0.60369734 -1.66101385 -0.44395115 -2.02171414 -1.04709019 -1.73371355\n",
      " -0.90206431 -1.6951795  -1.6959765  -0.62752449]\n",
      "\n",
      "     Weights  out:  [-0.27262965  0.23003023 -0.37886635  0.4959198  -0.05497603  0.27040312\n",
      " -0.12083829  0.24859314  0.24903432 -0.2589605 ] [-0.0650174   0.55427103  0.17447921 -0.2692039   0.47863677 -0.15974056\n",
      "  0.13323602 -0.30602364  0.54980591  0.26283631]\n",
      "\n",
      "# 37 Gradient out:  [1.1548496  2.45101654 0.99671546 2.81596277 1.67587851 2.52657134\n",
      " 1.48907295 2.48693096 2.48775975 1.1792085 ]\n",
      "\n",
      "     Weights  out:  [-0.27262965  0.23003023 -0.37886635  0.4959198  -0.05497603  0.27040312\n",
      " -0.12083829  0.24859314  0.24903432 -0.2589605 ] [-0.18575686  0.22206826  0.08568898 -0.67354673  0.26921873 -0.50648327\n",
      " -0.04717685 -0.64505954  0.21061061  0.13733142]\n",
      "\n",
      "# 38 Gradient out:  [-0.38983326 -1.06083854 -0.29892735 -1.26620114 -0.66572678 -1.10354897\n",
      " -0.57146444 -1.08102952 -1.08149795 -0.40371142]\n",
      "\n",
      "     Weights  out:  [-0.27262965  0.23003023 -0.37886635  0.4959198  -0.05497603  0.27040312\n",
      " -0.12083829  0.24859314  0.24903432 -0.2589605 ] [ 0.04521305  0.71227157  0.28503208 -0.11035418  0.60439444 -0.001169\n",
      "  0.25063774 -0.14767335  0.70816256  0.37317312]\n",
      "\n",
      "# 39 Gradient out:  [-0.40835584 -0.9565003  -0.29727505 -1.2088636  -0.65236247 -1.00312827\n",
      " -0.58292472 -0.97808673 -0.97859729 -0.42396947]\n",
      "\n",
      "     Weights  out:  [-0.27262965  0.23003023 -0.37886635  0.4959198  -0.05497603  0.27040312\n",
      " -0.12083829  0.24859314  0.24903432 -0.2589605 ] [-0.0327536   0.50010386  0.22524661 -0.3635944   0.47124908 -0.22187879\n",
      "  0.13634486 -0.36387925  0.49186297  0.29243083]\n",
      "\n",
      "# 40 Gradient out:  [1.22263192 2.97465189 1.02719671 3.42218965 1.91901033 3.07155432\n",
      " 1.66324812 3.02096358 3.02202709 1.25361736]\n",
      "\n",
      "     Weights  out:  [-0.27262965  0.23003023 -0.37886635  0.4959198  -0.05497603  0.27040312\n",
      " -0.12083829  0.24859314  0.24903432 -0.2589605 ] [-0.11442477  0.3088038   0.16579159 -0.60536712  0.34077658 -0.42250445\n",
      "  0.01975991 -0.5594966   0.29614351  0.20763694]\n",
      "\n",
      "# 41 Gradient out:  [-0.09319132 -0.23381345 -0.07417495 -0.27686312 -0.1509545  -0.24273654\n",
      " -0.13117908 -0.23803191 -0.23812977 -0.0960907 ]\n",
      "\n",
      "     Weights  out:  [-0.27262965  0.23003023 -0.37886635  0.4959198  -0.05497603  0.27040312\n",
      " -0.12083829  0.24859314  0.24903432 -0.2589605 ] [0.13010162 0.90373418 0.37123094 0.07907081 0.72457865 0.19180642\n",
      " 0.35240954 0.04469612 0.90054893 0.45836041]\n",
      "\n",
      "# 42 Gradient out:  [-0.13763529 -0.35404848 -0.10843184 -0.42011926 -0.22651566 -0.36776802\n",
      " -0.19607491 -0.36053539 -0.36068586 -0.14209224]\n",
      "\n",
      "     Weights  out:  [-0.27262965  0.23003023 -0.37886635  0.4959198  -0.05497603  0.27040312\n",
      " -0.12083829  0.24859314  0.24903432 -0.2589605 ] [ 0.11146335  0.85697149  0.35639595  0.02369818  0.69438775  0.14325911\n",
      "  0.32617372 -0.00291026  0.85292298  0.43914227]\n",
      "\n",
      "# 43 Gradient out:  [-0.24146937 -0.64341895 -0.18728175 -0.76590628 -0.40656693 -0.66890323\n",
      " -0.35003387 -0.65546948 -0.65574899 -0.24974711]\n",
      "\n",
      "     Weights  out:  [-0.27262965  0.23003023 -0.37886635  0.4959198  -0.05497603  0.27040312\n",
      " -0.12083829  0.24859314  0.24903432 -0.2589605 ] [ 0.0839363   0.78616179  0.33470958 -0.06032567  0.64908462  0.0697055\n",
      "  0.28695874 -0.07501734  0.7807858   0.41072382]\n",
      "\n",
      "# 44 Gradient out:  [-0.54692975 -1.51704715 -0.41332201 -1.81881653 -0.94697508 -1.57951909\n",
      " -0.8111647  -1.54655371 -1.54723884 -0.56725583]\n",
      "\n",
      "     Weights  out:  [-0.27262965  0.23003023 -0.37886635  0.4959198  -0.05497603  0.27040312\n",
      " -0.12083829  0.24859314  0.24903432 -0.2589605 ] [ 0.03564242  0.657478    0.29725323 -0.21350693  0.56777123 -0.06407514\n",
      "  0.21695196 -0.20611123  0.64963601  0.3607744 ]\n",
      "\n",
      "# 45 Gradient out:  [0.98230378 2.65644789 0.81113879 3.04616039 1.64064545 2.74443272\n",
      " 1.39334381 2.69872018 2.69968627 1.01021068]\n",
      "\n",
      "     Weights  out:  [-0.27262965  0.23003023 -0.37886635  0.4959198  -0.05497603  0.27040312\n",
      " -0.12083829  0.24859314  0.24903432 -0.2589605 ] [-0.07374353  0.35406857  0.21458883 -0.57727023  0.37837622 -0.37997896\n",
      "  0.05471902 -0.51542198  0.34018824  0.24732323]\n",
      "\n",
      "# 46 Gradient out:  [-0.11455782 -0.29210329 -0.0905821  -0.34636031 -0.18747859 -0.30336193\n",
      " -0.16250657 -0.29742637 -0.29754985 -0.11821558]\n",
      "\n",
      "     Weights  out:  [-0.27262965  0.23003023 -0.37886635  0.4959198  -0.05497603  0.27040312\n",
      " -0.12083829  0.24859314  0.24903432 -0.2589605 ] [0.12271723 0.88535815 0.37681658 0.03196185 0.70650531 0.16890758\n",
      " 0.33338778 0.02432206 0.88012549 0.44936537]\n",
      "\n",
      "# 47 Gradient out:  [-0.18459321 -0.48501768 -0.14409865 -0.57658818 -0.30797212 -0.50405662\n",
      " -0.26571162 -0.49402042 -0.49422924 -0.19077746]\n",
      "\n",
      "     Weights  out:  [-0.27262965  0.23003023 -0.37886635  0.4959198  -0.05497603  0.27040312\n",
      " -0.12083829  0.24859314  0.24903432 -0.2589605 ] [ 0.09980566  0.82693749  0.35870016 -0.03731022  0.66900959  0.1082352\n",
      "  0.30088647 -0.03516322  0.82061552  0.42572225]\n",
      "\n",
      "# 48 Gradient out:  [-0.37696905 -1.03245297 -0.28822863 -1.23291886 -0.64645174 -1.0741565\n",
      " -0.55435785 -1.05216876 -1.05262615 -0.3905192 ]\n",
      "\n",
      "     Weights  out:  [-0.27262965  0.23003023 -0.37886635  0.4959198  -0.05497603  0.27040312\n",
      " -0.12083829  0.24859314  0.24903432 -0.2589605 ] [ 0.06288702  0.72993396  0.32988043 -0.15262785  0.60741517  0.00742387\n",
      "  0.24774415 -0.1339673   0.72176968  0.38756676]\n",
      "\n",
      "# 49 Gradient out:  [-0.4782128  -1.1435743  -0.35375401 -1.42636857 -0.76904023 -1.19681645\n",
      " -0.68258156 -1.16831787 -1.16890103 -0.49594514]\n",
      "\n",
      "     Weights  out:  [-0.27262965  0.23003023 -0.37886635  0.4959198  -0.05497603  0.27040312\n",
      " -0.12083829  0.24859314  0.24903432 -0.2589605 ] [-0.01250679  0.52344336  0.27223471 -0.39921162  0.47812482 -0.20740743\n",
      "  0.13687258 -0.34440105  0.51124444  0.30946292]\n",
      "\n",
      "Lineal error cuadrático medio (ECM): 1.1991858715054264\n",
      "\n",
      "# 0 Gradient out:  [-0.31572759 -0.3526122  -0.27646857 -0.12281079 -0.60706862 -0.1938784\n",
      " -1.20258644 -0.73736495 -1.20002998 -0.84701386]\n",
      "\n",
      "     Weights  out:  [-0.20206575 -0.176888   -0.22972654 -0.35532124 -0.01065013 -0.29255809\n",
      "  0.44383667  0.07384336  0.44041848  0.14534194] [ 0.44007327 -0.39306392 -0.09327238  0.47009822 -0.21951946 -0.4124992\n",
      "  0.39744556  0.38142212 -0.12926899  0.05572976]\n",
      "\n",
      "# 1 Gradient out:  [1.35289098 1.42282936 1.28351983 1.05057665 2.03287934 1.15216731\n",
      " 3.21260749 2.3781964  3.20896533 2.63465514]\n",
      "\n",
      "     Weights  out:  [-0.20206575 -0.176888   -0.22972654 -0.35532124 -0.01065013 -0.29255809\n",
      "  0.44383667  0.07384336  0.44041848  0.14534194] [ 0.37692775 -0.46358636 -0.14856609  0.44553606 -0.34093318 -0.45127488\n",
      "  0.15692827  0.23394913 -0.36927498 -0.11367301]\n",
      "\n",
      "# 2 Gradient out:  [-0.22054534 -0.23315291 -0.20748501 -0.15885095 -0.3295049  -0.18099177\n",
      " -0.53793608 -0.38129578 -0.53711567 -0.42233051]\n",
      "\n",
      "     Weights  out:  [-0.20206575 -0.176888   -0.22972654 -0.35532124 -0.01065013 -0.29255809\n",
      "  0.44383667  0.07384336  0.44041848  0.14534194] [ 0.64750595 -0.17902048  0.10813787  0.65565139  0.06564269 -0.22084142\n",
      "  0.79944977  0.70958841  0.27251808  0.41325801]\n",
      "\n",
      "# 3 Gradient out:  [-0.39494621 -0.41880827 -0.3702219  -0.27817257 -0.60095023 -0.32006458\n",
      " -0.99506939 -0.69879511 -0.9935231  -0.77637965]\n",
      "\n",
      "     Weights  out:  [-0.20206575 -0.176888   -0.22972654 -0.35532124 -0.01065013 -0.29255809\n",
      "  0.44383667  0.07384336  0.44041848  0.14534194] [ 6.03396877e-01 -2.25651067e-01  6.66408722e-02  6.23881202e-01\n",
      " -2.58292019e-04 -2.57039774e-01  6.91862552e-01  6.33329254e-01\n",
      "  1.65094950e-01  3.28791910e-01]\n",
      "\n",
      "# 4 Gradient out:  [-0.75413116 -0.80641271 -0.69971742 -0.49547482 -1.19903042 -0.58866146\n",
      " -2.05887349 -1.40840457 -2.0554578  -1.5759482 ]\n",
      "\n",
      "     Weights  out:  [-0.20206575 -0.176888   -0.22972654 -0.35532124 -0.01065013 -0.29255809\n",
      "  0.44383667  0.07384336  0.44041848  0.14534194] [ 0.52440764 -0.30941272 -0.00740351  0.56824669 -0.12044834 -0.32105269\n",
      "  0.49284867  0.49357023 -0.03360967  0.17351598]\n",
      "\n",
      "# 5 Gradient out:  [1.47518587 1.55323804 1.39724012 1.1308855  2.22123885 1.24796166\n",
      " 3.53425216 2.59677942 3.53001976 2.87809509]\n",
      "\n",
      "     Weights  out:  [-0.20206575 -0.176888   -0.22972654 -0.35532124 -0.01065013 -0.29255809\n",
      "  0.44383667  0.07384336  0.44041848  0.14534194] [ 0.3735814  -0.47069526 -0.14734699  0.46915172 -0.36025442 -0.43878498\n",
      "  0.08107398  0.21188932 -0.44470123 -0.14167366]\n",
      "\n",
      "# 6 Gradient out:  [-0.19888852 -0.21049528 -0.18686808 -0.14214161 -0.29926128 -0.1624958\n",
      " -0.49112119 -0.34698751 -0.49036795 -0.3847893 ]\n",
      "\n",
      "     Weights  out:  [-0.20206575 -0.176888   -0.22972654 -0.35532124 -0.01065013 -0.29255809\n",
      "  0.44383667  0.07384336  0.44041848  0.14534194] [ 0.66861858 -0.16004765  0.13210103  0.69532882  0.08399335 -0.18919265\n",
      "  0.78792441  0.7312452   0.26130272  0.43394536]\n",
      "\n",
      "# 7 Gradient out:  [-0.3444249  -0.36541715 -0.3226797  -0.24176991 -0.52578537 -0.27858356\n",
      " -0.8725362  -0.61196335 -0.87117762 -0.68026764]\n",
      "\n",
      "     Weights  out:  [-0.20206575 -0.176888   -0.22972654 -0.35532124 -0.01065013 -0.29255809\n",
      "  0.44383667  0.07384336  0.44041848  0.14534194] [ 0.62884088 -0.20214671  0.09472742  0.6669005   0.02414109 -0.22169181\n",
      "  0.68970017  0.6618477   0.16322913  0.3569875 ]\n",
      "\n",
      "# 8 Gradient out:  [-0.70325334 -0.75009182 -0.65459385 -0.47255389 -1.10419663 -0.55552635\n",
      " -1.87587549 -1.2936034  -1.87282472 -1.44459893]\n",
      "\n",
      "     Weights  out:  [-0.20206575 -0.176888   -0.22972654 -0.35532124 -0.01065013 -0.29255809\n",
      "  0.44383667  0.07384336  0.44041848  0.14534194] [ 0.5599559  -0.27523014  0.03019148  0.61854652 -0.08101598 -0.27740852\n",
      "  0.51519293  0.53945503 -0.01100639  0.22093397]\n",
      "\n",
      "# 9 Gradient out:  [1.12701312 1.18630831 1.06869249 0.87731992 1.71533656 0.95987618\n",
      " 2.7181763  2.0171476  2.71526189 2.23909775]\n",
      "\n",
      "     Weights  out:  [-0.20206575 -0.176888   -0.22972654 -0.35532124 -0.01065013 -0.29255809\n",
      "  0.44383667  0.07384336  0.44041848  0.14534194] [ 0.41930523 -0.42524851 -0.10072729  0.52403574 -0.30185531 -0.38851379\n",
      "  0.14001783  0.28073435 -0.38557134 -0.06798582]\n",
      "\n",
      "# 10 Gradient out:  [-0.31400941 -0.33333152 -0.29399815 -0.21957763 -0.48102745 -0.25343125\n",
      " -0.80019181 -0.56041462 -0.79894304 -0.62331806]\n",
      "\n",
      "     Weights  out:  [-0.20206575 -0.176888   -0.22972654 -0.35532124 -0.01065013 -0.29255809\n",
      "  0.44383667  0.07384336  0.44041848  0.14534194] [ 0.64470785 -0.18798684  0.11301121  0.69949973  0.041212   -0.19653856\n",
      "  0.68365309  0.68416387  0.15748104  0.37983373]\n",
      "\n",
      "# 11 Gradient out:  [-0.64494974 -0.68741198 -0.6008725  -0.43623046 -1.00936456 -0.51123508\n",
      " -1.7094259  -1.18179558 -1.70666536 -1.31903717]\n",
      "\n",
      "     Weights  out:  [-0.20206575 -0.176888   -0.22972654 -0.35532124 -0.01065013 -0.29255809\n",
      "  0.44383667  0.07384336  0.44041848  0.14534194] [ 0.58190597 -0.25465315  0.05421157  0.6555842  -0.05499349 -0.24722481\n",
      "  0.52361473  0.57208095 -0.00230757  0.25517012]\n",
      "\n",
      "# 12 Gradient out:  [0.58613959 0.61509621 0.5589881  0.48177021 0.90562392 0.5126557\n",
      " 1.40376673 1.0776803  1.4027803  1.19836202]\n",
      "\n",
      "     Weights  out:  [-0.20206575 -0.176888   -0.22972654 -0.35532124 -0.01065013 -0.29255809\n",
      "  0.44383667  0.07384336  0.44041848  0.14534194] [ 0.45291602 -0.39213554 -0.06596293  0.56833811 -0.2568664  -0.34947182\n",
      "  0.18172955  0.33572183 -0.34364064 -0.00863731]\n",
      "\n",
      "# 13 Gradient out:  [-0.72797025 -0.77690529 -0.67707539 -0.48615125 -1.14551278 -0.5732731\n",
      " -1.95155527 -1.34236797 -1.94834377 -1.4996062 ]\n",
      "\n",
      "     Weights  out:  [-0.20206575 -0.176888   -0.22972654 -0.35532124 -0.01065013 -0.29255809\n",
      "  0.44383667  0.07384336  0.44041848  0.14534194] [ 0.57014394 -0.2691163   0.04583469  0.66469215 -0.07574162 -0.24694068\n",
      "  0.46248289  0.55125789 -0.06308458  0.23103509]\n",
      "\n",
      "# 14 Gradient out:  [1.26499956 1.33413893 1.19649672 0.96734013 1.93875763 1.06707422\n",
      " 3.10443094 2.28127923 3.10088005 2.5354075 ]\n",
      "\n",
      "     Weights  out:  [-0.20206575 -0.176888   -0.22972654 -0.35532124 -0.01065013 -0.29255809\n",
      "  0.44383667  0.07384336  0.44041848  0.14534194] [ 0.42454989 -0.42449736 -0.08958038  0.5674619  -0.30484417 -0.36159531\n",
      "  0.07217184  0.2827843  -0.45275334 -0.06888615]\n",
      "\n",
      "# 15 Gradient out:  [-0.23738101 -0.25214696 -0.222095   -0.16530762 -0.36516827 -0.19112849\n",
      " -0.60909293 -0.42595168 -0.6081412  -0.47408092]\n",
      "\n",
      "     Weights  out:  [-0.20206575 -0.176888   -0.22972654 -0.35532124 -0.01065013 -0.29255809\n",
      "  0.44383667  0.07384336  0.44041848  0.14534194] [ 0.6775498  -0.15766957  0.14971896  0.76092993  0.08290735 -0.14818046\n",
      "  0.69305802  0.73904014  0.16742268  0.43819535]\n",
      "\n",
      "# 16 Gradient out:  [-0.45192368 -0.48120223 -0.42158444 -0.30867089 -0.70452107 -0.36004135\n",
      " -1.18774711 -0.82443667 -1.18585622 -0.91957238]\n",
      "\n",
      "     Weights  out:  [-0.20206575 -0.176888   -0.22972654 -0.35532124 -0.01065013 -0.29255809\n",
      "  0.44383667  0.07384336  0.44041848  0.14534194] [ 0.6300736  -0.20809897  0.10529996  0.7278684   0.0098737  -0.18640616\n",
      "  0.57123944  0.65384981  0.04579444  0.34337917]\n",
      "\n",
      "# 17 Gradient out:  [-0.73689348 -0.78781943 -0.68327126 -0.47646788 -1.15563726 -0.57186473\n",
      " -1.99078125 -1.34840991 -1.98721097 -1.50601049]\n",
      "\n",
      "     Weights  out:  [-0.20206575 -0.176888   -0.22972654 -0.35532124 -0.01065013 -0.29255809\n",
      "  0.44383667  0.07384336  0.44041848  0.14534194] [ 0.53968886 -0.30433941  0.02098307  0.66613423 -0.13103051 -0.25841443\n",
      "  0.33369002  0.48896247 -0.19137681  0.15946469]\n",
      "\n",
      "# 18 Gradient out:  [1.46939882 1.54425544 1.39410917 1.13148492 2.1728133  1.24807314\n",
      " 3.43202746 2.52378911 3.42770778 2.78893479]\n",
      "\n",
      "     Weights  out:  [-0.20206575 -0.176888   -0.22972654 -0.35532124 -0.01065013 -0.29255809\n",
      "  0.44383667  0.07384336  0.44041848  0.14534194] [ 0.39231017 -0.4619033  -0.11567118  0.57084065 -0.36215797 -0.37278738\n",
      " -0.06446623  0.21928049 -0.588819   -0.14173741]\n",
      "\n",
      "# 19 Gradient out:  [-0.2883647  -0.30707389 -0.26899824 -0.19709764 -0.45026921 -0.22977725\n",
      " -0.75917706 -0.52727355 -0.7579755  -0.58825449]\n",
      "\n",
      "     Weights  out:  [-0.20206575 -0.176888   -0.22972654 -0.35532124 -0.01065013 -0.29255809\n",
      "  0.44383667  0.07384336  0.44041848  0.14534194] [ 0.68618993 -0.15305221  0.16315065  0.79713764  0.07240469 -0.12317275\n",
      "  0.62193926  0.72403831  0.09672255  0.41604955]\n",
      "\n",
      "# 20 Gradient out:  [-0.59656106 -0.63658873 -0.55501436 -0.39982662 -0.94018463 -0.4705287\n",
      " -1.6002629  -1.10280914 -1.59765725 -1.23222068]\n",
      "\n",
      "     Weights  out:  [-0.20206575 -0.176888   -0.22972654 -0.35532124 -0.01065013 -0.29255809\n",
      "  0.44383667  0.07384336  0.44041848  0.14534194] [ 0.62851699 -0.21446699  0.10935101  0.75771811 -0.01764915 -0.1691282\n",
      "  0.47010385  0.6185836  -0.05487255  0.29839865]\n",
      "\n",
      "# 21 Gradient out:  [0.10400597 0.11166892 0.09883734 0.10371117 0.23644191 0.09704106\n",
      " 0.37749247 0.31859616 0.37798192 0.36886328]\n",
      "\n",
      "     Weights  out:  [-0.20206575 -0.176888   -0.22972654 -0.35532124 -0.01065013 -0.29255809\n",
      "  0.44383667  0.07384336  0.44041848  0.14534194] [ 0.50920478 -0.34178473 -0.00165187  0.67775278 -0.20568607 -0.26323394\n",
      "  0.15005127  0.39802177 -0.374404    0.05195452]\n",
      "\n",
      "# 22 Gradient out:  [-0.48764317 -0.51958079 -0.45278114 -0.30756107 -0.72101669 -0.37650384\n",
      " -1.23936133 -0.81955873 -1.23665679 -0.90720319]\n",
      "\n",
      "     Weights  out:  [-0.20206575 -0.176888   -0.22972654 -0.35532124 -0.01065013 -0.29255809\n",
      "  0.44383667  0.07384336  0.44041848  0.14534194] [ 0.53000597 -0.31945095  0.0181156   0.69849502 -0.15839769 -0.24382573\n",
      "  0.22554976  0.46174101 -0.29880761  0.12572717]\n",
      "\n",
      "# 23 Gradient out:  [1.36396524 1.43929668 1.28885384 1.03319501 2.08685808 1.14537618\n",
      " 3.35494994 2.45148882 3.35089923 2.72408595]\n",
      "\n",
      "     Weights  out:  [-0.20206575 -0.176888   -0.22972654 -0.35532124 -0.01065013 -0.29255809\n",
      "  0.44383667  0.07384336  0.44041848  0.14534194] [ 0.43247734 -0.42336711 -0.07244063  0.6369828  -0.30260103 -0.31912649\n",
      " -0.0223225   0.29782926 -0.54613897 -0.05571347]\n",
      "\n",
      "# 24 Gradient out:  [-0.20916148 -0.22270467 -0.19514833 -0.14317129 -0.32650227 -0.16678672\n",
      " -0.55015463 -0.38235268 -0.54928666 -0.42654953]\n",
      "\n",
      "     Weights  out:  [-0.20206575 -0.176888   -0.22972654 -0.35532124 -0.01065013 -0.29255809\n",
      "  0.44383667  0.07384336  0.44041848  0.14534194] [ 0.70527039 -0.13550777  0.18533014  0.84362181  0.11477059 -0.09005126\n",
      "  0.64866748  0.78812702  0.12404088  0.48910373]\n",
      "\n",
      "# 25 Gradient out:  [-0.38324208 -0.40873119 -0.35684416 -0.25874236 -0.60346632 -0.30334535\n",
      " -1.02413792 -0.70810321 -1.02249865 -0.79104938]\n",
      "\n",
      "     Weights  out:  [-0.20206575 -0.176888   -0.22972654 -0.35532124 -0.01065013 -0.29255809\n",
      "  0.44383667  0.07384336  0.44041848  0.14534194] [ 0.66343809 -0.18004871  0.14630047  0.81498755  0.04947013 -0.1234086\n",
      "  0.53863656  0.71165649  0.01418354  0.40379382]\n",
      "\n",
      "# 26 Gradient out:  [-0.78280672 -0.83560774 -0.72761414 -0.51801475 -1.22689797 -0.61415137\n",
      " -2.0959564  -1.43439424 -2.09237142 -1.60158551]\n",
      "\n",
      "     Weights  out:  [-0.20206575 -0.176888   -0.22972654 -0.35532124 -0.01065013 -0.29255809\n",
      "  0.44383667  0.07384336  0.44041848  0.14534194] [ 0.58678968 -0.26179494  0.07493164  0.76323908 -0.07122313 -0.18407767\n",
      "  0.33380897  0.57003585 -0.19031619  0.24558394]\n",
      "\n",
      "# 27 Gradient out:  [1.44437709 1.52121266 1.36731979 1.10062661 2.17155895 1.21859523\n",
      " 3.46466374 2.53575606 3.4603225  2.80989561]\n",
      "\n",
      "     Weights  out:  [-0.20206575 -0.176888   -0.22972654 -0.35532124 -0.01065013 -0.29255809\n",
      "  0.44383667  0.07384336  0.44041848  0.14534194] [ 0.43022833 -0.42891649 -0.07059119  0.65963613 -0.31660272 -0.30690795\n",
      " -0.08538231  0.283157   -0.60879047 -0.07473316]\n",
      "\n",
      "# 28 Gradient out:  [-0.22175115 -0.23651623 -0.20647678 -0.14986583 -0.34972543 -0.17557584\n",
      " -0.59348024 -0.4106478  -0.59253712 -0.45885118]\n",
      "\n",
      "     Weights  out:  [-0.20206575 -0.176888   -0.22972654 -0.35532124 -0.01065013 -0.29255809\n",
      "  0.44383667  0.07384336  0.44041848  0.14534194] [ 0.71910375 -0.12467396  0.20287277  0.87976145  0.11770907 -0.0631889\n",
      "  0.60755044  0.79030821  0.08327403  0.48724596]\n",
      "\n",
      "# 29 Gradient out:  [-0.42063885 -0.44925408 -0.39099502 -0.2807776  -0.66764027 -0.33089601\n",
      " -1.13975654 -0.78492904 -1.13791542 -0.87796074]\n",
      "\n",
      "     Weights  out:  [-0.20206575 -0.176888   -0.22972654 -0.35532124 -0.01065013 -0.29255809\n",
      "  0.44383667  0.07384336  0.44041848  0.14534194] [ 0.67475352 -0.1719772   0.16157742  0.84978828  0.04776398 -0.09830407\n",
      "  0.48885439  0.70817865 -0.03523339  0.39547573]\n",
      "\n",
      "# 30 Gradient out:  [-0.78666073 -0.83837724 -0.73217231 -0.52131671 -1.21165802 -0.61877688\n",
      " -2.06179801 -1.40728787 -2.0581038  -1.56721936]\n",
      "\n",
      "     Weights  out:  [-0.20206575 -0.176888   -0.22972654 -0.35532124 -0.01065013 -0.29255809\n",
      "  0.44383667  0.07384336  0.44041848  0.14534194] [ 0.59062575 -0.26182802  0.08337841  0.79363276 -0.08576407 -0.16448327\n",
      "  0.26090308  0.55119284 -0.26281648  0.21988358]\n",
      "\n",
      "# 31 Gradient out:  [1.45326876 1.52683744 1.37912315 1.11873486 2.14149861 1.23473706\n",
      " 3.38036    2.48412825 3.37600726 2.74351274]\n",
      "\n",
      "     Weights  out:  [-0.20206575 -0.176888   -0.22972654 -0.35532124 -0.01065013 -0.29255809\n",
      "  0.44383667  0.07384336  0.44041848  0.14534194] [ 0.4332936  -0.42950347 -0.06305605  0.68936942 -0.32809568 -0.28823864\n",
      " -0.15145652  0.26973527 -0.67443724 -0.09356029]\n",
      "\n",
      "# 32 Gradient out:  [-0.29009493 -0.31011174 -0.26938389 -0.19261743 -0.46345357 -0.22747808\n",
      " -0.79374275 -0.54593787 -0.79246603 -0.61123695]\n",
      "\n",
      "     Weights  out:  [-0.20206575 -0.176888   -0.22972654 -0.35532124 -0.01065013 -0.29255809\n",
      "  0.44383667  0.07384336  0.44041848  0.14534194] [ 7.23947352e-01 -1.24135980e-01  2.12768582e-01  9.13116393e-01\n",
      "  1.00204045e-01 -4.12912319e-02  5.24615481e-01  7.66560919e-01\n",
      "  7.64216903e-04  4.55142255e-01]\n",
      "\n",
      "# 33 Gradient out:  [-0.613772   -0.65624946 -0.56962746 -0.40424054 -0.97711058 -0.47968391\n",
      " -1.67737918 -1.14868033 -1.67459092 -1.28550937]\n",
      "\n",
      "     Weights  out:  [-0.20206575 -0.176888   -0.22972654 -0.35532124 -0.01065013 -0.29255809\n",
      "  0.44383667  0.07384336  0.44041848  0.14534194] [ 0.66592837 -0.18615833  0.1588918   0.87459291  0.00751333 -0.08678685\n",
      "  0.36586693  0.65737334 -0.15772899  0.33289486]\n",
      "\n",
      "# 34 Gradient out:  [0.23618669 0.26112335 0.2136857  0.15969062 0.53022999 0.17865202\n",
      " 0.95565102 0.69266419 0.95531717 0.80390966]\n",
      "\n",
      "     Weights  out:  [-0.20206575 -0.176888   -0.22972654 -0.35532124 -0.01065013 -0.29255809\n",
      "  0.44383667  0.07384336  0.44041848  0.14534194] [ 0.54317397 -0.31740822  0.04496631  0.7937448  -0.18790878 -0.18272363\n",
      "  0.03039109  0.42763728 -0.49264717  0.07579299]\n",
      "\n",
      "# 35 Gradient out:  [-0.77900801 -0.8293917  -0.72574451 -0.51795585 -1.18895056 -0.61431734\n",
      " -2.0169716  -1.37641707 -2.01329034 -1.53065082]\n",
      "\n",
      "     Weights  out:  [-0.20206575 -0.176888   -0.22972654 -0.35532124 -0.01065013 -0.29255809\n",
      "  0.44383667  0.07384336  0.44041848  0.14534194] [ 0.5904113  -0.26518355  0.08770345  0.82568292 -0.08186279 -0.14699323\n",
      "  0.2215213   0.56617012 -0.30158374  0.23657492]\n",
      "\n",
      "# 36 Gradient out:  [1.44473824 1.51772229 1.37113238 1.11209405 2.1264573  1.22761708\n",
      " 3.35576339 2.46558055 3.35141381 2.72249997]\n",
      "\n",
      "     Weights  out:  [-0.20206575 -0.176888   -0.22972654 -0.35532124 -0.01065013 -0.29255809\n",
      "  0.44383667  0.07384336  0.44041848  0.14534194] [ 0.4346097  -0.43106189 -0.05744545  0.72209175 -0.3196529  -0.26985669\n",
      " -0.18187302  0.2908867  -0.70424181 -0.06955524]\n",
      "\n",
      "# 37 Gradient out:  [-0.29977859 -0.32073569 -0.27809459 -0.19773086 -0.48126394 -0.23422197\n",
      " -0.82701707 -0.56760847 -0.82568146 -0.63596919]\n",
      "\n",
      "     Weights  out:  [-0.20206575 -0.176888   -0.22972654 -0.35532124 -0.01065013 -0.29255809\n",
      "  0.44383667  0.07384336  0.44041848  0.14534194] [ 0.72355735 -0.12751743  0.21678103  0.94451056  0.10563856 -0.02433328\n",
      "  0.48927966  0.78400281 -0.03395905  0.47494475]\n",
      "\n",
      "# 38 Gradient out:  [-0.64096468 -0.68548381 -0.59465392 -0.42074113 -1.02073497 -0.50015307\n",
      " -1.75455886 -1.19976387 -1.75161723 -1.34277376]\n",
      "\n",
      "     Weights  out:  [-0.20206575 -0.176888   -0.22972654 -0.35532124 -0.01065013 -0.29255809\n",
      "  0.44383667  0.07384336  0.44041848  0.14534194] [ 0.66360163 -0.19166457  0.16116211  0.90496439  0.00938577 -0.07117767\n",
      "  0.32387625  0.67048112 -0.19909534  0.34775092]\n",
      "\n",
      "# 39 Gradient out:  [0.46007623 0.4998597  0.42234358 0.31279967 0.88612229 0.35685358\n",
      " 1.55838989 1.11238391 1.55712564 1.27335903]\n",
      "\n",
      "     Weights  out:  [-0.20206575 -0.176888   -0.22972654 -0.35532124 -0.01065013 -0.29255809\n",
      "  0.44383667  0.07384336  0.44041848  0.14534194] [ 0.5354087  -0.32876133  0.04223133  0.82081616 -0.19476122 -0.17120828\n",
      " -0.02703553  0.43052834 -0.54941878  0.07919616]\n",
      "\n",
      "# 40 Gradient out:  [-0.72837715 -0.77817098 -0.6764339  -0.48003046 -1.14977299 -0.56996914\n",
      " -1.97018637 -1.34744487 -1.96683466 -1.50610532]\n",
      "\n",
      "     Weights  out:  [-0.20206575 -0.176888   -0.22972654 -0.35532124 -0.01065013 -0.29255809\n",
      "  0.44383667  0.07384336  0.44041848  0.14534194] [ 0.62742394 -0.22878939  0.12670004  0.8833761  -0.01753676 -0.09983757\n",
      "  0.28464245  0.65300513 -0.23799366  0.33386797]\n",
      "\n",
      "# 41 Gradient out:  [1.12651306 1.19647933 1.05711725 0.82464604 1.80606507 0.9258368\n",
      " 2.98348624 2.15089954 2.97990957 2.40722129]\n",
      "\n",
      "     Weights  out:  [-0.20206575 -0.176888   -0.22972654 -0.35532124 -0.01065013 -0.29255809\n",
      "  0.44383667  0.07384336  0.44041848  0.14534194] [ 0.48174851 -0.38442359 -0.00858674  0.78737001 -0.24749136 -0.2138314\n",
      " -0.10939482  0.38351615 -0.63136059  0.03264691]\n",
      "\n",
      "# 42 Gradient out:  [-0.28619395 -0.3060883  -0.26561083 -0.18933214 -0.45851621 -0.22396828\n",
      " -0.78677451 -0.54051395 -0.78550641 -0.60542298]\n",
      "\n",
      "     Weights  out:  [-0.20206575 -0.176888   -0.22972654 -0.35532124 -0.01065013 -0.29255809\n",
      "  0.44383667  0.07384336  0.44041848  0.14534194] [ 0.70705112 -0.14512772  0.20283671  0.95229921  0.11372165 -0.02866404\n",
      "  0.48730243  0.81369606 -0.03537867  0.51409116]\n",
      "\n",
      "# 43 Gradient out:  [-0.60507407 -0.64718781 -0.56131483 -0.39743835 -0.96547169 -0.47217993\n",
      " -1.65976499 -1.1357021  -1.65700361 -1.2714248 ]\n",
      "\n",
      "     Weights  out:  [-0.20206575 -0.176888   -0.22972654 -0.35532124 -0.01065013 -0.29255809\n",
      "  0.44383667  0.07384336  0.44041848  0.14534194] [ 0.64981233 -0.20634538  0.14971455  0.91443279  0.02201841 -0.07345769\n",
      "  0.32994752  0.70559327 -0.19247996  0.39300657]\n",
      "\n",
      "# 44 Gradient out:  [0.16117719 0.18237088 0.14253072 0.10281782 0.42215357 0.11527012\n",
      " 0.78490619 0.56864529 0.78482476 0.66743989]\n",
      "\n",
      "     Weights  out:  [-0.20206575 -0.176888   -0.22972654 -0.35532124 -0.01065013 -0.29255809\n",
      "  0.44383667  0.07384336  0.44041848  0.14534194] [ 0.52879752 -0.33578294  0.03745158  0.83494511 -0.17107593 -0.16789368\n",
      " -0.00200547  0.47845285 -0.52388068  0.13872161]\n",
      "\n",
      "# 45 Gradient out:  [-0.73084998 -0.77647416 -0.68217745 -0.48834039 -1.09180912 -0.57895989\n",
      " -1.84037278 -1.25374211 -1.836856   -1.38947167]\n",
      "\n",
      "     Weights  out:  [-0.20206575 -0.176888   -0.22972654 -0.35532124 -0.01065013 -0.29255809\n",
      "  0.44383667  0.07384336  0.44041848  0.14534194] [ 0.56103296 -0.29930877  0.06595772  0.85550868 -0.08664521 -0.14483965\n",
      "  0.15497576  0.59218191 -0.36691573  0.27220959]\n",
      "\n",
      "# 46 Gradient out:  [1.44145199 1.51521497 1.36710611 1.10594181 2.13143766 1.22230902\n",
      " 3.37372045 2.47492922 3.36935028 2.7349734 ]\n",
      "\n",
      "     Weights  out:  [-0.20206575 -0.176888   -0.22972654 -0.35532124 -0.01065013 -0.29255809\n",
      "  0.44383667  0.07384336  0.44041848  0.14534194] [ 0.41486296 -0.4546036  -0.07047777  0.7578406  -0.30500704 -0.26063163\n",
      " -0.21309879  0.34143348 -0.73428693 -0.00568475]\n",
      "\n",
      "# 47 Gradient out:  [-0.2867041  -0.30679857 -0.26591485 -0.18888605 -0.46077331 -0.22385939\n",
      " -0.7923087  -0.54360533 -0.79102884 -0.60917283]\n",
      "\n",
      "     Weights  out:  [-0.20206575 -0.176888   -0.22972654 -0.35532124 -0.01065013 -0.29255809\n",
      "  0.44383667  0.07384336  0.44041848  0.14534194] [ 0.70315336 -0.1515606   0.20294345  0.97902896  0.1212805  -0.01616983\n",
      "  0.4616453   0.83641933 -0.06041687  0.54130993]\n",
      "\n",
      "# 48 Gradient out:  [-0.60789988 -0.65039748 -0.56373364 -0.39825866 -0.97139101 -0.47374361\n",
      " -1.67198917 -1.1430267  -1.66919909 -1.2799132 ]\n",
      "\n",
      "     Weights  out:  [-0.20206575 -0.176888   -0.22972654 -0.35532124 -0.01065013 -0.29255809\n",
      "  0.44383667  0.07384336  0.44041848  0.14534194] [ 0.64581254 -0.21292032  0.14976048  0.94125175  0.02912583 -0.06094171\n",
      "  0.30318356  0.72769826 -0.21862264  0.41947536]\n",
      "\n",
      "# 49 Gradient out:  [0.18300672 0.20667293 0.1618612  0.11346803 0.46672022 0.12974363\n",
      " 0.87023158 0.62440446 0.8700221  0.7317748 ]\n",
      "\n",
      "     Weights  out:  [-0.20206575 -0.176888   -0.22972654 -0.35532124 -0.01065013 -0.29255809\n",
      "  0.44383667  0.07384336  0.44041848  0.14534194] [ 0.52423256 -0.34299981  0.03701375  0.86160002 -0.16515237 -0.15569043\n",
      " -0.03121427  0.49909292 -0.55246245  0.16349272]\n",
      "\n",
      "Tanh error cuadrático medio (ECM): 0.595390589580377\n",
      "\n",
      "# 0 Gradient out:  [0.96706219 1.85806933 3.16836898 2.97600993 3.07250747 3.09189966\n",
      " 1.31819747 3.32690911 3.19326969 2.23344759]\n",
      "\n",
      "     Weights  out:  [-0.4646638  -0.07380256  0.35525544  0.25518733  0.30119532  0.31137587\n",
      " -0.25183404  0.47098708  0.37100222  0.02293154] [ 0.13001134  0.44551845  0.26600572 -0.07306572 -0.04299636 -0.1625181\n",
      "  0.10779584 -0.42696479 -0.46686666 -0.454004  ]\n",
      "\n",
      "# 1 Gradient out:  [-0.03767835 -0.066431   -0.10559069 -0.09889328 -0.10220112 -0.10287975\n",
      " -0.05001211 -0.11122772 -0.10648162 -0.0768008 ]\n",
      "\n",
      "     Weights  out:  [-0.4646638  -0.07380256  0.35525544  0.25518733  0.30119532  0.31137587\n",
      " -0.25183404  0.47098708  0.37100222  0.02293154] [ 0.32342378  0.81713232  0.89967952  0.52213627  0.57150513  0.45586184\n",
      "  0.37143533  0.23841703  0.17178727 -0.00731448]\n",
      "\n",
      "# 2 Gradient out:  [-0.04521237 -0.08155232 -0.13110337 -0.12264482 -0.12682425 -0.1276813\n",
      " -0.06077645 -0.13820844 -0.13222739 -0.09468474]\n",
      "\n",
      "     Weights  out:  [-0.4646638  -0.07380256  0.35525544  0.25518733  0.30119532  0.31137587\n",
      " -0.25183404  0.47098708  0.37100222  0.02293154] [ 0.31588811  0.80384612  0.87856138  0.50235762  0.55106491  0.43528589\n",
      "  0.36143291  0.21617149  0.15049095 -0.02267464]\n",
      "\n",
      "# 3 Gradient out:  [-0.05657757 -0.1050864  -0.17132595 -0.16004511 -0.16562201 -0.16676496\n",
      " -0.07731307 -0.18077781 -0.17282301 -0.12265963]\n",
      "\n",
      "     Weights  out:  [-0.4646638  -0.07380256  0.35525544  0.25518733  0.30119532  0.31137587\n",
      " -0.25183404  0.47098708  0.37100222  0.02293154] [ 0.30684564  0.78753566  0.85234071  0.47782865  0.52570006  0.40974963\n",
      "  0.34927762  0.1885298   0.12404547 -0.04161159]\n",
      "\n",
      "# 4 Gradient out:  [-0.0755085  -0.14596698 -0.24235882 -0.22599238 -0.23408906 -0.23574717\n",
      " -0.1055502  -0.2560258  -0.24452695 -0.17157257]\n",
      "\n",
      "     Weights  out:  [-0.4646638  -0.07380256  0.35525544  0.25518733  0.30119532  0.31137587\n",
      " -0.25183404  0.47098708  0.37100222  0.02293154] [ 0.29553012  0.76651838  0.81807552  0.44581963  0.49257565  0.37639663\n",
      "  0.33381501  0.15237424  0.08948087 -0.06614351]\n",
      "\n",
      "# 5 Gradient out:  [-0.11223958 -0.23034058 -0.39231675 -0.36492488 -0.37848916 -0.38126405\n",
      " -0.16241928 -0.41508059 -0.39593641 -0.27343948]\n",
      "\n",
      "     Weights  out:  [-0.4646638  -0.07380256  0.35525544  0.25518733  0.30119532  0.31137587\n",
      " -0.25183404  0.47098708  0.37100222  0.02293154] [ 0.28042842  0.73732498  0.76960375  0.40062116  0.44575784  0.3292472\n",
      "  0.31270497  0.10116908  0.04057548 -0.10045803]\n",
      "\n",
      "# 6 Gradient out:  [-0.20270821 -0.46167758 -0.81792875 -0.75795439 -0.78769424 -0.79376942\n",
      " -0.31223721 -0.86739634 -0.82582466 -0.55661638]\n",
      "\n",
      "     Weights  out:  [-0.4646638  -0.07380256  0.35525544  0.25518733  0.30119532  0.31137587\n",
      " -0.25183404  0.47098708  0.37100222  0.02293154] [ 0.25798051  0.69125686  0.6911404   0.32763618  0.37006001  0.25299439\n",
      "  0.28022111  0.01815296 -0.0386118  -0.15514592]\n",
      "\n",
      "# 7 Gradient out:  [-0.36987052 -1.05812653 -1.98067171 -1.81746752 -1.89804052 -1.91459728\n",
      " -0.66857975 -2.11599385 -2.00231794 -1.29677866]\n",
      "\n",
      "     Weights  out:  [-0.4646638  -0.07380256  0.35525544  0.25518733  0.30119532  0.31137587\n",
      " -0.25183404  0.47098708  0.37100222  0.02293154] [ 0.21743887  0.59892135  0.52755465  0.1760453   0.21252116  0.09424051\n",
      "  0.21777367 -0.15532631 -0.20377673 -0.2664692 ]\n",
      "\n",
      "# 8 Gradient out:  [0.67607276 1.08189352 1.64103019 1.54883273 1.59410682 1.60343057\n",
      " 0.85045852 1.72371718 1.65358014 1.23442637]\n",
      "\n",
      "     Weights  out:  [-0.4646638  -0.07380256  0.35525544  0.25518733  0.30119532  0.31137587\n",
      " -0.25183404  0.47098708  0.37100222  0.02293154] [ 0.14346476  0.38729604  0.13142031 -0.1874482  -0.16708694 -0.28867895\n",
      "  0.08405772 -0.57852508 -0.60424032 -0.52582493]\n",
      "\n",
      "# 9 Gradient out:  [-0.25043303 -0.73767067 -1.32618659 -1.20148274 -1.26180948 -1.27451663\n",
      " -0.48423595 -1.43568654 -1.34341804 -0.87236578]\n",
      "\n",
      "     Weights  out:  [-0.4646638  -0.07380256  0.35525544  0.25518733  0.30119532  0.31137587\n",
      " -0.25183404  0.47098708  0.37100222  0.02293154] [ 0.27867931  0.60367475  0.45962635  0.12231835  0.15173442  0.03200716\n",
      "  0.25414943 -0.23378164 -0.27352429 -0.27893966]\n",
      "\n",
      "# 10 Gradient out:  [0.91030042 1.60009248 2.57769953 2.42425056 2.50028786 2.51578633\n",
      " 1.19616993 2.71050986 2.5981516  1.87258542]\n",
      "\n",
      "     Weights  out:  [-0.4646638  -0.07380256  0.35525544  0.25518733  0.30119532  0.31137587\n",
      " -0.25183404  0.47098708  0.37100222  0.02293154] [ 0.22859271  0.45614061  0.19438903 -0.1179782  -0.10062747 -0.22289616\n",
      "  0.15730224 -0.52091895 -0.5422079  -0.45341281]\n",
      "\n",
      "# 11 Gradient out:  [-0.13535296 -0.31038542 -0.55135286 -0.51085229 -0.5309371  -0.53503942\n",
      " -0.20933127 -0.58477342 -0.5566849  -0.3746659 ]\n",
      "\n",
      "     Weights  out:  [-0.4646638  -0.07380256  0.35525544  0.25518733  0.30119532  0.31137587\n",
      " -0.25183404  0.47098708  0.37100222  0.02293154] [ 0.41065279  0.77615911  0.70992894  0.36687191  0.3994301   0.28026111\n",
      "  0.39653622  0.02118302 -0.02257758 -0.07889573]\n",
      "\n",
      "# 12 Gradient out:  [-0.29302835 -0.76088599 -1.40353739 -1.29495635 -1.34881436 -1.35981485\n",
      " -0.49101123 -1.4927489  -1.41781365 -0.93167207]\n",
      "\n",
      "     Weights  out:  [-0.4646638  -0.07380256  0.35525544  0.25518733  0.30119532  0.31137587\n",
      " -0.25183404  0.47098708  0.37100222  0.02293154] [ 0.3835822   0.71408202  0.59965836  0.26470145  0.29324268  0.17325322\n",
      "  0.35466997 -0.09577166 -0.13391456 -0.15382891]\n",
      "\n",
      "# 13 Gradient out:  [0.58410616 1.35042688 2.56570899 2.4120202  2.49114189 2.50655804\n",
      " 0.85400459 2.68067726 2.58440982 1.71849993]\n",
      "\n",
      "     Weights  out:  [-0.4646638  -0.07380256  0.35525544  0.25518733  0.30119532  0.31137587\n",
      " -0.25183404  0.47098708  0.37100222  0.02293154] [ 0.32497653  0.56190483  0.31895089  0.00571018  0.02347981 -0.09870975\n",
      "  0.25646772 -0.39432144 -0.41747729 -0.34016332]\n",
      "\n",
      "# 14 Gradient out:  [-0.04992515 -0.09876779 -0.1656424  -0.15430396 -0.15991464 -0.16106331\n",
      " -0.07072916 -0.17510139 -0.16714356 -0.11654467]\n",
      "\n",
      "     Weights  out:  [-0.4646638  -0.07380256  0.35525544  0.25518733  0.30119532  0.31137587\n",
      " -0.25183404  0.47098708  0.37100222  0.02293154] [0.44179776 0.8319902  0.83209268 0.48811422 0.52170818 0.40260186\n",
      " 0.42726864 0.14181401 0.09940467 0.00353666]\n",
      "\n",
      "# 15 Gradient out:  [-0.06638651 -0.13642171 -0.23247094 -0.21622948 -0.22427132 -0.22591661\n",
      " -0.09615006 -0.24597991 -0.23461789 -0.1619829 ]\n",
      "\n",
      "     Weights  out:  [-0.4646638  -0.07380256  0.35525544  0.25518733  0.30119532  0.31137587\n",
      " -0.25183404  0.47098708  0.37100222  0.02293154] [ 0.43181273  0.81223664  0.7989642   0.45725343  0.48972526  0.3703892\n",
      "  0.41312281  0.10679373  0.06597596 -0.01977227]\n",
      "\n",
      "# 16 Gradient out:  [-0.09818929 -0.21314258 -0.37112732 -0.34450291 -0.35769669 -0.36039361\n",
      " -0.14689762 -0.39318137 -0.37463928 -0.25524436]\n",
      "\n",
      "     Weights  out:  [-0.4646638  -0.07380256  0.35525544  0.25518733  0.30119532  0.31137587\n",
      " -0.25183404  0.47098708  0.37100222  0.02293154] [ 0.41853543  0.7849523   0.75247002  0.41400754  0.44487099  0.32520588\n",
      "  0.3938928   0.05759775  0.01905238 -0.05216885]\n",
      "\n",
      "# 17 Gradient out:  [-0.17683931 -0.41960688 -0.7539659  -0.69779385 -0.72565901 -0.73134872\n",
      " -0.27935859 -0.80022516 -0.76135441 -0.50879743]\n",
      "\n",
      "     Weights  out:  [-0.4646638  -0.07380256  0.35525544  0.25518733  0.30119532  0.31137587\n",
      " -0.25183404  0.47098708  0.37100222  0.02293154] [ 0.39889757  0.74232379  0.67824455  0.34510695  0.37333166  0.25312715\n",
      "  0.36451327 -0.02103853 -0.05587547 -0.10321772]\n",
      "\n",
      "# 18 Gradient out:  [-0.38594388 -1.06213972 -1.9737907  -1.8143492  -1.89312362 -1.90929272\n",
      " -0.67787114 -2.10611253 -1.99491993 -1.29970689]\n",
      "\n",
      "     Weights  out:  [-0.4646638  -0.07380256  0.35525544  0.25518733  0.30119532  0.31137587\n",
      " -0.25183404  0.47098708  0.37100222  0.02293154] [ 0.36352971  0.65840241  0.52745137  0.20554818  0.22819985  0.10685741\n",
      "  0.30864156 -0.18108356 -0.20814636 -0.20497721]\n",
      "\n",
      "# 19 Gradient out:  [0.7886657  1.27642933 1.94709293 1.83613614 1.89058142 1.90180306\n",
      " 0.9988345  2.04693574 1.96222395 1.45913681]\n",
      "\n",
      "     Weights  out:  [-0.4646638  -0.07380256  0.35525544  0.25518733  0.30119532  0.31137587\n",
      " -0.25183404  0.47098708  0.37100222  0.02293154] [ 0.28634094  0.44597447  0.13269323 -0.15732166 -0.15042487 -0.27500114\n",
      "  0.17306733 -0.60230607 -0.60713034 -0.46491859]\n",
      "\n",
      "# 20 Gradient out:  [-0.37989882 -1.04360453 -1.93889991 -1.78252741 -1.8597809  -1.87563785\n",
      " -0.66635789 -2.06884209 -1.95963027 -1.27714258]\n",
      "\n",
      "     Weights  out:  [-0.4646638  -0.07380256  0.35525544  0.25518733  0.30119532  0.31137587\n",
      " -0.25183404  0.47098708  0.37100222  0.02293154] [ 0.44407408  0.70126033  0.52211182  0.20990557  0.22769141  0.10535948\n",
      "  0.37283423 -0.19291892 -0.21468555 -0.17309123]\n",
      "\n",
      "# 21 Gradient out:  [0.84473587 1.38844639 2.13609675 2.01243178 2.07311108 2.08561782\n",
      " 1.0790086  2.24741099 2.15296278 1.59216293]\n",
      "\n",
      "     Weights  out:  [-0.4646638  -0.07380256  0.35525544  0.25518733  0.30119532  0.31137587\n",
      " -0.25183404  0.47098708  0.37100222  0.02293154] [ 0.36809431  0.49253943  0.13433183 -0.14659991 -0.14426477 -0.26976809\n",
      "  0.23956265 -0.60668734 -0.60661161 -0.42851974]\n",
      "\n",
      "# 22 Gradient out:  [-0.29914936 -0.81632568 -1.52457734 -1.40424711 -1.4638951  -1.47608777\n",
      " -0.51871702 -1.62361993 -1.54041897 -1.00395929]\n",
      "\n",
      "     Weights  out:  [-0.4646638  -0.07380256  0.35525544  0.25518733  0.30119532  0.31137587\n",
      " -0.25183404  0.47098708  0.37100222  0.02293154] [ 0.53704149  0.77022871  0.56155118  0.25588645  0.27035745  0.14735547\n",
      "  0.45536437 -0.15720514 -0.17601905 -0.11008716]\n",
      "\n",
      "# 23 Gradient out:  [0.69291698 1.56368809 2.88267911 2.69970347 2.79240617 2.81081951\n",
      " 1.02189058 3.02795273 2.90581711 1.95000239]\n",
      "\n",
      "     Weights  out:  [-0.4646638  -0.07380256  0.35525544  0.25518733  0.30119532  0.31137587\n",
      " -0.25183404  0.47098708  0.37100222  0.02293154] [ 0.47721161  0.60696357  0.25663572 -0.02496298 -0.02242157 -0.14786209\n",
      "  0.35162097 -0.48192912 -0.48410285 -0.31087902]\n",
      "\n",
      "# 24 Gradient out:  [-0.03334554 -0.06973305 -0.11967071 -0.11123703 -0.11541369 -0.116268\n",
      " -0.04879688 -0.12668118 -0.12078509 -0.0830312 ]\n",
      "\n",
      "     Weights  out:  [-0.4646638  -0.07380256  0.35525544  0.25518733  0.30119532  0.31137587\n",
      " -0.25183404  0.47098708  0.37100222  0.02293154] [0.61579501 0.91970119 0.83317154 0.51497772 0.53605966 0.41430182\n",
      " 0.55599908 0.12366142 0.09706058 0.07912146]\n",
      "\n",
      "# 25 Gradient out:  [-0.04125071 -0.08857009 -0.15358351 -0.14262398 -0.14805377 -0.14916391\n",
      " -0.06131331 -0.16267558 -0.15503012 -0.1058965 ]\n",
      "\n",
      "     Weights  out:  [-0.4646638  -0.07380256  0.35525544  0.25518733  0.30119532  0.31137587\n",
      " -0.25183404  0.47098708  0.37100222  0.02293154] [0.6091259  0.90575458 0.80923739 0.49273031 0.51297693 0.39104822\n",
      " 0.54623971 0.09832519 0.07290356 0.06251522]\n",
      "\n",
      "# 26 Gradient out:  [-0.05411045 -0.12019958 -0.21112527 -0.19583168 -0.20341264 -0.20496172\n",
      " -0.08207838 -0.22378138 -0.21314132 -0.14445445]\n",
      "\n",
      "     Weights  out:  [-0.4646638  -0.07380256  0.35525544  0.25518733  0.30119532  0.31137587\n",
      " -0.25183404  0.47098708  0.37100222  0.02293154] [0.60087576 0.88804056 0.77852069 0.46420552 0.48336617 0.36121543\n",
      " 0.53397704 0.06579007 0.04189753 0.04133592]\n",
      "\n",
      "# 27 Gradient out:  [-0.07819914 -0.18199411 -0.32502982 -0.3010346  -0.31293665 -0.31536699\n",
      " -0.1220216  -0.34482256 -0.32818764 -0.22018998]\n",
      "\n",
      "     Weights  out:  [-0.4646638  -0.07380256  0.35525544  0.25518733  0.30119532  0.31137587\n",
      " -0.25183404  0.47098708  0.37100222  0.02293154] [ 5.90053669e-01  8.64000643e-01  7.36295639e-01  4.25039182e-01\n",
      "  4.42683646e-01  3.20223090e-01  5.17561369e-01  2.10337922e-02\n",
      " -7.30732138e-04  1.24450344e-02]\n",
      "\n",
      "# 28 Gradient out:  [-0.13512869 -0.33704511 -0.61569385 -0.56904464 -0.59220021 -0.59692491\n",
      " -0.2201816  -0.654016   -0.6218208  -0.41149842]\n",
      "\n",
      "     Weights  out:  [-0.4646638  -0.07380256  0.35525544  0.25518733  0.30119532  0.31137587\n",
      " -0.25183404  0.47098708  0.37100222  0.02293154] [ 0.57441384  0.82760182  0.67128967  0.36483226  0.38009632  0.25714969\n",
      "  0.49315705 -0.04793072 -0.06636826 -0.03159296]\n",
      "\n",
      "# 29 Gradient out:  [-0.31858389 -0.87557127 -1.63624809 -1.50635895 -1.57070481 -1.58386791\n",
      " -0.55577589 -1.74335841 -1.65337076 -1.07653702]\n",
      "\n",
      "     Weights  out:  [-0.4646638  -0.07380256  0.35525544  0.25518733  0.30119532  0.31137587\n",
      " -0.25183404  0.47098708  0.37100222  0.02293154] [ 0.5473881   0.7601928   0.54815091  0.25102334  0.26165628  0.13776471\n",
      "  0.44912073 -0.17873392 -0.19073242 -0.11389265]\n",
      "\n",
      "# 30 Gradient out:  [0.82786791 1.69391046 2.96170686 2.77410973 2.86804982 2.88696566\n",
      " 1.17151006 3.11764354 2.98610331 2.05607512]\n",
      "\n",
      "     Weights  out:  [-0.4646638  -0.07380256  0.35525544  0.25518733  0.30119532  0.31137587\n",
      " -0.25183404  0.47098708  0.37100222  0.02293154] [ 0.48367133  0.58507855  0.22090129 -0.05024845 -0.05248469 -0.17900887\n",
      "  0.33796555 -0.5274056  -0.52140657 -0.32920005]\n",
      "\n",
      "# 31 Gradient out:  [-0.03663757 -0.0792503  -0.13781407 -0.12794667 -0.13283584 -0.13383534\n",
      " -0.05469788 -0.14599679 -0.13911623 -0.09486144]\n",
      "\n",
      "     Weights  out:  [-0.4646638  -0.07380256  0.35525544  0.25518733  0.30119532  0.31137587\n",
      " -0.25183404  0.47098708  0.37100222  0.02293154] [0.64924491 0.92386064 0.81324266 0.50457349 0.52112528 0.39838426\n",
      " 0.57226756 0.09612311 0.07581409 0.08201497]\n",
      "\n",
      "# 32 Gradient out:  [-0.04682507 -0.10439623 -0.18361505 -0.1702943  -0.1768976  -0.17824684\n",
      " -0.07118391 -0.19463714 -0.18537088 -0.1255313 ]\n",
      "\n",
      "     Weights  out:  [-0.4646638  -0.07380256  0.35525544  0.25518733  0.30119532  0.31137587\n",
      " -0.25183404  0.47098708  0.37100222  0.02293154] [0.6419174  0.90801058 0.78567984 0.47898416 0.49455811 0.37161719\n",
      " 0.56132799 0.06692375 0.04799084 0.06304269]\n",
      "\n",
      "# 33 Gradient out:  [-0.0647457  -0.15026094 -0.26810551 -0.2483372  -0.25814228 -0.26014449\n",
      " -0.10085286 -0.28441639 -0.27070737 -0.18173203]\n",
      "\n",
      "     Weights  out:  [-0.4646638  -0.07380256  0.35525544  0.25518733  0.30119532  0.31137587\n",
      " -0.25183404  0.47098708  0.37100222  0.02293154] [0.63255238 0.88713133 0.74895683 0.4449253  0.45917859 0.33596782\n",
      " 0.54709121 0.02799632 0.01091667 0.03793643]\n",
      "\n",
      "# 34 Gradient out:  [-0.10287411 -0.2527975  -0.45972427 -0.42509678 -0.44228385 -0.4457909\n",
      " -0.16602771 -0.48819183 -0.46427353 -0.30810699]\n",
      "\n",
      "     Weights  out:  [-0.4646638  -0.07380256  0.35525544  0.25518733  0.30119532  0.31137587\n",
      " -0.25183404  0.47098708  0.37100222  0.02293154] [ 0.61960324  0.85707915  0.69533573  0.39525786  0.40755013  0.28393893\n",
      "  0.52692063 -0.02888696 -0.04322481  0.00159002]\n",
      "\n",
      "# 35 Gradient out:  [-0.21373448 -0.57212344 -1.06600205 -0.98306358 -1.02423255 -1.03263363\n",
      " -0.36484874 -1.13402196 -1.07689083 -0.70380644]\n",
      "\n",
      "     Weights  out:  [-0.4646638  -0.07380256  0.35525544  0.25518733  0.30119532  0.31137587\n",
      " -0.25183404  0.47098708  0.37100222  0.02293154] [ 0.59902842  0.80651964  0.60339088  0.3102385   0.31909336  0.19478075\n",
      "  0.49371509 -0.12652532 -0.13607951 -0.06003138]\n",
      "\n",
      "# 36 Gradient out:  [-0.22467759 -0.46199569 -0.64488221 -0.56999279 -0.60419913 -0.6119092\n",
      " -0.37557499 -0.72216673 -0.65641747 -0.47376361]\n",
      "\n",
      "     Weights  out:  [-0.4646638  -0.07380256  0.35525544  0.25518733  0.30119532  0.31137587\n",
      " -0.25183404  0.47098708  0.37100222  0.02293154] [ 0.55628152  0.69209496  0.39019047  0.11362579  0.11424685 -0.01174598\n",
      "  0.42074534 -0.35332971 -0.35145768 -0.20079267]\n",
      "\n",
      "# 37 Gradient out:  [0.64526408 1.50158444 2.81041227 2.63199003 2.72267382 2.74061877\n",
      " 0.96443733 2.95029641 2.83280049 1.88740408]\n",
      "\n",
      "     Weights  out:  [-0.4646638  -0.07380256  0.35525544  0.25518733  0.30119532  0.31137587\n",
      " -0.25183404  0.47098708  0.37100222  0.02293154] [ 5.11346007e-01  5.99695818e-01  2.61214026e-01 -3.72771916e-04\n",
      " -6.59297328e-03 -1.34127819e-01  3.45630347e-01 -4.97763061e-01\n",
      " -4.82741173e-01 -2.95545387e-01]\n",
      "\n",
      "# 38 Gradient out:  [-0.03560838 -0.07539725 -0.13002325 -0.12080337 -0.12537001 -0.12630396\n",
      " -0.0524954  -0.13768206 -0.13124108 -0.08994761]\n",
      "\n",
      "     Weights  out:  [-0.4646638  -0.07380256  0.35525544  0.25518733  0.30119532  0.31137587\n",
      " -0.25183404  0.47098708  0.37100222  0.02293154] [0.64039882 0.90001271 0.82329648 0.52602523 0.53794179 0.41399593\n",
      " 0.53851781 0.09229622 0.08381892 0.08193543]\n",
      "\n",
      "# 39 Gradient out:  [-0.04485986 -0.09774793 -0.17044503 -0.15819917 -0.1642673  -0.16550772\n",
      " -0.06726954 -0.1805958  -0.17206073 -0.11712801]\n",
      "\n",
      "     Weights  out:  [-0.4646638  -0.07380256  0.35525544  0.25518733  0.30119532  0.31137587\n",
      " -0.25183404  0.47098708  0.37100222  0.02293154] [0.63327715 0.88493326 0.79729183 0.50186456 0.51286779 0.38873514\n",
      " 0.52801873 0.06475981 0.05757071 0.06394591]\n",
      "\n",
      "# 40 Gradient out:  [-0.06058043 -0.13706593 -0.24235267 -0.22465918 -0.23343164 -0.23522377\n",
      " -0.09292298 -0.25697921 -0.2446838  -0.16516177]\n",
      "\n",
      "     Weights  out:  [-0.4646638  -0.07380256  0.35525544  0.25518733  0.30119532  0.31137587\n",
      " -0.25183404  0.47098708  0.37100222  0.02293154] [0.62430518 0.86538367 0.76320282 0.47022473 0.48001433 0.3556336\n",
      " 0.51456482 0.02864065 0.02315856 0.04052031]\n",
      "\n",
      "# 41 Gradient out:  [-0.09217138 -0.21988325 -0.39598449 -0.36646998 -0.38111356 -0.38410287\n",
      " -0.14604292 -0.42029596 -0.39986594 -0.2669249 ]\n",
      "\n",
      "     Weights  out:  [-0.4646638  -0.07380256  0.35525544  0.25518733  0.30119532  0.31137587\n",
      " -0.25183404  0.47098708  0.37100222  0.02293154] [ 0.61218909  0.83797048  0.71473229  0.42529289  0.433328    0.30858884\n",
      "  0.49598023 -0.02275519 -0.0257782   0.00748795]\n",
      "\n",
      "# 42 Gradient out:  [-0.17608059 -0.45531374 -0.84053916 -0.7759934  -0.80803573 -0.81457326\n",
      " -0.2937058  -0.89349977 -0.84901302 -0.5581734 ]\n",
      "\n",
      "     Weights  out:  [-0.4646638  -0.07380256  0.35525544  0.25518733  0.30119532  0.31137587\n",
      " -0.25183404  0.47098708  0.37100222  0.02293154] [ 0.59375481  0.79399383  0.63553539  0.35199889  0.35710529  0.23176827\n",
      "  0.46677164 -0.10681439 -0.10575139 -0.04589703]\n",
      "\n",
      "# 43 Gradient out:  [-0.39123616 -1.06184044 -1.93520537 -1.77293167 -1.85245675 -1.86893808\n",
      " -0.69175905 -2.07362552 -1.95709228 -1.28154641]\n",
      "\n",
      "     Weights  out:  [-0.4646638  -0.07380256  0.35525544  0.25518733  0.30119532  0.31137587\n",
      " -0.25183404  0.47098708  0.37100222  0.02293154] [ 0.5585387   0.70293108  0.46742756  0.19680022  0.19549814  0.06885362\n",
      "  0.40803048 -0.28551434 -0.27555399 -0.15753171]\n",
      "\n",
      "# 44 Gradient out:  [0.76378118 1.18154649 1.74137168 1.64464985 1.69172654 1.70151842\n",
      " 0.94955027 1.8312281  1.75481003 1.33106215]\n",
      "\n",
      "     Weights  out:  [-0.4646638  -0.07380256  0.35525544  0.25518733  0.30119532  0.31137587\n",
      " -0.25183404  0.47098708  0.37100222  0.02293154] [ 0.48029146  0.490563    0.08038648 -0.15778612 -0.17499321 -0.304934\n",
      "  0.26967867 -0.70023944 -0.66697245 -0.41384099]\n",
      "\n",
      "# 45 Gradient out:  [-0.37216553 -0.95337475 -1.66792227 -1.52152836 -1.59240197 -1.60730439\n",
      " -0.64779434 -1.79775738 -1.68817784 -1.12184629]\n",
      "\n",
      "     Weights  out:  [-0.4646638  -0.07380256  0.35525544  0.25518733  0.30119532  0.31137587\n",
      " -0.25183404  0.47098708  0.37100222  0.02293154] [ 0.6330477   0.7268723   0.42866082  0.17114385  0.1633521   0.03536969\n",
      "  0.45958873 -0.33399382 -0.31601044 -0.14762856]\n",
      "\n",
      "# 46 Gradient out:  [0.83405226 1.34134666 2.02596064 1.90907299 1.96608729 1.97791746\n",
      " 1.05774337 2.13366003 2.04212187 1.52521695]\n",
      "\n",
      "     Weights  out:  [-0.4646638  -0.07380256  0.35525544  0.25518733  0.30119532  0.31137587\n",
      " -0.25183404  0.47098708  0.37100222  0.02293154] [ 0.55861459  0.53619735  0.09507636 -0.13316182 -0.15512829 -0.28609119\n",
      "  0.33002986 -0.6935453  -0.65364601 -0.37199782]\n",
      "\n",
      "# 47 Gradient out:  [-0.33576989 -0.95428275 -1.79011521 -1.64467634 -1.71654175 -1.73128798\n",
      " -0.60230449 -1.91108936 -1.80939506 -1.17283448]\n",
      "\n",
      "     Weights  out:  [-0.4646638  -0.07380256  0.35525544  0.25518733  0.30119532  0.31137587\n",
      " -0.25183404  0.47098708  0.37100222  0.02293154] [ 0.72542505  0.80446668  0.50026849  0.24865278  0.23808917  0.1094923\n",
      "  0.54157853 -0.26681329 -0.24522163 -0.06695443]\n",
      "\n",
      "# 48 Gradient out:  [0.85118798 1.56734007 2.56646105 2.40531495 2.48473967 2.50102655\n",
      " 1.15424354 2.70906734 2.58821539 1.84268027]\n",
      "\n",
      "     Weights  out:  [-0.4646638  -0.07380256  0.35525544  0.25518733  0.30119532  0.31137587\n",
      " -0.25183404  0.47098708  0.37100222  0.02293154] [ 0.65827107  0.61361013  0.14224545 -0.08028249 -0.10521918 -0.2367653\n",
      "  0.42111764 -0.64903116 -0.60710065 -0.30152133]\n",
      "\n",
      "# 49 Gradient out:  [-0.09680265 -0.26177759 -0.49018702 -0.45216154 -0.47105712 -0.47490787\n",
      " -0.16599784 -0.52127684 -0.49516802 -0.3229623 ]\n",
      "\n",
      "     Weights  out:  [-0.4646638  -0.07380256  0.35525544  0.25518733  0.30119532  0.31137587\n",
      " -0.25183404  0.47098708  0.37100222  0.02293154] [ 0.82850866  0.92707814  0.65553766  0.4007805   0.39172875  0.26344002\n",
      "  0.65196634 -0.10721769 -0.08945757  0.06701473]\n",
      "\n",
      "Lineal error cuadrático medio (ECM): 1.948368957614507\n",
      "\n",
      "# 0 Gradient out:  [-1.32910975 -0.90807561 -1.40521933 -1.00412785 -0.20104671 -1.33299991\n",
      " -0.96415453 -0.31897381 -1.64453254 -1.52597895]\n",
      "\n",
      "     Weights  out:  [ 0.12363414 -0.05463389  0.15822649 -0.01439416 -0.46865126  0.12536714\n",
      " -0.03108566 -0.35977365  0.28304562  0.21718293] [ 0.07807725 -0.18509618 -0.04101315  0.3159141  -0.35800199  0.27801178\n",
      "  0.37047534  0.05581827  0.23510674 -0.02147801]\n",
      "\n",
      "# 1 Gradient out:  [2.68503518 1.95335496 2.80467934 2.12296846 0.94203102 2.691301\n",
      " 2.05198192 1.08813262 3.13938553 2.98121758]\n",
      "\n",
      "     Weights  out:  [ 0.12363414 -0.05463389  0.15822649 -0.01439416 -0.46865126  0.12536714\n",
      " -0.03108566 -0.35977365  0.28304562  0.21718293] [-0.1877447  -0.36671131 -0.32205702  0.11508853 -0.39821133  0.0114118\n",
      "  0.17764443 -0.0079765  -0.09379977 -0.3266738 ]\n",
      "\n",
      "# 2 Gradient out:  [-0.40471702 -0.303111   -0.42264477 -0.32638691 -0.13801967 -0.40563837\n",
      " -0.31668579 -0.16571471 -0.4778173  -0.4506751 ]\n",
      "\n",
      "     Weights  out:  [ 0.12363414 -0.05463389  0.15822649 -0.01439416 -0.46865126  0.12536714\n",
      " -0.03108566 -0.35977365  0.28304562  0.21718293] [ 0.34926233  0.02395969  0.23887885  0.53968223 -0.20980513  0.549672\n",
      "  0.58804081  0.20965003  0.53407734  0.26956972]\n",
      "\n",
      "# 3 Gradient out:  [-0.76674578 -0.56481688 -0.80236137 -0.61107651 -0.23791354 -0.76857643\n",
      " -0.59179594 -0.29228464 -0.91180586 -0.85800955]\n",
      "\n",
      "     Weights  out:  [ 0.12363414 -0.05463389  0.15822649 -0.01439416 -0.46865126  0.12536714\n",
      " -0.03108566 -0.35977365  0.28304562  0.21718293] [ 0.26831893 -0.03666251  0.1543499   0.47440484 -0.23740906  0.46854432\n",
      "  0.52470366  0.17650708  0.43851388  0.1794347 ]\n",
      "\n",
      "# 4 Gradient out:  [-1.50667732 -1.06119446 -1.58622417 -1.16303654 -0.32762603 -1.51075463\n",
      " -1.12062186 -0.44934819 -1.83340064 -1.71145793]\n",
      "\n",
      "     Weights  out:  [ 0.12363414 -0.05463389  0.15822649 -0.01439416 -0.46865126  0.12536714\n",
      " -0.03108566 -0.35977365  0.28304562  0.21718293] [ 0.11496978 -0.14962589 -0.00612238  0.35218954 -0.28499177  0.31482904\n",
      "  0.40634447  0.11805016  0.25615271  0.00783279]\n",
      "\n",
      "# 5 Gradient out:  [2.69217002 1.9633694  2.81145071 2.13229363 0.95306189 2.69841528\n",
      " 2.0615986  1.09980566 3.14565784 2.98760212]\n",
      "\n",
      "     Weights  out:  [ 0.12363414 -0.05463389  0.15822649 -0.01439416 -0.46865126  0.12536714\n",
      " -0.03108566 -0.35977365  0.28304562  0.21718293] [-0.18636569 -0.36186478 -0.32336721  0.11958223 -0.35051698  0.01267811\n",
      "  0.1822201   0.02818052 -0.11052742 -0.3344588 ]\n",
      "\n",
      "# 6 Gradient out:  [-0.39478864 -0.29439229 -0.41249712 -0.31739225 -0.13143689 -0.39569881\n",
      " -0.30780598 -0.1587318  -0.46696628 -0.44017655]\n",
      "\n",
      "     Weights  out:  [ 0.12363414 -0.05463389  0.15822649 -0.01439416 -0.46865126  0.12536714\n",
      " -0.03108566 -0.35977365  0.28304562  0.21718293] [ 0.35206832  0.0308091   0.23892293  0.54604096 -0.1599046   0.55236117\n",
      "  0.59453982  0.24814165  0.51860415  0.26306163]\n",
      "\n",
      "# 7 Gradient out:  [-0.7420834  -0.54532704 -0.77678049 -0.59040296 -0.22693157 -0.74386692\n",
      " -0.57161556 -0.27986618 -0.88337702 -0.83098589]\n",
      "\n",
      "     Weights  out:  [ 0.12363414 -0.05463389  0.15822649 -0.01439416 -0.46865126  0.12536714\n",
      " -0.03108566 -0.35977365  0.28304562  0.21718293] [ 0.27311059 -0.02806936  0.15642351  0.48256251 -0.18619198  0.47322141\n",
      "  0.53297862  0.21639529  0.42521089  0.17502632]\n",
      "\n",
      "# 8 Gradient out:  [-1.52021856 -1.07829636 -1.59904318 -1.17934389 -0.35112129 -1.5242598\n",
      " -1.13725712 -0.47208313 -1.84379832 -1.72306895]\n",
      "\n",
      "     Weights  out:  [ 0.12363414 -0.05463389  0.15822649 -0.01439416 -0.46865126  0.12536714\n",
      " -0.03108566 -0.35977365  0.28304562  0.21718293] [ 0.12469391 -0.13713477  0.00106741  0.36448192 -0.23157829  0.32444802\n",
      "  0.41865551  0.16042205  0.24853549  0.00882914]\n",
      "\n",
      "# 9 Gradient out:  [2.72432484 1.98893009 2.84463028 2.15939481 0.96996944 2.73062443\n",
      " 2.08805326 1.11807847 3.1815613  3.02224123]\n",
      "\n",
      "     Weights  out:  [ 0.12363414 -0.05463389  0.15822649 -0.01439416 -0.46865126  0.12536714\n",
      " -0.03108566 -0.35977365  0.28304562  0.21718293] [-0.1793498  -0.35279404 -0.31874123  0.12861314 -0.30180255  0.01959606\n",
      "  0.19120408  0.06600543 -0.12022418 -0.33578465]\n",
      "\n",
      "# 10 Gradient out:  [-0.35206417 -0.2618691  -0.36796956 -0.2825328  -0.1155545  -0.35288172\n",
      " -0.27392017 -0.14004748 -0.41687748 -0.3928261 ]\n",
      "\n",
      "     Weights  out:  [ 0.12363414 -0.05463389  0.15822649 -0.01439416 -0.46865126  0.12536714\n",
      " -0.03108566 -0.35977365  0.28304562  0.21718293] [ 0.36551517  0.04499198  0.25018483  0.5604921  -0.10780867  0.56572095\n",
      "  0.60881474  0.28962112  0.51608809  0.2686636 ]\n",
      "\n",
      "# 11 Gradient out:  [-0.6297346  -0.46299591 -0.65912969 -0.50119686 -0.1932149  -0.63124568\n",
      " -0.48527461 -0.23810343 -0.74942089 -0.70504517]\n",
      "\n",
      "     Weights  out:  [ 0.12363414 -0.05463389  0.15822649 -0.01439416 -0.46865126  0.12536714\n",
      " -0.03108566 -0.35977365  0.28304562  0.21718293] [ 0.29510233 -0.00738184  0.17659092  0.50398554 -0.13091956  0.4951446\n",
      "  0.5540307   0.26161163  0.43271259  0.19009838]\n",
      "\n",
      "# 12 Gradient out:  [-1.37928339 -0.9900625  -1.44825236 -1.07915788 -0.35651321 -1.38282475\n",
      " -1.04203455 -0.46151961 -1.66100262 -1.55630699]\n",
      "\n",
      "     Weights  out:  [ 0.12363414 -0.05463389  0.15822649 -0.01439416 -0.46865126  0.12536714\n",
      " -0.03108566 -0.35977365  0.28304562  0.21718293] [ 0.16915541 -0.09998102  0.04476498  0.40374617 -0.16956255  0.36889547\n",
      "  0.45697578  0.21399094  0.28282841  0.04908934]\n",
      "\n",
      "# 13 Gradient out:  [2.4216159  1.74613722 2.52922354 1.90332271 0.86793797 2.42728952\n",
      " 1.83745064 0.98544081 2.81916583 2.68454303]\n",
      "\n",
      "     Weights  out:  [ 0.12363414 -0.05463389  0.15822649 -0.01439416 -0.46865126  0.12536714\n",
      " -0.03108566 -0.35977365  0.28304562  0.21718293] [-0.10670127 -0.29799352 -0.24488549  0.18791459 -0.24086519  0.09233052\n",
      "  0.24856887  0.12168702 -0.04937211 -0.26217206]\n",
      "\n",
      "# 14 Gradient out:  [-0.3224616  -0.23942191 -0.33710285 -0.25844678 -0.10476685 -0.32321419\n",
      " -0.25051714 -0.12729866 -0.3821142  -0.359981  ]\n",
      "\n",
      "     Weights  out:  [ 0.12363414 -0.05463389  0.15822649 -0.01439416 -0.46865126  0.12536714\n",
      " -0.03108566 -0.35977365  0.28304562  0.21718293] [ 0.37762191  0.05123392  0.26095921  0.56857913 -0.06727759  0.57778842\n",
      "  0.616059    0.31877518  0.51446105  0.27473655]\n",
      "\n",
      "# 15 Gradient out:  [-0.55517524 -0.40815469 -0.58108918 -0.44183919 -0.17031218 -0.55650742\n",
      " -0.42779924 -0.20990177 -0.66067663 -0.62156283]\n",
      "\n",
      "     Weights  out:  [ 0.12363414 -0.05463389  0.15822649 -0.01439416 -0.46865126  0.12536714\n",
      " -0.03108566 -0.35977365  0.28304562  0.21718293] [ 0.31312959  0.00334954  0.19353864  0.51688978 -0.08823096  0.51314558\n",
      "  0.56595557  0.29331545  0.43803821  0.20274035]\n",
      "\n",
      "# 16 Gradient out:  [-1.19985497 -0.86578004 -1.25890559 -0.94228393 -0.32406984 -1.20288878\n",
      " -0.91040232 -0.41379937 -1.44062375 -1.35127415]\n",
      "\n",
      "     Weights  out:  [ 0.12363414 -0.05463389  0.15822649 -0.01439416 -0.46865126  0.12536714\n",
      " -0.03108566 -0.35977365  0.28304562  0.21718293] [ 0.20209455 -0.0782814   0.07732081  0.42852194 -0.1222934   0.4018441\n",
      "  0.48039572  0.25133509  0.30590289  0.07842778]\n",
      "\n",
      "# 17 Gradient out:  [0.99423552 0.71048364 1.03436813 0.77758655 0.43694527 0.99642093\n",
      " 0.74931037 0.45750608 1.12262587 1.08604932]\n",
      "\n",
      "     Weights  out:  [ 0.12363414 -0.05463389  0.15822649 -0.01439416 -0.46865126  0.12536714\n",
      " -0.03108566 -0.35977365  0.28304562  0.21718293] [-0.03787645 -0.25143741 -0.17446031  0.24006515 -0.18710737  0.16126634\n",
      "  0.29831526  0.16857522  0.01777814 -0.19182705]\n",
      "\n",
      "# 18 Gradient out:  [-1.44830588 -1.03770612 -1.52124124 -1.13165755 -0.36595932 -1.45204871\n",
      " -1.0925164  -0.47778584 -1.74685553 -1.63570716]\n",
      "\n",
      "     Weights  out:  [ 0.12363414 -0.05463389  0.15822649 -0.01439416 -0.46865126  0.12536714\n",
      " -0.03108566 -0.35977365  0.28304562  0.21718293] [ 0.16097066 -0.10934068  0.03241331  0.39558247 -0.09971831  0.36055053\n",
      "  0.44817733  0.26007643  0.24230331  0.02538282]\n",
      "\n",
      "# 19 Gradient out:  [2.67021536 1.93064275 2.78965391 2.10240149 0.93728538 2.67649045\n",
      " 2.03047141 1.07601401 3.11797821 2.96407489]\n",
      "\n",
      "     Weights  out:  [ 0.12363414 -0.05463389  0.15822649 -0.01439416 -0.46865126  0.12536714\n",
      " -0.03108566 -0.35977365  0.28304562  0.21718293] [-0.12869052 -0.3168819  -0.27183493  0.16925096 -0.17291018  0.07014079\n",
      "  0.22967405  0.16451927 -0.10706779 -0.30175862]\n",
      "\n",
      "# 20 Gradient out:  [-0.25924076 -0.19182904 -0.27112285 -0.20727422 -0.08259767 -0.25985157\n",
      " -0.2008365  -0.10086089 -0.30763685 -0.28968509]\n",
      "\n",
      "     Weights  out:  [ 0.12363414 -0.05463389  0.15822649 -0.01439416 -0.46865126  0.12536714\n",
      " -0.03108566 -0.35977365  0.28304562  0.21718293] [0.40535255 0.06924665 0.28609585 0.58973125 0.0145469  0.60543888\n",
      " 0.63576834 0.37972207 0.51652785 0.29105636]\n",
      "\n",
      "# 21 Gradient out:  [-0.40773774 -0.29961516 -0.42678876 -0.32438908 -0.12475531 -0.40871719\n",
      " -0.3140629  -0.15387809 -0.48528292 -0.45653762]\n",
      "\n",
      "     Weights  out:  [ 0.12363414 -0.05463389  0.15822649 -0.01439416 -0.46865126  0.12536714\n",
      " -0.03108566 -0.35977365  0.28304562  0.21718293] [ 0.3535044   0.03088084  0.23187128  0.54827641 -0.00197264  0.55346856\n",
      "  0.59560104  0.35954989  0.45500048  0.23311934]\n",
      "\n",
      "# 22 Gradient out:  [-0.78371638 -0.56940328 -0.82149737 -0.61850348 -0.22315106 -0.78565861\n",
      " -0.59803854 -0.28054309 -0.93748113 -0.88049818]\n",
      "\n",
      "     Weights  out:  [ 0.12363414 -0.05463389  0.15822649 -0.01439416 -0.46865126  0.12536714\n",
      " -0.03108566 -0.35977365  0.28304562  0.21718293] [ 0.27195685 -0.02904219  0.14651353  0.48339859 -0.0269237   0.47172512\n",
      "  0.53278846  0.32877427  0.35794389  0.14181182]\n",
      "\n",
      "# 23 Gradient out:  [-1.54939606 -1.10689298 -1.62907334 -1.20791635 -0.36310278 -1.55347171\n",
      " -1.16586257 -0.4895015  -1.87927705 -1.75529844]\n",
      "\n",
      "     Weights  out:  [ 0.12363414 -0.05463389  0.15822649 -0.01439416 -0.46865126  0.12536714\n",
      " -0.03108566 -0.35977365  0.28304562  0.21718293] [ 0.11521358 -0.14292285 -0.01778595  0.3596979  -0.07155391  0.3145934\n",
      "  0.41318075  0.27266566  0.17044767 -0.03428782]\n",
      "\n",
      "# 24 Gradient out:  [2.5907714  1.91643619 2.70204058 2.07255098 0.95921264 2.59658465\n",
      " 2.00724292 1.10360813 3.01786909 2.86756276]\n",
      "\n",
      "     Weights  out:  [ 0.12363414 -0.05463389  0.15822649 -0.01439416 -0.46865126  0.12536714\n",
      " -0.03108566 -0.35977365  0.28304562  0.21718293] [-0.19466563 -0.36430145 -0.34360061  0.11811463 -0.14417447  0.00389906\n",
      "  0.18000823  0.17476536 -0.20540774 -0.38534751]\n",
      "\n",
      "# 25 Gradient out:  [-0.51910055 -0.37802181 -0.54394888 -0.41034858 -0.15031554 -0.52037821\n",
      " -0.39687404 -0.18809179 -0.62017475 -0.58273301]\n",
      "\n",
      "     Weights  out:  [ 0.12363414 -0.05463389  0.15822649 -0.01439416 -0.46865126  0.12536714\n",
      " -0.03108566 -0.35977365  0.28304562  0.21718293] [0.32348865 0.01898579 0.1968075  0.53262482 0.04766806 0.52321599\n",
      " 0.58145682 0.39548698 0.39816607 0.18816505]\n",
      "\n",
      "# 26 Gradient out:  [-1.10750489 -0.79707881 -1.16234829 -0.86817296 -0.29398717 -1.11032288\n",
      " -0.83854487 -0.37737298 -1.33104589 -1.24810965]\n",
      "\n",
      "     Weights  out:  [ 0.12363414 -0.05463389  0.15822649 -0.01439416 -0.46865126  0.12536714\n",
      " -0.03108566 -0.35977365  0.28304562  0.21718293] [ 0.21966854 -0.05661857  0.08801773  0.45055511  0.01760495  0.41914035\n",
      "  0.50208201  0.35786862  0.27413112  0.07161844]\n",
      "\n",
      "# 27 Gradient out:  [0.07448641 0.02005891 0.07545072 0.03435167 0.09747706 0.07464309\n",
      " 0.02812704 0.06111637 0.04745756 0.06727642]\n",
      "\n",
      "     Weights  out:  [ 0.12363414 -0.05463389  0.15822649 -0.01439416 -0.46865126  0.12536714\n",
      " -0.03108566 -0.35977365  0.28304562  0.21718293] [-0.00183244 -0.21603433 -0.14445193  0.27692052 -0.04119248  0.19707577\n",
      "  0.33437303  0.28239403  0.00792195 -0.17800349]\n",
      "\n",
      "# 28 Gradient out:  [-0.22873273 -0.19606905 -0.24235492 -0.20189129  0.00747692 -0.22933542\n",
      " -0.19970824 -0.04776142 -0.3123844  -0.27244932]\n",
      "\n",
      "     Weights  out:  [ 0.12363414 -0.05463389  0.15822649 -0.01439416 -0.46865126  0.12536714\n",
      " -0.03108566 -0.35977365  0.28304562  0.21718293] [ 0.01306484 -0.21202255 -0.12936179  0.28379085 -0.02169707  0.21200439\n",
      "  0.33999844  0.2946173   0.01741346 -0.1645482 ]\n",
      "\n",
      "# 29 Gradient out:  [1.01909506 0.68760538 1.06678594 0.76582076 0.35831119 1.02168017\n",
      " 0.73288784 0.38345562 1.17458361 1.12919479]\n",
      "\n",
      "     Weights  out:  [ 0.12363414 -0.05463389  0.15822649 -0.01439416 -0.46865126  0.12536714\n",
      " -0.03108566 -0.35977365  0.28304562  0.21718293] [-0.03268171 -0.25123636 -0.17783277  0.24341259 -0.02020168  0.1661373\n",
      "  0.30005679  0.28506502 -0.04506342 -0.21903807]\n",
      "\n",
      "# 30 Gradient out:  [-1.45781188 -1.044828   -1.53130423 -1.13929733 -0.36597292 -1.4615816\n",
      " -1.09994441 -0.4796664  -1.75918502 -1.64680558]\n",
      "\n",
      "     Weights  out:  [ 0.12363414 -0.05463389  0.15822649 -0.01439416 -0.46865126  0.12536714\n",
      " -0.03108566 -0.35977365  0.28304562  0.21718293] [ 0.17113731 -0.11371528  0.03552441  0.39657674  0.05146055  0.37047334\n",
      "  0.44663436  0.36175614  0.1898533   0.00680089]\n",
      "\n",
      "# 31 Gradient out:  [2.63296662 1.89707813 2.75202952 2.06793519 0.90410697 2.63921898\n",
      " 1.99638936 1.04368329 3.08021207 2.92617611]\n",
      "\n",
      "     Weights  out:  [ 0.12363414 -0.05463389  0.15822649 -0.01439416 -0.46865126  0.12536714\n",
      " -0.03108566 -0.35977365  0.28304562  0.21718293] [-0.12042507 -0.32268089 -0.27073643  0.16871728 -0.02173403  0.07815702\n",
      "  0.22664548  0.26582286 -0.1619837  -0.32256022]\n",
      "\n",
      "# 32 Gradient out:  [-0.27850176 -0.2033137  -0.29174015 -0.22054339 -0.08193261 -0.2791825\n",
      " -0.21336148 -0.10210905 -0.33234824 -0.31240066]\n",
      "\n",
      "     Weights  out:  [ 0.12363414 -0.05463389  0.15822649 -0.01439416 -0.46865126  0.12536714\n",
      " -0.03108566 -0.35977365  0.28304562  0.21718293] [0.40616825 0.05673474 0.27966947 0.58230432 0.15908736 0.60600081\n",
      " 0.62592335 0.47455952 0.45405871 0.262675  ]\n",
      "\n",
      "# 33 Gradient out:  [-0.45220833 -0.32808571 -0.4740604  -0.35652918 -0.12796777 -0.45333205\n",
      " -0.344673   -0.1611262  -0.54105504 -0.50815594]\n",
      "\n",
      "     Weights  out:  [ 0.12363414 -0.05463389  0.15822649 -0.01439416 -0.46865126  0.12536714\n",
      " -0.03108566 -0.35977365  0.28304562  0.21718293] [0.3504679  0.016072   0.22132144 0.53819564 0.14270084 0.55016432\n",
      " 0.58325106 0.45413771 0.38758906 0.20019487]\n",
      "\n",
      "# 34 Gradient out:  [-0.91497029 -0.6579739  -0.96030037 -0.71684731 -0.24270584 -0.91730035\n",
      " -0.69230981 -0.31142099 -1.09949135 -1.03110655]\n",
      "\n",
      "     Weights  out:  [ 0.12363414 -0.05463389  0.15822649 -0.01439416 -0.46865126  0.12536714\n",
      " -0.03108566 -0.35977365  0.28304562  0.21718293] [ 0.26002623 -0.04954514  0.12650936  0.4668898   0.11710729  0.45949791\n",
      "  0.51431646  0.42191247  0.27937805  0.09856368]\n",
      "\n",
      "# 35 Gradient out:  [-1.26230763 -0.92491405 -1.32617004 -1.00128608 -0.2958908  -1.26553629\n",
      " -0.96959021 -0.41211496 -1.53791009 -1.43080985]\n",
      "\n",
      "     Weights  out:  [ 0.12363414 -0.05463389  0.15822649 -0.01439416 -0.46865126  0.12536714\n",
      " -0.03108566 -0.35977365  0.28304562  0.21718293] [ 0.07703218 -0.18113992 -0.06555071  0.32352034  0.06856612  0.27603783\n",
      "  0.37585449  0.35962827  0.05947978 -0.10765763]\n",
      "\n",
      "# 36 Gradient out:  [2.62424969 1.94446821 2.73652682 2.1018226  0.97568185 2.63011398\n",
      " 2.03599884 1.12300764 3.05583161 2.90371696]\n",
      "\n",
      "     Weights  out:  [ 0.12363414 -0.05463389  0.15822649 -0.01439416 -0.46865126  0.12536714\n",
      " -0.03108566 -0.35977365  0.28304562  0.21718293] [-0.17542935 -0.36612273 -0.33078472  0.12326313  0.00938796  0.02293058\n",
      "  0.18193645  0.27720528 -0.24810223 -0.3938196 ]\n",
      "\n",
      "# 37 Gradient out:  [-0.45598548 -0.32913372 -0.47830944 -0.35820425 -0.1248907  -0.45713359\n",
      " -0.34608647 -0.15865954 -0.54670561 -0.51312876]\n",
      "\n",
      "     Weights  out:  [ 0.12363414 -0.05463389  0.15822649 -0.01439416 -0.46865126  0.12536714\n",
      " -0.03108566 -0.35977365  0.28304562  0.21718293] [0.34942059 0.02277091 0.21652065 0.54362765 0.20452433 0.54895337\n",
      " 0.58913622 0.50180681 0.36306409 0.18692379]\n",
      "\n",
      "# 38 Gradient out:  [-0.92713642 -0.66439116 -0.97348532 -0.72458035 -0.23984752 -0.92951881\n",
      " -0.69949465 -0.31006582 -1.11580662 -1.04588517]\n",
      "\n",
      "     Weights  out:  [ 0.12363414 -0.05463389  0.15822649 -0.01439416 -0.46865126  0.12536714\n",
      " -0.03108566 -0.35977365  0.28304562  0.21718293] [ 0.25822349 -0.04305584  0.12085876  0.47198679  0.17954619  0.45752666\n",
      "  0.51991892  0.4700749   0.25372297  0.08429804]\n",
      "\n",
      "# 39 Gradient out:  [-1.20935038 -0.89871712 -1.26908869 -0.96883495 -0.29965071 -1.21235948\n",
      " -0.93976366 -0.41343111 -1.47051891 -1.36799653]\n",
      "\n",
      "     Weights  out:  [ 0.12363414 -0.05463389  0.15822649 -0.01439416 -0.46865126  0.12536714\n",
      " -0.03108566 -0.35977365  0.28304562  0.21718293] [ 0.07279621 -0.17593407 -0.0738383   0.32707073  0.13157669  0.27162289\n",
      "  0.38001999  0.40806174  0.03056164 -0.124879  ]\n",
      "\n",
      "# 40 Gradient out:  [2.63354134 1.94865772 2.74662734 2.10720063 0.97289831 2.6394483\n",
      " 2.04087856 1.12135826 3.06814178 2.91498875]\n",
      "\n",
      "     Weights  out:  [ 0.12363414 -0.05463389  0.15822649 -0.01439416 -0.46865126  0.12536714\n",
      " -0.03108566 -0.35977365  0.28304562  0.21718293] [-0.16907387 -0.35567749 -0.32765604  0.13330374  0.07164654  0.029151\n",
      "  0.19206726  0.32537551 -0.26354214 -0.3984783 ]\n",
      "\n",
      "# 41 Gradient out:  [-0.42735082 -0.3072672  -0.44847492 -0.33478843 -0.11415973 -0.42843733\n",
      " -0.32331621 -0.14603196 -0.5131543  -0.48141113]\n",
      "\n",
      "     Weights  out:  [ 0.12363414 -0.05463389  0.15822649 -0.01439416 -0.46865126  0.12536714\n",
      " -0.03108566 -0.35977365  0.28304562  0.21718293] [0.3576344  0.03405405 0.22166943 0.55474386 0.26622621 0.55704066\n",
      " 0.60024298 0.54964717 0.35008622 0.18451945]\n",
      "\n",
      "# 42 Gradient out:  [-0.84460172 -0.60369499 -0.88706789 -0.65888779 -0.21505559 -0.84678491\n",
      " -0.63588357 -0.27923837 -1.01735198 -0.9533674 ]\n",
      "\n",
      "     Weights  out:  [ 0.12363414 -0.05463389  0.15822649 -0.01439416 -0.46865126  0.12536714\n",
      " -0.03108566 -0.35977365  0.28304562  0.21718293] [ 0.27216424 -0.02739939  0.13197444  0.48778617  0.24339426  0.47135319\n",
      "  0.53557973  0.52044077  0.24745535  0.08823722]\n",
      "\n",
      "# 43 Gradient out:  [-1.47974955 -1.08244371 -1.55325494 -1.17273758 -0.37269794 -1.4834852\n",
      " -1.13521022 -0.50053579 -1.79147301 -1.67195548]\n",
      "\n",
      "     Weights  out:  [ 0.12363414 -0.05463389  0.15822649 -0.01439416 -0.46865126  0.12536714\n",
      " -0.03108566 -0.35977365  0.28304562  0.21718293] [ 0.10324389 -0.14813839 -0.04543914  0.35600862  0.20038314  0.30199621\n",
      "  0.40840302  0.4645931   0.04398496 -0.10243626]\n",
      "\n",
      "# 44 Gradient out:  [2.48639707 1.86644745 2.58959729 2.00978729 0.96259061 2.49177613\n",
      " 1.94984931 1.10484581 2.88672834 2.74434244]\n",
      "\n",
      "     Weights  out:  [ 0.12363414 -0.05463389  0.15822649 -0.01439416 -0.46865126  0.12536714\n",
      " -0.03108566 -0.35977365  0.28304562  0.21718293] [-0.19270602 -0.36462713 -0.35609012  0.1214611   0.12584355  0.00529917\n",
      "  0.18136097  0.36448594 -0.31430964 -0.43682735]\n",
      "\n",
      "# 45 Gradient out:  [-0.65441012 -0.4665595  -0.68747376 -0.50960744 -0.1644875  -0.65611055\n",
      " -0.49166362 -0.21422985 -0.78872928 -0.7390371 ]\n",
      "\n",
      "     Weights  out:  [ 0.12363414 -0.05463389  0.15822649 -0.01439416 -0.46865126  0.12536714\n",
      " -0.03108566 -0.35977365  0.28304562  0.21718293] [0.3045734  0.00866236 0.16182933 0.52341856 0.31836168 0.50365439\n",
      " 0.57133084 0.5854551  0.26303602 0.11204114]\n",
      "\n",
      "# 46 Gradient out:  [-1.44914781 -1.03728719 -1.52270879 -1.1314437  -0.35413599 -1.45291767\n",
      " -1.0922292  -0.46976857 -1.75185908 -1.63863523]\n",
      "\n",
      "     Weights  out:  [ 0.12363414 -0.05463389  0.15822649 -0.01439416 -0.46865126  0.12536714\n",
      " -0.03108566 -0.35977365  0.28304562  0.21718293] [ 0.17369137 -0.08464954  0.02433458  0.42149707  0.28546418  0.37243228\n",
      "  0.47299811  0.54260913  0.10529017 -0.03576628]\n",
      "\n",
      "# 47 Gradient out:  [2.46106636 1.74768235 2.57646944 1.91331742 0.78603066 2.46712687\n",
      " 1.84395796 0.92087236 2.8944077  2.74522311]\n",
      "\n",
      "     Weights  out:  [ 0.12363414 -0.05463389  0.15822649 -0.01439416 -0.46865126  0.12536714\n",
      " -0.03108566 -0.35977365  0.28304562  0.21718293] [-0.11613819 -0.29210698 -0.28020718  0.19520833  0.21463698  0.08184875\n",
      "  0.25455227  0.44865542 -0.24508165 -0.36349333]\n",
      "\n",
      "# 48 Gradient out:  [-0.36698024 -0.26222796 -0.3853933  -0.28623834 -0.09414747 -0.3679275\n",
      " -0.27622922 -0.12180432 -0.44170774 -0.41408399]\n",
      "\n",
      "     Weights  out:  [ 0.12363414 -0.05463389  0.15822649 -0.01439416 -0.46865126  0.12536714\n",
      " -0.03108566 -0.35977365  0.28304562  0.21718293] [0.37607508 0.05742949 0.23508671 0.57787181 0.37184311 0.57527412\n",
      " 0.62334386 0.63282989 0.33379989 0.18555129]\n",
      "\n",
      "# 49 Gradient out:  [-0.67603684 -0.48095471 -0.71037477 -0.52565941 -0.16729824 -0.67780279\n",
      " -0.50702507 -0.21892021 -0.81552831 -0.76392508]\n",
      "\n",
      "     Weights  out:  [ 0.12363414 -0.05463389  0.15822649 -0.01439416 -0.46865126  0.12536714\n",
      " -0.03108566 -0.35977365  0.28304562  0.21718293] [0.30267903 0.0049839  0.15800805 0.52062415 0.35301362 0.50168862\n",
      " 0.56809802 0.60846903 0.24545835 0.10273449]\n",
      "\n",
      "Tanh error cuadrático medio (ECM): 0.7833249381852975\n",
      "\n",
      "# 0 Gradient out:  [0.7864102  1.26924277 0.69839068 0.70264155 1.62121312 0.86167075\n",
      " 0.867591   1.68425358 1.27013423 1.71528526]\n",
      "\n",
      "     Weights  out:  [-0.29839049  0.02607142 -0.48162939 -0.47073439  0.26699727 -0.2103273\n",
      " -0.2049056   0.36918613  0.02650501  0.43941451] [-0.0428968   0.4653252  -0.43083475 -0.10172603 -0.00512033 -0.29888473\n",
      "  0.09801028  0.148542    0.18092845 -0.48369192]\n",
      "\n",
      "# 1 Gradient out:  [-0.47815115 -0.99933597 -0.31915334 -0.32605858 -1.3863556  -0.59221738\n",
      " -0.60008475 -1.49710275 -1.00013616 -1.55467357]\n",
      "\n",
      "     Weights  out:  [-0.29839049  0.02607142 -0.48162939 -0.47073439  0.26699727 -0.2103273\n",
      " -0.2049056   0.36918613  0.02650501  0.43941451] [ 0.11438524  0.71917375 -0.29115661  0.03880229  0.3191223  -0.12655058\n",
      "  0.27152849  0.48539272  0.43495529 -0.14063487]\n",
      "\n",
      "# 2 Gradient out:  [0.27143257 0.25595464 0.32063712 0.31883512 0.2394827  0.24510477\n",
      " 0.24383763 0.20641233 0.25604427 0.18797986]\n",
      "\n",
      "     Weights  out:  [-0.29839049  0.02607142 -0.48162939 -0.47073439  0.26699727 -0.2103273\n",
      " -0.2049056   0.36918613  0.02650501  0.43941451] [ 0.01875501  0.51930656 -0.35498728 -0.02640943  0.04185118 -0.24499406\n",
      "  0.15151153  0.18597217  0.23492806 -0.45156958]\n",
      "\n",
      "# 3 Gradient out:  [-0.20999899 -0.64546192 -0.0468154  -0.05372244 -0.97210108 -0.32060043\n",
      " -0.32785994 -1.08463363 -0.64605512 -1.1441759 ]\n",
      "\n",
      "     Weights  out:  [-0.29839049  0.02607142 -0.48162939 -0.47073439  0.26699727 -0.2103273\n",
      " -0.2049056   0.36918613  0.02650501  0.43941451] [ 0.07304153  0.57049749 -0.29085986  0.03735759  0.08974772 -0.1959731\n",
      "  0.20027906  0.22725464  0.28613691 -0.41397361]\n",
      "\n",
      "# 4 Gradient out:  [1.09785786 2.13150389 0.86441504 0.87475941 2.89111946 1.28509135\n",
      " 1.29897132 3.05870361 2.13328081 3.14174854]\n",
      "\n",
      "     Weights  out:  [-0.29839049  0.02607142 -0.48162939 -0.47073439  0.26699727 -0.2103273\n",
      " -0.2049056   0.36918613  0.02650501  0.43941451] [ 0.03104173  0.4414051  -0.30022294  0.0266131  -0.1046725  -0.26009319\n",
      "  0.13470707  0.01032791  0.15692589 -0.64280879]\n",
      "\n",
      "# 5 Gradient out:  [-0.19380805 -0.38921751 -0.13415319 -0.13676931 -0.53425842 -0.23640276\n",
      " -0.23934288 -0.57564016 -0.38951853 -0.59726119]\n",
      "\n",
      "     Weights  out:  [-0.29839049  0.02607142 -0.48162939 -0.47073439  0.26699727 -0.2103273\n",
      " -0.2049056   0.36918613  0.02650501  0.43941451] [ 0.2506133   0.86770588 -0.12733993  0.20156499  0.4735514  -0.00307492\n",
      "  0.39450134  0.62206863  0.58358205 -0.01445908]\n",
      "\n",
      "# 6 Gradient out:  [-0.35456581 -0.73918045 -0.23724428 -0.2423656  -1.02472012 -0.43854865\n",
      " -0.44434414 -1.10627236 -0.73977204 -1.14877171]\n",
      "\n",
      "     Weights  out:  [-0.29839049  0.02607142 -0.48162939 -0.47073439  0.26699727 -0.2103273\n",
      " -0.2049056   0.36918613  0.02650501  0.43941451] [ 0.21185169  0.78986238 -0.15417057  0.17421113  0.36669971 -0.05035547\n",
      "  0.34663276  0.5069406   0.50567835 -0.13391132]\n",
      "\n",
      "# 7 Gradient out:  [-0.56266769 -1.27404267 -0.33081372 -0.34086221 -1.80372815 -0.72535661\n",
      " -0.73640461 -1.96424004 -1.27510071 -2.04846306]\n",
      "\n",
      "     Weights  out:  [-0.29839049  0.02607142 -0.48162939 -0.47073439  0.26699727 -0.2103273\n",
      " -0.2049056   0.36918613  0.02650501  0.43941451] [ 0.14093853  0.64202629 -0.20161942  0.12573801  0.16175569 -0.1380652\n",
      "  0.25776393  0.28568613  0.35772394 -0.36366566]\n",
      "\n",
      "# 8 Gradient out:  [1.24101722 2.34449932 0.96261602 0.97515021 3.15746163 1.45256054\n",
      " 1.46787049 3.35297179 2.34634136 3.4529571 ]\n",
      "\n",
      "     Weights  out:  [-0.29839049  0.02607142 -0.48162939 -0.47073439  0.26699727 -0.2103273\n",
      " -0.2049056   0.36918613  0.02650501  0.43941451] [ 0.02840499  0.38721776 -0.26778217  0.05756556 -0.19898994 -0.28313652\n",
      "  0.11048301 -0.10716188  0.1027038  -0.77335828]\n",
      "\n",
      "# 9 Gradient out:  [-0.22690591 -0.47381836 -0.15186863 -0.15514843 -0.65709167 -0.28066249\n",
      " -0.28437582 -0.70924578 -0.47419893 -0.73642529]\n",
      "\n",
      "     Weights  out:  [-0.29839049  0.02607142 -0.48162939 -0.47073439  0.26699727 -0.2103273\n",
      " -0.2049056   0.36918613  0.02650501  0.43941451] [ 0.27660843  0.85611762 -0.07525896  0.25259561  0.43250238  0.00737559\n",
      "  0.40405711  0.56343248  0.57197207 -0.08276685]\n",
      "\n",
      "# 10 Gradient out:  [-0.45108672 -0.97485224 -0.29048267 -0.29747956 -1.36382326 -0.56595709\n",
      " -0.573873   -1.47549673 -0.97565529 -1.53367809]\n",
      "\n",
      "     Weights  out:  [-0.29839049  0.02607142 -0.48162939 -0.47073439  0.26699727 -0.2103273\n",
      " -0.2049056   0.36918613  0.02650501  0.43941451] [ 0.23122725  0.76135395 -0.10563269  0.22156592  0.30108405 -0.04875691\n",
      "  0.34718195  0.42158332  0.47713229 -0.23005191]\n",
      "\n",
      "# 11 Gradient out:  [-0.05099962 -0.09166104  0.02763101  0.02402609 -0.12732448 -0.08876221\n",
      " -0.09062441 -0.17573014 -0.09158505 -0.20567688]\n",
      "\n",
      "     Weights  out:  [-0.29839049  0.02607142 -0.48162939 -0.47073439  0.26699727 -0.2103273\n",
      " -0.2049056   0.36918613  0.02650501  0.43941451] [ 0.14100991  0.5663835  -0.16372922  0.16207001  0.0283194  -0.16194833\n",
      "  0.23240735  0.12648398  0.28200123 -0.53678753]\n",
      "\n",
      "# 12 Gradient out:  [0.17906538 0.41256774 0.18545415 0.18498273 0.57948802 0.19614446\n",
      " 0.1981958  0.58231826 0.41308955 0.57829443]\n",
      "\n",
      "     Weights  out:  [-0.29839049  0.02607142 -0.48162939 -0.47073439  0.26699727 -0.2103273\n",
      " -0.2049056   0.36918613  0.02650501  0.43941451] [ 0.13080998  0.54805129 -0.15820302  0.16687523  0.0028545  -0.17970077\n",
      "  0.21428247  0.09133795  0.26368422 -0.57792291]\n",
      "\n",
      "# 13 Gradient out:  [-0.49244184 -1.0666239  -0.2825133  -0.29176436 -1.49580661 -0.63304458\n",
      " -0.64235646 -1.6384859  -1.06743392 -1.71529254]\n",
      "\n",
      "     Weights  out:  [-0.29839049  0.02607142 -0.48162939 -0.47073439  0.26699727 -0.2103273\n",
      " -0.2049056   0.36918613  0.02650501  0.43941451] [ 0.16662306  0.63056484 -0.12111219  0.20387177  0.1187521  -0.14047188\n",
      "  0.25392163  0.2078016   0.34630213 -0.46226402]\n",
      "\n",
      "# 14 Gradient out:  [1.23050562 2.34180303 0.94993858 0.96260505 3.16044815 1.44337542\n",
      " 1.45878358 3.35723449 2.34365916 3.4580292 ]\n",
      "\n",
      "     Weights  out:  [-0.29839049  0.02607142 -0.48162939 -0.47073439  0.26699727 -0.2103273\n",
      " -0.2049056   0.36918613  0.02650501  0.43941451] [ 0.06813469  0.41724006 -0.17761485  0.1455189  -0.18040922 -0.26708079\n",
      "  0.12545033 -0.11989558  0.13281534 -0.80532253]\n",
      "\n",
      "# 15 Gradient out:  [-0.20089582 -0.43111625 -0.13121703 -0.13425706 -0.60199281 -0.25093518\n",
      " -0.25439445 -0.65048373 -0.43147145 -0.67571139]\n",
      "\n",
      "     Weights  out:  [-0.29839049  0.02607142 -0.48162939 -0.47073439  0.26699727 -0.2103273\n",
      " -0.2049056   0.36918613  0.02650501  0.43941451] [ 0.31423582  0.88560066  0.01237286  0.33803991  0.45168041  0.02159429\n",
      "  0.41720705  0.55155132  0.60154718 -0.11371669]\n",
      "\n",
      "# 16 Gradient out:  [-0.39368276 -0.86371709 -0.2500558  -0.25631407 -1.21274255 -0.4965404\n",
      " -0.5036344  -1.31264705 -0.86443885 -1.36466776]\n",
      "\n",
      "     Weights  out:  [-0.29839049  0.02607142 -0.48162939 -0.47073439  0.26699727 -0.2103273\n",
      " -0.2049056   0.36918613  0.02650501  0.43941451] [ 0.27405665  0.79937741 -0.01387054  0.3111885   0.33128185 -0.02859274\n",
      "  0.36632816  0.42145457  0.51525289 -0.24885897]\n",
      "\n",
      "# 17 Gradient out:  [-0.43362553 -0.86364703 -0.25103256 -0.25926495 -1.18680831 -0.54899605\n",
      " -0.55639411 -1.30804268 -0.86420632 -1.37542147]\n",
      "\n",
      "     Weights  out:  [-0.29839049  0.02607142 -0.48162939 -0.47073439  0.26699727 -0.2103273\n",
      " -0.2049056   0.36918613  0.02650501  0.43941451] [ 0.1953201   0.626634   -0.0638817   0.25992569  0.08873334 -0.12790082\n",
      "  0.26560128  0.15892516  0.34236512 -0.52179252]\n",
      "\n",
      "# 18 Gradient out:  [1.13711742 2.25277629 0.86589786 0.87803461 3.07404538 1.34700885\n",
      " 1.36232124 3.26596775 2.25465728 3.36306367]\n",
      "\n",
      "     Weights  out:  [-0.29839049  0.02607142 -0.48162939 -0.47073439  0.26699727 -0.2103273\n",
      " -0.2049056   0.36918613  0.02650501  0.43941451] [ 0.108595    0.45390459 -0.11408821  0.20807269 -0.14862832 -0.23770003\n",
      "  0.15432246 -0.10268337  0.16952385 -0.79687681]\n",
      "\n",
      "# 19 Gradient out:  [-0.18278848 -0.39756385 -0.11794279 -0.12076912 -0.55697176 -0.22942232\n",
      " -0.23264769 -0.60213139 -0.39789542 -0.62560337]\n",
      "\n",
      "     Weights  out:  [-0.29839049  0.02607142 -0.48162939 -0.47073439  0.26699727 -0.2103273\n",
      " -0.2049056   0.36918613  0.02650501  0.43941451] [ 0.33601848  0.90445985  0.05909136  0.38367962  0.46618075  0.03170173\n",
      "  0.42678671  0.55051018  0.62045531 -0.12426408]\n",
      "\n",
      "# 20 Gradient out:  [-0.34900199 -0.77275947 -0.21999158 -0.22561032 -1.0873888  -0.44153923\n",
      " -0.44792664 -1.17718217 -0.77341108 -1.22389668]\n",
      "\n",
      "     Weights  out:  [-0.29839049  0.02607142 -0.48162939 -0.47073439  0.26699727 -0.2103273\n",
      " -0.2049056   0.36918613  0.02650501  0.43941451] [ 0.29946078  0.82494708  0.0355028   0.35952579  0.3547864  -0.01418273\n",
      "  0.38025717  0.4300839   0.54087622 -0.24938475]\n",
      "\n",
      "# 21 Gradient out:  [-0.58207784 -1.21441433 -0.35381686 -0.3639927  -1.6864851  -0.73470961\n",
      " -0.74485838 -1.84104295 -1.21531786 -1.92460068]\n",
      "\n",
      "     Weights  out:  [-0.29839049  0.02607142 -0.48162939 -0.47073439  0.26699727 -0.2103273\n",
      " -0.2049056   0.36918613  0.02650501  0.43941451] [ 0.22966039  0.67039518 -0.00849551  0.31440373  0.13730864 -0.10249057\n",
      "  0.29067184  0.19464746  0.38619401 -0.49416409]\n",
      "\n",
      "# 22 Gradient out:  [1.20600398 2.26471955 0.92883875 0.94151171 3.0450337  1.41197335\n",
      " 1.42677671 3.23745509 2.26647371 3.3373946 ]\n",
      "\n",
      "     Weights  out:  [-0.29839049  0.02607142 -0.48162939 -0.47073439  0.26699727 -0.2103273\n",
      " -0.2049056   0.36918613  0.02650501  0.43941451] [ 0.11324482  0.42751232 -0.07925889  0.24160519 -0.19998838 -0.2494325\n",
      "  0.14170016 -0.17356113  0.14313044 -0.87908423]\n",
      "\n",
      "# 23 Gradient out:  [-0.24189198 -0.54304606 -0.15103481 -0.15498566 -0.76659033 -0.30732865\n",
      " -0.31185432 -0.82993515 -0.54351067 -0.86281232]\n",
      "\n",
      "     Weights  out:  [-0.29839049  0.02607142 -0.48162939 -0.47073439  0.26699727 -0.2103273\n",
      " -0.2049056   0.36918613  0.02650501  0.43941451] [ 0.35444562  0.88045623  0.10650886  0.42990753  0.40901836  0.03296217\n",
      "  0.42705551  0.47392989  0.59642518 -0.21160531]\n",
      "\n",
      "# 24 Gradient out:  [-0.51532261 -1.15189267 -0.31536654 -0.32411554 -1.62498535 -0.65685376\n",
      " -0.66655648 -1.76342287 -1.15285954 -1.83597993]\n",
      "\n",
      "     Weights  out:  [-0.29839049  0.02607142 -0.48162939 -0.47073439  0.26699727 -0.2103273\n",
      " -0.2049056   0.36918613  0.02650501  0.43941451] [ 0.30606722  0.77184702  0.0763019   0.3989104   0.25570029 -0.02850356\n",
      "  0.36468464  0.30794286  0.48772305 -0.38416777]\n",
      "\n",
      "# 25 Gradient out:  [0.50891572 1.30713517 0.36511548 0.3710197  1.89186675 0.64067939\n",
      " 0.65087969 2.00202282 1.30856588 2.05180862]\n",
      "\n",
      "     Weights  out:  [-0.29839049  0.02607142 -0.48162939 -0.47073439  0.26699727 -0.2103273\n",
      " -0.2049056   0.36918613  0.02650501  0.43941451] [ 0.2030027   0.54146848  0.01322859  0.33408729 -0.06929678 -0.15987431\n",
      "  0.23137335 -0.04474171  0.25715114 -0.75136376]\n",
      "\n",
      "# 26 Gradient out:  [-0.44073756 -0.98990403 -0.27142412 -0.27880801 -1.39781822 -0.56156636\n",
      " -0.56988324 -1.51543905 -0.99074412 -1.57679602]\n",
      "\n",
      "     Weights  out:  [-0.29839049  0.02607142 -0.48162939 -0.47073439  0.26699727 -0.2103273\n",
      " -0.2049056   0.36918613  0.02650501  0.43941451] [ 0.30478584  0.80289552  0.08625169  0.40829123  0.30907657 -0.03173843\n",
      "  0.36154928  0.35566285  0.51886431 -0.34100203]\n",
      "\n",
      "# 27 Gradient out:  [-0.11358096  0.00341593 -0.05880765 -0.06178827  0.08380489 -0.12475211\n",
      " -0.1245455   0.05662895  0.00376936  0.03449966]\n",
      "\n",
      "     Weights  out:  [-0.29839049  0.02607142 -0.48162939 -0.47073439  0.26699727 -0.2103273\n",
      " -0.2049056   0.36918613  0.02650501  0.43941451] [ 0.21663833  0.60491471  0.03196686  0.35252963  0.02951293 -0.14405171\n",
      "  0.24757264  0.05257504  0.32071549 -0.65636124]\n",
      "\n",
      "# 28 Gradient out:  [-0.10447301 -0.01304619 -0.04616953 -0.04922982  0.04853116 -0.11985566\n",
      " -0.11998076  0.01807595 -0.0127382  -0.00518587]\n",
      "\n",
      "     Weights  out:  [-0.29839049  0.02607142 -0.48162939 -0.47073439  0.26699727 -0.2103273\n",
      " -0.2049056   0.36918613  0.02650501  0.43941451] [ 0.19392214  0.6055979   0.02020533  0.34017197  0.04627391 -0.16900213\n",
      "  0.22266354  0.06390083  0.32146936 -0.6494613 ]\n",
      "\n",
      "# 29 Gradient out:  [-0.06008189  0.05540506 -0.0108945  -0.0134969   0.13483607 -0.06986036\n",
      " -0.06961266  0.11018271  0.05574878  0.0903072 ]\n",
      "\n",
      "     Weights  out:  [-0.29839049  0.02607142 -0.48162939 -0.47073439  0.26699727 -0.2103273\n",
      " -0.2049056   0.36918613  0.02650501  0.43941451] [ 0.17302754  0.60298866  0.01097143  0.33032601  0.05598014 -0.19297326\n",
      "  0.19866738  0.06751602  0.31892172 -0.65049848]\n",
      "\n",
      "# 30 Gradient out:  [-0.14641136 -0.16669481 -0.0638453  -0.06784341 -0.18751768 -0.18265278\n",
      " -0.18431716 -0.23640552 -0.16657485 -0.2681603 ]\n",
      "\n",
      "     Weights  out:  [-0.29839049  0.02607142 -0.48162939 -0.47073439  0.26699727 -0.2103273\n",
      " -0.2049056   0.36918613  0.02650501  0.43941451] [ 0.16101116  0.61406967  0.00879253  0.32762663  0.08294735 -0.20694533\n",
      "  0.18474485  0.08955256  0.33007148 -0.63243704]\n",
      "\n",
      "# 31 Gradient out:  [0.22538711 0.62670504 0.19232882 0.19342603 0.91759746 0.27494445\n",
      " 0.27935567 0.9497779  0.62750401 0.95968273]\n",
      "\n",
      "     Weights  out:  [-0.29839049  0.02607142 -0.48162939 -0.47073439  0.26699727 -0.2103273\n",
      " -0.2049056   0.36918613  0.02650501  0.43941451] [ 0.13172889  0.58073071 -0.00397653  0.31405795  0.04544381 -0.24347589\n",
      "  0.14788142  0.04227146  0.29675651 -0.6860691 ]\n",
      "\n",
      "# 32 Gradient out:  [-0.60490522 -1.30965493 -0.3672243  -0.37768728 -1.83469064 -0.7685176\n",
      " -0.77955644 -1.9975946  -1.31069237 -2.08421174]\n",
      "\n",
      "     Weights  out:  [-0.29839049  0.02607142 -0.48162939 -0.47073439  0.26699727 -0.2103273\n",
      " -0.2049056   0.36918613  0.02650501  0.43941451] [ 0.17680631  0.70607172  0.03448923  0.35274316  0.22896331 -0.188487\n",
      "  0.20375255  0.23222704  0.42225731 -0.49413255]\n",
      "\n",
      "# 33 Gradient out:  [1.21947057 2.2885022  0.94128175 0.953961   3.0763873  1.42701282\n",
      " 1.44194494 3.26993612 2.29027526 3.37017372]\n",
      "\n",
      "     Weights  out:  [-0.29839049  0.02607142 -0.48162939 -0.47073439  0.26699727 -0.2103273\n",
      " -0.2049056   0.36918613  0.02650501  0.43941451] [ 0.05582526  0.44414073 -0.03895563  0.2772057  -0.13797482 -0.34219052\n",
      "  0.04784127 -0.16729188  0.16011884 -0.9109749 ]\n",
      "\n",
      "# 34 Gradient out:  [-0.24166021 -0.536644   -0.15248544 -0.15636829 -0.75560613 -0.30579335\n",
      " -0.31022732 -0.81772728 -0.53709897 -0.85000427]\n",
      "\n",
      "     Weights  out:  [-0.29839049  0.02607142 -0.48162939 -0.47073439  0.26699727 -0.2103273\n",
      " -0.2049056   0.36918613  0.02650501  0.43941451] [ 0.29971938  0.90184117  0.14930072  0.4679979   0.47730264 -0.05678795\n",
      "  0.33623025  0.48669534  0.61817389 -0.23694016]\n",
      "\n",
      "# 35 Gradient out:  [-0.51149435 -1.14205668 -0.31476132 -0.32335599 -1.61059462 -0.65117328\n",
      " -0.66076265 -1.7469843  -1.14301686 -1.81833666]\n",
      "\n",
      "     Weights  out:  [-0.29839049  0.02607142 -0.48162939 -0.47073439  0.26699727 -0.2103273\n",
      " -0.2049056   0.36918613  0.02650501  0.43941451] [ 0.25138734  0.79451237  0.11880364  0.43672424  0.32618141 -0.11794662\n",
      "  0.27418479  0.32314989  0.5107541  -0.40694101]\n",
      "\n",
      "# 36 Gradient out:  [0.49115394 1.24062939 0.36036408 0.36573118 1.78924412 0.61287196\n",
      " 0.62236091 1.89000231 1.24198249 1.93513924]\n",
      "\n",
      "     Weights  out:  [-0.29839049  0.02607142 -0.48162939 -0.47073439  0.26699727 -0.2103273\n",
      " -0.2049056   0.36918613  0.02650501  0.43941451] [ 0.14908847  0.56610103  0.05585137  0.37205304  0.00406249 -0.24818128\n",
      "  0.14203226 -0.02624697  0.28215072 -0.77060834]\n",
      "\n",
      "# 37 Gradient out:  [-0.46796956 -1.04601358 -0.28941024 -0.29719924 -1.4753975  -0.595286\n",
      " -0.60404568 -1.59939885 -1.04689724 -1.66411594]\n",
      "\n",
      "     Weights  out:  [-0.29839049  0.02607142 -0.48162939 -0.47073439  0.26699727 -0.2103273\n",
      " -0.2049056   0.36918613  0.02650501  0.43941451] [ 0.24731925  0.81422691  0.12792419  0.44519928  0.36191131 -0.12560689\n",
      "  0.26650444  0.35175349  0.53054722 -0.38358049]\n",
      "\n",
      "# 38 Gradient out:  [0.10908243 0.45028259 0.0978494  0.09786487 0.69659257 0.14491183\n",
      " 0.14840185 0.71472678 0.45099109 0.71655146]\n",
      "\n",
      "     Weights  out:  [-0.29839049  0.02607142 -0.48162939 -0.47073439  0.26699727 -0.2103273\n",
      " -0.2049056   0.36918613  0.02650501  0.43941451] [ 0.15372534  0.60502419  0.07004214  0.38575943  0.06683181 -0.24466409\n",
      "  0.14569531  0.03187372  0.32116777 -0.71640368]\n",
      "\n",
      "# 39 Gradient out:  [-0.55194203 -1.14159864 -0.3325957  -0.34239464 -1.58231298 -0.69702508\n",
      " -0.70660694 -1.73027296 -1.14242807 -1.81069437]\n",
      "\n",
      "     Weights  out:  [-0.29839049  0.02607142 -0.48162939 -0.47073439  0.26699727 -0.2103273\n",
      " -0.2049056   0.36918613  0.02650501  0.43941451] [ 0.17554183  0.69508071  0.08961202  0.40533241  0.20615032 -0.21568172\n",
      "  0.17537568  0.17481907  0.41136599 -0.57309339]\n",
      "\n",
      "# 40 Gradient out:  [1.20054652 2.28117685 0.920567   0.93331825 3.07753445 1.40987364\n",
      " 1.4249484  3.27250454 2.28297132 3.37335065]\n",
      "\n",
      "     Weights  out:  [-0.29839049  0.02607142 -0.48162939 -0.47073439  0.26699727 -0.2103273\n",
      " -0.2049056   0.36918613  0.02650501  0.43941451] [ 0.06515342  0.46676098  0.02309288  0.33685348 -0.11031227 -0.35508674\n",
      "  0.03405429 -0.17123552  0.18288038 -0.93523226]\n",
      "\n",
      "# 41 Gradient out:  [-0.2281593  -0.51183914 -0.14253913 -0.14626488 -0.72240572 -0.28979143\n",
      " -0.29405383 -0.78207754 -0.51227686 -0.81306303]\n",
      "\n",
      "     Weights  out:  [-0.29839049  0.02607142 -0.48162939 -0.47073439  0.26699727 -0.2103273\n",
      " -0.2049056   0.36918613  0.02650501  0.43941451] [ 0.30526273  0.92299635  0.20720628  0.52351713  0.50519462 -0.07311201\n",
      "  0.31904397  0.48326539  0.63947464 -0.26056213]\n",
      "\n",
      "# 42 Gradient out:  [-0.48121171 -1.08330435 -0.29427665 -0.30243874 -1.53062022 -0.61420429\n",
      " -0.62334454 -1.66031777 -1.08422297 -1.72809452]\n",
      "\n",
      "     Weights  out:  [-0.29839049  0.02607142 -0.48162939 -0.47073439  0.26699727 -0.2103273\n",
      " -0.2049056   0.36918613  0.02650501  0.43941451] [ 0.25963087  0.82062852  0.17869845  0.49426415  0.36071347 -0.1310703\n",
      "  0.2602332   0.32684988  0.53701927 -0.42317474]\n",
      "\n",
      "# 43 Gradient out:  [0.22604763 0.74894419 0.16655307 0.16862913 1.12964353 0.29865526\n",
      " 0.30476081 1.18219168 0.74994581 1.20131591]\n",
      "\n",
      "     Weights  out:  [-0.29839049  0.02607142 -0.48162939 -0.47073439  0.26699727 -0.2103273\n",
      " -0.2049056   0.36918613  0.02650501  0.43941451] [ 0.16338852  0.60396766  0.11984312  0.4337764   0.05458943 -0.25391116\n",
      "  0.13556429 -0.00521367  0.32017468 -0.76879364]\n",
      "\n",
      "# 44 Gradient out:  [-0.6132323  -1.3350825  -0.37387691 -0.38443464 -1.87244096 -0.7788071\n",
      " -0.79002454 -2.03664319 -1.33615493 -2.12381208]\n",
      "\n",
      "     Weights  out:  [-0.29839049  0.02607142 -0.48162939 -0.47073439  0.26699727 -0.2103273\n",
      " -0.2049056   0.36918613  0.02650501  0.43941451] [ 0.20859805  0.75375649  0.15315374  0.46750223  0.28051814 -0.1941801\n",
      "  0.19651645  0.23122467  0.47016384 -0.52853046]\n",
      "\n",
      "# 45 Gradient out:  [1.16295952 2.24972759 0.88315427 0.89588033 3.05050908 1.37283051\n",
      " 1.38796448 3.24563341 2.25153522 3.346361  ]\n",
      "\n",
      "     Weights  out:  [-0.29839049  0.02607142 -0.48162939 -0.47073439  0.26699727 -0.2103273\n",
      " -0.2049056   0.36918613  0.02650501  0.43941451] [ 0.08595159  0.48673999  0.07837836  0.3906153  -0.09397006 -0.34994152\n",
      "  0.03851154 -0.17610397  0.20293285 -0.95329288]\n",
      "\n",
      "# 46 Gradient out:  [-0.22173137 -0.50348174 -0.13683812 -0.1405289  -0.71261423 -0.28290635\n",
      " -0.28713846 -0.77181409 -0.50391664 -0.80253049]\n",
      "\n",
      "     Weights  out:  [-0.29839049  0.02607142 -0.48162939 -0.47073439  0.26699727 -0.2103273\n",
      " -0.2049056   0.36918613  0.02650501  0.43941451] [ 0.31854349  0.93668551  0.25500921  0.56979137  0.51613176 -0.07537542\n",
      "  0.31610444  0.47302271  0.6532399  -0.28402068]\n",
      "\n",
      "# 47 Gradient out:  [-0.46816197 -1.06206972 -0.28383907 -0.29188754 -1.5033005  -0.59931578\n",
      " -0.60833054 -1.63119058 -1.062976   -1.69801845]\n",
      "\n",
      "     Weights  out:  [-0.29839049  0.02607142 -0.48162939 -0.47073439  0.26699727 -0.2103273\n",
      " -0.2049056   0.36918613  0.02650501  0.43941451] [ 0.27419722  0.83598916  0.22764159  0.54168559  0.37360892 -0.13195669\n",
      "  0.25867675  0.31865989  0.55245657 -0.44452678]\n",
      "\n",
      "# 48 Gradient out:  [0.11823087 0.56390715 0.08447574 0.08532381 0.88737368 0.17377969\n",
      " 0.17872185 0.92289022 0.56479023 0.93256261]\n",
      "\n",
      "     Weights  out:  [-0.29839049  0.02607142 -0.48162939 -0.47073439  0.26699727 -0.2103273\n",
      " -0.2049056   0.36918613  0.02650501  0.43941451] [ 0.18056483  0.62357522  0.17087377  0.48330808  0.07294881 -0.25181985\n",
      "  0.13701064 -0.00757822  0.33986137 -0.78413047]\n",
      "\n",
      "# 49 Gradient out:  [-0.60166463 -1.25961207 -0.36657143 -0.37707466 -1.75052838 -0.75922175\n",
      " -0.76972485 -1.90971958 -1.26055841 -1.99575558]\n",
      "\n",
      "     Weights  out:  [-0.29839049  0.02607142 -0.48162939 -0.47073439  0.26699727 -0.2103273\n",
      " -0.2049056   0.36918613  0.02650501  0.43941451] [ 0.204211    0.73635665  0.18776892  0.50037284  0.25042355 -0.21706391\n",
      "  0.17275501  0.17699982  0.45281942 -0.59761795]\n",
      "\n",
      "Lineal error cuadrático medio (ECM): 1.1630094737541492\n",
      "\n",
      "# 0 Gradient out:  [1.09657319 0.99069494 0.94119128 1.88322053 0.98035558 0.90296216\n",
      " 2.6028124  2.60289872 2.34138965 2.30853644]\n",
      "\n",
      "     Weights  out:  [-0.23296386 -0.30337602 -0.34577882  0.04738963 -0.31161795 -0.3843562\n",
      "  0.44784625  0.4479602   0.22462401  0.20730123] [ 0.34173015  0.30910639 -0.4699337  -0.10338777 -0.32207225 -0.14358269\n",
      "  0.39325807 -0.17395066 -0.10880911 -0.36390257]\n",
      "\n",
      "# 1 Gradient out:  [-0.2734523  -0.23834101 -0.22032833 -0.46467086 -0.2346599  -0.20586451\n",
      " -0.67361365 -0.67364611 -0.58125495 -0.57141406]\n",
      "\n",
      "     Weights  out:  [-0.23296386 -0.30337602 -0.34577882  0.04738963 -0.31161795 -0.3843562\n",
      "  0.44784625  0.4479602   0.22462401  0.20730123] [ 0.56104479  0.50724538 -0.28169545  0.27325633 -0.12600114  0.03700974\n",
      "  0.91382055  0.34662908  0.35946882  0.09780472]\n",
      "\n",
      "# 2 Gradient out:  [-0.51779991 -0.43989976 -0.40003267 -0.94179445 -0.43174459 -0.36811367\n",
      " -1.40463114 -1.40470213 -1.20044486 -1.17859169]\n",
      "\n",
      "     Weights  out:  [-0.23296386 -0.30337602 -0.34577882  0.04738963 -0.31161795 -0.3843562\n",
      "  0.44784625  0.4479602   0.22462401  0.20730123] [ 0.50635433  0.45957718 -0.32576111  0.18032216 -0.17293312 -0.00416316\n",
      "  0.77909782  0.21189986  0.24321783 -0.0164781 ]\n",
      "\n",
      "# 3 Gradient out:  [ 0.02294422  0.07618705  0.10577957 -0.15259712  0.08214038  0.13005375\n",
      " -0.41858422 -0.41863627 -0.27170907 -0.25843862]\n",
      "\n",
      "     Weights  out:  [-0.23296386 -0.30337602 -0.34577882  0.04738963 -0.31161795 -0.3843562\n",
      "  0.44784625  0.4479602   0.22462401  0.20730123] [ 0.40279435  0.37159723 -0.40576765 -0.00803673 -0.25928203 -0.0777859\n",
      "  0.49817159 -0.06904056  0.00312885 -0.25219643]\n",
      "\n",
      "# 4 Gradient out:  [0.49466945 0.46828044 0.45903589 0.82251468 0.46619005 0.45301844\n",
      " 1.05754975 1.05756317 1.00367551 0.99347769]\n",
      "\n",
      "     Weights  out:  [-0.23296386 -0.30337602 -0.34577882  0.04738963 -0.31161795 -0.3843562\n",
      "  0.44784625  0.4479602   0.22462401  0.20730123] [ 0.40738319  0.38683464 -0.38461173 -0.03855615 -0.24285396 -0.05177515\n",
      "  0.41445475 -0.15276782 -0.05121296 -0.30388416]\n",
      "\n",
      "# 5 Gradient out:  [-0.68288659 -0.56414364 -0.50306631 -1.31326238 -0.55166236 -0.45409802\n",
      " -2.01167171 -2.01178029 -1.69951825 -1.66643426]\n",
      "\n",
      "     Weights  out:  [-0.23296386 -0.30337602 -0.34577882  0.04738963 -0.31161795 -0.3843562\n",
      "  0.44784625  0.4479602   0.22462401  0.20730123] [ 0.50631708  0.48049072 -0.29280456  0.12594678 -0.14961595  0.03882854\n",
      "  0.6259647   0.05874482  0.14952214 -0.10518862]\n",
      "\n",
      "# 6 Gradient out:  [1.38794616 1.22789336 1.14993129 2.44555156 1.21177311 1.08855998\n",
      " 3.47806389 3.47820299 3.07127341 3.02362354]\n",
      "\n",
      "     Weights  out:  [-0.23296386 -0.30337602 -0.34577882  0.04738963 -0.31161795 -0.3843562\n",
      "  0.44784625  0.4479602   0.22462401  0.20730123] [ 0.36973976  0.367662   -0.39341782 -0.13670569 -0.25994842 -0.05199106\n",
      "  0.22363035 -0.34361124 -0.19038151 -0.43847547]\n",
      "\n",
      "# 7 Gradient out:  [-0.16388395 -0.14164084 -0.13024612 -0.28550819 -0.13931128 -0.12110428\n",
      " -0.41805432 -0.41807481 -0.35961878 -0.35337585]\n",
      "\n",
      "     Weights  out:  [-0.23296386 -0.30337602 -0.34577882  0.04738963 -0.31161795 -0.3843562\n",
      "  0.44784625  0.4479602   0.22462401  0.20730123] [ 0.647329    0.61324067 -0.16343156  0.35240462 -0.0175938   0.16572094\n",
      "  0.91924313  0.35202936  0.42387317  0.16624924]\n",
      "\n",
      "# 8 Gradient out:  [-0.270492   -0.2315799  -0.2116644  -0.48332341 -0.22750693 -0.19570247\n",
      " -0.71512394 -0.71515962 -0.61302405 -0.60209782]\n",
      "\n",
      "     Weights  out:  [-0.23296386 -0.30337602 -0.34577882  0.04738963 -0.31161795 -0.3843562\n",
      "  0.44784625  0.4479602   0.22462401  0.20730123] [ 0.61455221  0.5849125  -0.18948078  0.29530298 -0.04545605  0.14150008\n",
      "  0.83563227  0.26841439  0.35194942  0.09557407]\n",
      "\n",
      "# 9 Gradient out:  [-0.54660516 -0.45925403 -0.41450572 -1.02009509 -0.45010256 -0.37866122\n",
      " -1.53828659 -1.53836636 -1.30914637 -1.28467039]\n",
      "\n",
      "     Weights  out:  [-0.23296386 -0.30337602 -0.34577882  0.04738963 -0.31161795 -0.3843562\n",
      "  0.44784625  0.4479602   0.22462401  0.20730123] [ 0.56045381  0.53859652 -0.23181366  0.1986383  -0.09095744  0.10235959\n",
      "  0.69260748  0.12538247  0.22934461 -0.0248455 ]\n",
      "\n",
      "# 10 Gradient out:  [0.23470435 0.22889033 0.23062565 0.45567319 0.22901405 0.23375028\n",
      " 0.56856319 0.56855409 0.57148726 0.56690952]\n",
      "\n",
      "     Weights  out:  [-0.23296386 -0.30337602 -0.34577882  0.04738963 -0.31161795 -0.3843562\n",
      "  0.44784625  0.4479602   0.22462401  0.20730123] [ 0.45113278  0.44674572 -0.31471481 -0.00538072 -0.18097795  0.02662734\n",
      "  0.38495016 -0.1822908  -0.03248466 -0.28177958]\n",
      "\n",
      "# 11 Gradient out:  [-0.61100267 -0.49580699 -0.43508238 -1.16664902 -0.48347232 -0.38580696\n",
      " -1.82159576 -1.82170625 -1.51278734 -1.481579  ]\n",
      "\n",
      "     Weights  out:  [-0.23296386 -0.30337602 -0.34577882  0.04738963 -0.31161795 -0.3843562\n",
      "  0.44784625  0.4479602   0.22462401  0.20730123] [ 0.49807365  0.49252378 -0.26858968  0.08575392 -0.13517514  0.0733774\n",
      "  0.4986628  -0.06857998  0.08181279 -0.16839767]\n",
      "\n",
      "# 12 Gradient out:  [1.37285277 1.21990979 1.14419218 2.34588649 1.20432481 1.08395859\n",
      " 3.31875576 3.31889454 2.92433425 2.87944921]\n",
      "\n",
      "     Weights  out:  [-0.23296386 -0.30337602 -0.34577882  0.04738963 -0.31161795 -0.3843562\n",
      "  0.44784625  0.4479602   0.22462401  0.20730123] [ 0.37587311  0.39336238 -0.35560615 -0.14757588 -0.23186961 -0.00378399\n",
      "  0.13434365 -0.43292123 -0.22074468 -0.46471347]\n",
      "\n",
      "# 13 Gradient out:  [-0.24763317 -0.20997534 -0.19072745 -0.45415657 -0.20603732 -0.17531613\n",
      " -0.67864677 -0.67868115 -0.57997324 -0.56938772]\n",
      "\n",
      "     Weights  out:  [-0.23296386 -0.30337602 -0.34577882  0.04738963 -0.31161795 -0.3843562\n",
      "  0.44784625  0.4479602   0.22462401  0.20730123] [ 0.65044367  0.63734434 -0.12676772  0.32160141  0.00899536  0.21300772\n",
      "  0.7980948   0.23085767  0.36412217  0.11117637]\n",
      "\n",
      "# 14 Gradient out:  [-0.50334552 -0.42118389 -0.37909942 -0.94910937 -0.41257715 -0.34538647\n",
      " -1.43672639 -1.43680147 -1.22119238 -1.19816484]\n",
      "\n",
      "     Weights  out:  [-0.23296386 -0.30337602 -0.34577882  0.04738963 -0.31161795 -0.3843562\n",
      "  0.44784625  0.4479602   0.22462401  0.20730123] [ 0.60091703  0.59534927 -0.16491321  0.2307701  -0.03221211  0.1779445\n",
      "  0.66236545  0.09512144  0.24812752 -0.00270117]\n",
      "\n",
      "# 15 Gradient out:  [-0.16807156 -0.12942679 -0.10495283 -0.20277446 -0.12466101 -0.08343801\n",
      " -0.36176045 -0.3618125  -0.24119431 -0.23315931]\n",
      "\n",
      "     Weights  out:  [-0.23296386 -0.30337602 -0.34577882  0.04738963 -0.31161795 -0.3843562\n",
      "  0.44784625  0.4479602   0.22462401  0.20730123] [ 0.50024793  0.51111249 -0.24073309  0.04094823 -0.11472754  0.1088672\n",
      "  0.37502017 -0.19223885  0.00388905 -0.24233414]\n",
      "\n",
      "# 16 Gradient out:  [0.41249595 0.36518577 0.34677258 0.89476525 0.36112844 0.33428019\n",
      " 1.26762675 1.26765068 1.16666985 1.14985217]\n",
      "\n",
      "     Weights  out:  [-0.23296386 -0.30337602 -0.34577882  0.04738963 -0.31161795 -0.3843562\n",
      "  0.44784625  0.4479602   0.22462401  0.20730123] [ 4.66633616e-01  4.85227136e-01 -2.61723658e-01  3.93335845e-04\n",
      " -1.39659739e-01  9.21796022e-02  3.02668078e-01 -2.64601351e-01\n",
      " -4.43498135e-02 -2.88966001e-01]\n",
      "\n",
      "# 17 Gradient out:  [-0.68250804 -0.56654635 -0.50673337 -1.29529806 -0.5543346  -0.45865968\n",
      " -1.97680882 -1.97691613 -1.67096772 -1.6387232 ]\n",
      "\n",
      "     Weights  out:  [-0.23296386 -0.30337602 -0.34577882  0.04738963 -0.31161795 -0.3843562\n",
      "  0.44784625  0.4479602   0.22462401  0.20730123] [ 0.54913281  0.55826429 -0.19236914  0.17934639 -0.06743405  0.15903564\n",
      "  0.55619343 -0.01107122  0.18898416 -0.05899557]\n",
      "\n",
      "# 18 Gradient out:  [1.28946104 1.13475142 1.05970164 2.32349129 1.11921703 1.00074866\n",
      " 3.32630359 3.32643701 2.93433094 2.88808449]\n",
      "\n",
      "     Weights  out:  [-0.23296386 -0.30337602 -0.34577882  0.04738963 -0.31161795 -0.3843562\n",
      "  0.44784625  0.4479602   0.22462401  0.20730123] [ 0.4126312   0.44495502 -0.29371582 -0.07971323 -0.17830097  0.06730371\n",
      "  0.16083166 -0.40645444 -0.14520939 -0.38674021]\n",
      "\n",
      "# 19 Gradient out:  [-0.16426292 -0.13952864 -0.12688973 -0.30014921 -0.13694269 -0.11676991\n",
      " -0.44771197 -0.44773456 -0.38290623 -0.37595035]\n",
      "\n",
      "     Weights  out:  [-0.23296386 -0.30337602 -0.34577882  0.04738963 -0.31161795 -0.3843562\n",
      "  0.44784625  0.4479602   0.22462401  0.20730123] [ 0.67052341  0.67190531 -0.08177549  0.38498503  0.04554243  0.26745344\n",
      "  0.82609238  0.25883296  0.4416568   0.19087669]\n",
      "\n",
      "# 20 Gradient out:  [-0.28158078 -0.23720181 -0.21452765 -0.52496657 -0.23256209 -0.19638103\n",
      " -0.78947275 -0.78951317 -0.67325126 -0.66077426]\n",
      "\n",
      "     Weights  out:  [-0.23296386 -0.30337602 -0.34577882  0.04738963 -0.31161795 -0.3843562\n",
      "  0.44784625  0.4479602   0.22462401  0.20730123] [ 0.63767082  0.64399958 -0.10715343  0.32495519  0.0181539   0.24409946\n",
      "  0.73654999  0.16928605  0.36507555  0.11568662]\n",
      "\n",
      "# 21 Gradient out:  [-0.60019732 -0.49919326 -0.44729233 -1.14180411 -0.48858727 -0.40565011\n",
      " -1.7386562  -1.73874906 -1.47302578 -1.44481766]\n",
      "\n",
      "     Weights  out:  [-0.23296386 -0.30337602 -0.34577882  0.04738963 -0.31161795 -0.3843562\n",
      "  0.44784625  0.4479602   0.22462401  0.20730123] [ 0.58135466  0.59655921 -0.15005896  0.21996188 -0.02835852  0.20482325\n",
      "  0.57865544  0.01138341  0.2304253  -0.01646823]\n",
      "\n",
      "# 22 Gradient out:  [0.69887687 0.60540298 0.56358874 1.44511505 0.59655073 0.53238589\n",
      " 2.09813158 2.09819765 1.87668279 1.8467363 ]\n",
      "\n",
      "     Weights  out:  [-0.23296386 -0.30337602 -0.34577882  0.04738963 -0.31161795 -0.3843562\n",
      "  0.44784625  0.4479602   0.22462401  0.20730123] [ 0.4613152   0.49672056 -0.23951743 -0.00839894 -0.12607598  0.12369323\n",
      "  0.2309242  -0.3363664  -0.06417985 -0.30543176]\n",
      "\n",
      "# 23 Gradient out:  [-0.42329913 -0.35420645 -0.31886112 -0.79999933 -0.34697575 -0.29056275\n",
      " -1.21082206 -1.21088506 -1.02974172 -1.01034851]\n",
      "\n",
      "     Weights  out:  [-0.23296386 -0.30337602 -0.34577882  0.04738963 -0.31161795 -0.3843562\n",
      "  0.44784625  0.4479602   0.22462401  0.20730123] [ 0.60109058  0.61780116 -0.12679968  0.28062407 -0.00676583  0.23017041\n",
      "  0.65055051  0.08327313  0.3111567   0.06391549]\n",
      "\n",
      "# 24 Gradient out:  [-0.62933355 -0.52024153 -0.46172128 -1.1252564  -0.50841086 -0.41372169\n",
      " -1.73460468 -1.73471438 -1.43732948 -1.40830366]\n",
      "\n",
      "     Weights  out:  [-0.23296386 -0.30337602 -0.34577882  0.04738963 -0.31161795 -0.3843562\n",
      "  0.44784625  0.4479602   0.22462401  0.20730123] [ 0.51643075  0.54695987 -0.19057191  0.1206242  -0.07616098  0.17205786\n",
      "  0.4083861  -0.15890388  0.10520836 -0.13815421]\n",
      "\n",
      "# 25 Gradient out:  [1.36438466 1.21208422 1.1363632  2.32556285 1.19651868 1.07593201\n",
      " 3.29194186 3.29208202 2.89747946 2.85293222]\n",
      "\n",
      "     Weights  out:  [-0.23296386 -0.30337602 -0.34577882  0.04738963 -0.31161795 -0.3843562\n",
      "  0.44784625  0.4479602   0.22462401  0.20730123] [ 0.39056404  0.44291156 -0.28291616 -0.10442708 -0.17784315  0.08931352\n",
      "  0.06146517 -0.50584676 -0.18225754 -0.41981494]\n",
      "\n",
      "# 26 Gradient out:  [-0.23876071 -0.20004448 -0.18028437 -0.45164409 -0.19599983 -0.16448123\n",
      " -0.68258864 -0.68262379 -0.58129889 -0.57040363]\n",
      "\n",
      "     Weights  out:  [-0.23296386 -0.30337602 -0.34577882  0.04738963 -0.31161795 -0.3843562\n",
      "  0.44784625  0.4479602   0.22462401  0.20730123] [ 0.66344097  0.68532841 -0.05564352  0.36068549  0.06146058  0.30449992\n",
      "  0.71985354  0.15256964  0.39723836  0.1507715 ]\n",
      "\n",
      "# 27 Gradient out:  [-0.49374092 -0.4099199  -0.36696946 -0.94771826 -0.4011368  -0.33255721\n",
      " -1.44485132 -1.44492796 -1.22490233 -1.20142067]\n",
      "\n",
      "     Weights  out:  [-0.23296386 -0.30337602 -0.34577882  0.04738963 -0.31161795 -0.3843562\n",
      "  0.44784625  0.4479602   0.22462401  0.20730123] [ 0.61568883  0.64531951 -0.0917004   0.27035667  0.02226062  0.27160367\n",
      "  0.58333581  0.01604489  0.28097858  0.03669078]\n",
      "\n",
      "# 28 Gradient out:  [-0.23354268 -0.19968646 -0.17677264 -0.21960162 -0.19529348 -0.15599139\n",
      " -0.34283172 -0.34288457 -0.23023872 -0.22397233]\n",
      "\n",
      "     Weights  out:  [-0.23296386 -0.30337602 -0.34577882  0.04738963 -0.31161795 -0.3843562\n",
      "  0.44784625  0.4479602   0.22462401  0.20730123] [ 0.51694065  0.56333553 -0.16509429  0.08081302 -0.05796674  0.20509223\n",
      "  0.29436555 -0.27294071  0.03599811 -0.20359336]\n",
      "\n",
      "# 29 Gradient out:  [0.35566203 0.30300097 0.28239372 0.88015574 0.29846154 0.26846865\n",
      " 1.28888651 1.28891206 1.17675565 1.15818356]\n",
      "\n",
      "     Weights  out:  [-0.23296386 -0.30337602 -0.34577882  0.04738963 -0.31161795 -0.3843562\n",
      "  0.44784625  0.4479602   0.22462401  0.20730123] [ 0.47023211  0.52339824 -0.20044882  0.0368927  -0.09702544  0.17389395\n",
      "  0.2257992  -0.34151762 -0.01004963 -0.24838782]\n",
      "\n",
      "# 30 Gradient out:  [-0.68908026 -0.571177   -0.51020896 -1.30750015 -0.55873813 -0.46112744\n",
      " -1.99875784 -1.99886773 -1.68705941 -1.65435528]\n",
      "\n",
      "     Weights  out:  [-0.23296386 -0.30337602 -0.34577882  0.04738963 -0.31161795 -0.3843562\n",
      "  0.44784625  0.4479602   0.22462401  0.20730123] [ 0.54136451  0.58399843 -0.14397007  0.21292384 -0.03733313  0.22758768\n",
      "  0.48357651 -0.08373521  0.2253015  -0.01675111]\n",
      "\n",
      "# 31 Gradient out:  [1.27091204 1.11597792 1.04044835 2.2945751  1.10036565 0.98093218\n",
      " 3.29440989 3.29454521 2.90016835 2.85405618]\n",
      "\n",
      "     Weights  out:  [-0.23296386 -0.30337602 -0.34577882  0.04738963 -0.31161795 -0.3843562\n",
      "  0.44784625  0.4479602   0.22462401  0.20730123] [ 0.40354846  0.46976303 -0.24601187 -0.04857619 -0.14908076  0.13536219\n",
      "  0.08382494 -0.48350876 -0.11211038 -0.34762217]\n",
      "\n",
      "# 32 Gradient out:  [-0.17636545 -0.14834499 -0.13404375 -0.33060791 -0.14541783 -0.12260407\n",
      " -0.49784673 -0.4978722  -0.42452563 -0.41663868]\n",
      "\n",
      "     Weights  out:  [-0.23296386 -0.30337602 -0.34577882  0.04738963 -0.31161795 -0.3843562\n",
      "  0.44784625  0.4479602   0.22462401  0.20730123] [ 0.65773087  0.69295861 -0.0379222   0.41033883  0.07099237  0.33154863\n",
      "  0.74270692  0.17540029  0.46792329  0.22318907]\n",
      "\n",
      "# 33 Gradient out:  [-0.31825313 -0.26557791 -0.23867542 -0.60702463 -0.26007208 -0.21715533\n",
      " -0.92085695 -0.92090482 -0.78298908 -0.76817818]\n",
      "\n",
      "     Weights  out:  [-0.23296386 -0.30337602 -0.34577882  0.04738963 -0.31161795 -0.3843562\n",
      "  0.44784625  0.4479602   0.22462401  0.20730123] [ 0.62245778  0.66328962 -0.06473095  0.34421725  0.04190881  0.30702782\n",
      "  0.64313757  0.07582585  0.38301816  0.13986133]\n",
      "\n",
      "# 34 Gradient out:  [-0.68870432 -0.5703966  -0.50910611 -1.30586851 -0.55789837 -0.45970586\n",
      " -1.99829325 -1.9984041  -1.68497802 -1.6522205 ]\n",
      "\n",
      "     Weights  out:  [-0.23296386 -0.30337602 -0.34577882  0.04738963 -0.31161795 -0.3843562\n",
      "  0.44784625  0.4479602   0.22462401  0.20730123] [ 0.55880716  0.61017403 -0.11246603  0.22281233 -0.01010561  0.26359675\n",
      "  0.45896618 -0.10835512  0.22642035 -0.0137743 ]\n",
      "\n",
      "# 35 Gradient out:  [1.23644522 1.08303565 1.0081482  2.24697506 1.06756208 0.94908447\n",
      " 3.23586757 3.23600207 2.84502314 2.7994177 ]\n",
      "\n",
      "     Weights  out:  [-0.23296386 -0.30337602 -0.34577882  0.04738963 -0.31161795 -0.3843562\n",
      "  0.44784625  0.4479602   0.22462401  0.20730123] [ 0.42106629  0.49609471 -0.21428725 -0.03836138 -0.12168528  0.17165558\n",
      "  0.05930753 -0.50803594 -0.11057526 -0.3442184 ]\n",
      "\n",
      "# 36 Gradient out:  [-0.18919136 -0.15811725 -0.14226993 -0.36044733 -0.15487281 -0.12960197\n",
      " -0.54595388 -0.54598205 -0.46471346 -0.45596207]\n",
      "\n",
      "     Weights  out:  [-0.23296386 -0.30337602 -0.34577882  0.04738963 -0.31161795 -0.3843562\n",
      "  0.44784625  0.4479602   0.22462401  0.20730123] [ 0.66835534  0.71270184 -0.01265761  0.41103364  0.09182713  0.36147247\n",
      "  0.70648104  0.13916448  0.45842937  0.21566514]\n",
      "\n",
      "# 37 Gradient out:  [-0.35701884 -0.29617511 -0.26509389 -0.68997022 -0.28981419 -0.24023242\n",
      " -1.05217816 -1.05223343 -0.89292677 -0.87582641]\n",
      "\n",
      "     Weights  out:  [-0.23296386 -0.30337602 -0.34577882  0.04738963 -0.31161795 -0.3843562\n",
      "  0.44784625  0.4479602   0.22462401  0.20730123] [ 0.63051706  0.68107839 -0.0411116   0.33894417  0.06085257  0.33555208\n",
      "  0.59729027  0.02996807  0.36548668  0.12447273]\n",
      "\n",
      "# 38 Gradient out:  [-0.72692224 -0.60355747 -0.53866217 -1.33754357 -0.59037697 -0.4859025\n",
      " -2.04698756 -2.04710741 -1.71589164 -1.68228304]\n",
      "\n",
      "     Weights  out:  [-0.23296386 -0.30337602 -0.34577882  0.04738963 -0.31161795 -0.3843562\n",
      "  0.44784625  0.4479602   0.22462401  0.20730123] [ 0.5591133   0.62184337 -0.09413037  0.20095013  0.00288973  0.28750559\n",
      "  0.38685463 -0.18047862  0.18690133 -0.05069256]\n",
      "\n",
      "# 39 Gradient out:  [1.32023948 1.17187514 1.0975775  2.24260547 1.15663461 1.03797667\n",
      " 3.17939699 3.17953655 2.79241281 2.74927559]\n",
      "\n",
      "     Weights  out:  [-0.23296386 -0.30337602 -0.34577882  0.04738963 -0.31161795 -0.3843562\n",
      "  0.44784625  0.4479602   0.22462401  0.20730123] [ 0.41372885  0.50113188 -0.20186281 -0.06655859 -0.11518566  0.19032509\n",
      " -0.02254288 -0.5899001  -0.156277   -0.38714917]\n",
      "\n",
      "# 40 Gradient out:  [-0.27030476 -0.22321049 -0.19920091 -0.52947504 -0.21829413 -0.18001977\n",
      " -0.81036782 -0.81041037 -0.68732563 -0.67406415]\n",
      "\n",
      "     Weights  out:  [-0.23296386 -0.30337602 -0.34577882  0.04738963 -0.31161795 -0.3843562\n",
      "  0.44784625  0.4479602   0.22462401  0.20730123] [0.67777675 0.73550691 0.01765269 0.38196251 0.11614126 0.39792043\n",
      " 0.61333652 0.04600721 0.40220556 0.16270595]\n",
      "\n",
      "# 41 Gradient out:  [-0.5943291  -0.48953645 -0.43548174 -1.14945423 -0.47850176 -0.39200985\n",
      " -1.76615564 -1.76625297 -1.48960191 -1.46044885]\n",
      "\n",
      "     Weights  out:  [-0.23296386 -0.30337602 -0.34577882  0.04738963 -0.31161795 -0.3843562\n",
      "  0.44784625  0.4479602   0.22462401  0.20730123] [ 0.62371579  0.69086481 -0.02218749  0.2760675   0.07248243  0.36191647\n",
      "  0.45126296 -0.11607487  0.26474043  0.02789312]\n",
      "\n",
      "# 42 Gradient out:  [0.57486964 0.47464745 0.42989478 1.36651398 0.46515992 0.39669631\n",
      " 2.06139266 2.06146116 1.82525976 1.7932168 ]\n",
      "\n",
      "     Weights  out:  [-0.23296386 -0.30337602 -0.34577882  0.04738963 -0.31161795 -0.3843562\n",
      "  0.44784625  0.4479602   0.22462401  0.20730123] [ 0.50484997  0.59295752 -0.10928384  0.04617665 -0.02321792  0.2835145\n",
      "  0.09803183 -0.46932546 -0.03317995 -0.26419665]\n",
      "\n",
      "# 43 Gradient out:  [-0.46484427 -0.38331783 -0.34155796 -0.90666658 -0.37477728 -0.30810832\n",
      " -1.39026632 -1.39034079 -1.17641393 -1.15356866]\n",
      "\n",
      "     Weights  out:  [-0.23296386 -0.30337602 -0.34577882  0.04738963 -0.31161795 -0.3843562\n",
      "  0.44784625  0.4479602   0.22462401  0.20730123] [ 0.6198239   0.68788701 -0.02330488  0.31947945  0.06981407  0.36285377\n",
      "  0.51031036 -0.05703323  0.331872    0.09444671]\n",
      "\n",
      "# 44 Gradient out:  [-0.4072306  -0.35336638 -0.32007744 -0.50556181 -0.34686448 -0.29081186\n",
      " -0.7504493  -0.75052246 -0.58410757 -0.57222448]\n",
      "\n",
      "     Weights  out:  [-0.23296386 -0.30337602 -0.34577882  0.04738963 -0.31161795 -0.3843562\n",
      "  0.44784625  0.4479602   0.22462401  0.20730123] [ 0.52685505  0.61122344 -0.09161647  0.13814613 -0.00514139  0.3012321\n",
      "  0.2322571  -0.33510139  0.09658922 -0.13626702]\n",
      "\n",
      "# 45 Gradient out:  [0.78848599 0.67008276 0.61537683 1.66661082 0.6586013  0.573777\n",
      " 2.46645147 2.46654103 2.17898071 2.14210752]\n",
      "\n",
      "     Weights  out:  [-0.23296386 -0.30337602 -0.34577882  0.04738963 -0.31161795 -0.3843562\n",
      "  0.44784625  0.4479602   0.22462401  0.20730123] [ 0.44540893  0.54055017 -0.15563196  0.03703377 -0.07451429  0.24306973\n",
      "  0.08216724 -0.48520588 -0.0202323  -0.25071192]\n",
      "\n",
      "# 46 Gradient out:  [-0.33113795 -0.27488432 -0.24615828 -0.63943163 -0.26900491 -0.22318416\n",
      " -0.97450913 -0.9745602  -0.82730987 -0.81149272]\n",
      "\n",
      "     Weights  out:  [-0.23296386 -0.30337602 -0.34577882  0.04738963 -0.31161795 -0.3843562\n",
      "  0.44784625  0.4479602   0.22462401  0.20730123] [ 0.60310613  0.67456672 -0.03255659  0.37035593  0.05720597  0.35782513\n",
      "  0.57545753  0.00810233  0.41556384  0.17770959]\n",
      "\n",
      "# 47 Gradient out:  [-0.71108009 -0.58871121 -0.52495378 -1.33751856 -0.57572958 -0.47339429\n",
      " -2.04920716 -2.04932343 -1.72349705 -1.6898151 ]\n",
      "\n",
      "     Weights  out:  [-0.23296386 -0.30337602 -0.34577882  0.04738963 -0.31161795 -0.3843562\n",
      "  0.44784625  0.4479602   0.22462401  0.20730123] [ 0.53687854  0.61958985 -0.08178825  0.24246961  0.00340499  0.3131883\n",
      "  0.3805557  -0.18680971  0.25010187  0.01541105]\n",
      "\n",
      "# 48 Gradient out:  [1.28136502 1.12812602 1.05226325 2.2591049  1.11251329 0.99187271\n",
      " 3.23547849 3.23561808 2.84004025 2.79503904]\n",
      "\n",
      "     Weights  out:  [-0.23296386 -0.30337602 -0.34577882  0.04738963 -0.31161795 -0.3843562\n",
      "  0.44784625  0.4479602   0.22462401  0.20730123] [ 0.39466252  0.50184761 -0.186779   -0.0250341  -0.11174092  0.21850944\n",
      " -0.02928573 -0.5966744  -0.09459754 -0.32255198]\n",
      "\n",
      "# 49 Gradient out:  [-0.21870123 -0.181206   -0.16209654 -0.42544928 -0.1772928  -0.14683071\n",
      " -0.64927498 -0.64930886 -0.55132634 -0.54076256]\n",
      "\n",
      "     Weights  out:  [-0.23296386 -0.30337602 -0.34577882  0.04738963 -0.31161795 -0.3843562\n",
      "  0.44784625  0.4479602   0.22462401  0.20730123] [0.65093552 0.72747282 0.02367364 0.42678688 0.11076173 0.41688398\n",
      " 0.61780997 0.05044922 0.47341051 0.23645583]\n",
      "\n",
      "Tanh error cuadrático medio (ECM): 0.9569400908228407\n",
      "\n",
      "# 0 Gradient out:  [2.28399085 2.86744123 3.28535916 1.85082728 1.31135433 1.69162095\n",
      " 2.63923478 2.74621377 3.08756066 3.08808312]\n",
      "\n",
      "     Weights  out:  [ 0.02401211  0.18680564  0.38372482 -0.08416954 -0.26180588 -0.1281048\n",
      "  0.1162847   0.14763482  0.27388734  0.27412871] [ 0.14047258  0.14491641 -0.34692934 -0.2415005   0.45294485 -0.33678179\n",
      " -0.45476708  0.1984783  -0.06288936 -0.428015  ]\n",
      "\n",
      "# 1 Gradient out:  [-0.13807626 -0.17131998 -0.19948803 -0.1144992  -0.08178623 -0.10545035\n",
      " -0.15774477 -0.16397901 -0.18559939 -0.18563488]\n",
      "\n",
      "     Weights  out:  [ 0.02401211  0.18680564  0.38372482 -0.08416954 -0.26180588 -0.1281048\n",
      "  0.1162847   0.14763482  0.27388734  0.27412871] [0.59727075 0.71840465 0.31014249 0.12866495 0.71521571 0.0015424\n",
      " 0.07307988 0.74772106 0.55462277 0.18960162]\n",
      "\n",
      "# 2 Gradient out:  [-0.19457941 -0.24274559 -0.28345049 -0.16040432 -0.11304118 -0.1472929\n",
      " -0.22308488 -0.23211586 -0.26340173 -0.26345304]\n",
      "\n",
      "     Weights  out:  [ 0.02401211  0.18680564  0.38372482 -0.08416954 -0.26180588 -0.1281048\n",
      "  0.1162847   0.14763482  0.27388734  0.27412871] [ 0.5696555   0.68414066  0.27024488  0.10576511  0.69885847 -0.01954767\n",
      "  0.04153093  0.71492525  0.5175029   0.15247465]\n",
      "\n",
      "# 3 Gradient out:  [-0.31251354 -0.39290884 -0.46061365 -0.25544179 -0.17645821 -0.23355559\n",
      " -0.36010911 -0.37517964 -0.42731616 -0.42740152]\n",
      "\n",
      "     Weights  out:  [ 0.02401211  0.18680564  0.38372482 -0.08416954 -0.26180588 -0.1281048\n",
      "  0.1162847   0.14763482  0.27388734  0.27412871] [ 0.53073962  0.63559154  0.21355479  0.07368425  0.67625023 -0.04900625\n",
      " -0.00308605  0.66850208  0.46482255  0.09978404]\n",
      "\n",
      "# 4 Gradient out:  [-0.63507857 -0.80852045 -0.95405919 -0.51193354 -0.34164348 -0.46471285\n",
      " -0.73777465 -0.77028618 -0.88262831 -0.88281189]\n",
      "\n",
      "     Weights  out:  [ 0.02401211  0.18680564  0.38372482 -0.08416954 -0.26180588 -0.1281048\n",
      "  0.1162847   0.14763482  0.27388734  0.27412871] [ 0.46823691  0.55700977  0.12143206  0.02259589  0.64095859 -0.09571737\n",
      " -0.07510787  0.59346615  0.37935932  0.01430374]\n",
      "\n",
      "# 5 Gradient out:  [-1.26012836 -1.63772194 -1.97448928 -0.99736088 -0.61728508 -0.89465569\n",
      " -1.48095383 -1.55240754 -1.80720656 -1.80763267]\n",
      "\n",
      "     Weights  out:  [ 0.02401211  0.18680564  0.38372482 -0.08416954 -0.26180588 -0.1281048\n",
      "  0.1162847   0.14763482  0.27388734  0.27412871] [ 0.34122119  0.39530568 -0.06937978 -0.07979082  0.57262989 -0.18865994\n",
      " -0.2226628   0.43940892  0.20283365 -0.16225864]\n",
      "\n",
      "# 6 Gradient out:  [1.61379456 1.97625673 2.25371273 1.34813842 1.00620355 1.24933625\n",
      " 1.83266124 1.89954018 2.11923757 2.11958423]\n",
      "\n",
      "     Weights  out:  [ 0.02401211  0.18680564  0.38372482 -0.08416954 -0.26180588 -0.1281048\n",
      "  0.1162847   0.14763482  0.27388734  0.27412871] [ 0.08919552  0.06776129 -0.46427764 -0.27926299  0.44917288 -0.36759108\n",
      " -0.51885357  0.12892741 -0.15860766 -0.52378518]\n",
      "\n",
      "# 7 Gradient out:  [-1.17237846 -1.51291594 -1.8017321  -0.93157764 -0.59560137 -0.83887663\n",
      " -1.37351167 -1.43747169 -1.65979917 -1.66016386]\n",
      "\n",
      "     Weights  out:  [ 0.02401211  0.18680564  0.38372482 -0.08416954 -0.26180588 -0.1281048\n",
      "  0.1162847   0.14763482  0.27388734  0.27412871] [ 0.41195443  0.46301264 -0.01353509 -0.00963531  0.65041358 -0.11772383\n",
      " -0.15232132  0.50883545  0.26523986 -0.09986833]\n",
      "\n",
      "# 8 Gradient out:  [2.31583119 2.91946049 3.34673945 1.86657231 1.31053972 1.70183164\n",
      " 2.68394586 2.79448688 3.14530446 3.14583851]\n",
      "\n",
      "     Weights  out:  [ 0.02401211  0.18680564  0.38372482 -0.08416954 -0.26180588 -0.1281048\n",
      "  0.1162847   0.14763482  0.27388734  0.27412871] [ 0.17747874  0.16042945 -0.37388151 -0.19595084  0.53129331 -0.28549915\n",
      " -0.42702365  0.22134111 -0.06671998 -0.4319011 ]\n",
      "\n",
      "# 9 Gradient out:  [-0.10425124 -0.12949266 -0.15086785 -0.08634707 -0.06151353 -0.07947635\n",
      " -0.11918654 -0.1239198  -0.14033032 -0.14035725]\n",
      "\n",
      "     Weights  out:  [ 0.02401211  0.18680564  0.38372482 -0.08416954 -0.26180588 -0.1281048\n",
      "  0.1162847   0.14763482  0.27388734  0.27412871] [0.64064498 0.74432155 0.29546638 0.17736362 0.79340126 0.05486717\n",
      " 0.10976552 0.78023848 0.56234091 0.1972666 ]\n",
      "\n",
      "# 10 Gradient out:  [-0.13570485 -0.16922109 -0.19754874 -0.11192337 -0.07896649 -0.10279988\n",
      " -0.15554066 -0.16182469 -0.18359449 -0.18363019]\n",
      "\n",
      "     Weights  out:  [ 0.02401211  0.18680564  0.38372482 -0.08416954 -0.26180588 -0.1281048\n",
      "  0.1162847   0.14763482  0.27388734  0.27412871] [0.61979473 0.71842302 0.26529281 0.16009421 0.78109855 0.03897191\n",
      " 0.08592821 0.75545452 0.53427485 0.16919515]\n",
      "\n",
      "# 11 Gradient out:  [-0.19080167 -0.23913648 -0.27988857 -0.15649198 -0.10899554 -0.14333404\n",
      " -0.21941507 -0.22847591 -0.25983405 -0.25988541]\n",
      "\n",
      "     Weights  out:  [ 0.02401211  0.18680564  0.38372482 -0.08416954 -0.26180588 -0.1281048\n",
      "  0.1162847   0.14763482  0.27388734  0.27412871] [0.59265376 0.6845788  0.22578306 0.13770954 0.76530525 0.01841193\n",
      " 0.05482008 0.72308959 0.49755595 0.13246911]\n",
      "\n",
      "# 12 Gradient out:  [-0.30528439 -0.38534895 -0.45264211 -0.24842645 -0.16981339 -0.22662942\n",
      " -0.35269556 -0.36770163 -0.41957142 -0.41965626]\n",
      "\n",
      "     Weights  out:  [ 0.02401211  0.18680564  0.38372482 -0.08416954 -0.26180588 -0.1281048\n",
      "  0.1162847   0.14763482  0.27388734  0.27412871] [ 0.55449343  0.6367515   0.16980535  0.10641114  0.74350615 -0.01025488\n",
      "  0.01093706  0.6773944   0.44558914  0.08049203]\n",
      "\n",
      "# 13 Gradient out:  [-0.61684778 -0.78727867 -0.93014616 -0.49581698 -0.32853415 -0.44941508\n",
      " -0.71777387 -0.74971822 -0.86005178 -0.86023199]\n",
      "\n",
      "     Weights  out:  [ 0.02401211  0.18680564  0.38372482 -0.08416954 -0.26180588 -0.1281048\n",
      "  0.1162847   0.14763482  0.27388734  0.27412871] [ 0.49343655  0.55968171  0.07927692  0.05672585  0.70954347 -0.05558076\n",
      " -0.05960205  0.60385408  0.36167486 -0.00343922]\n",
      "\n",
      "# 14 Gradient out:  [-1.29677665 -1.67842497 -2.01834902 -1.03092618 -0.64715106 -0.9271173\n",
      " -1.52010366 -1.59228999 -1.8494282  -1.8498581 ]\n",
      "\n",
      "     Weights  out:  [ 0.02401211  0.18680564  0.38372482 -0.08416954 -0.26180588 -0.1281048\n",
      "  0.1162847   0.14763482  0.27388734  0.27412871] [ 0.37006699  0.40222598 -0.10675231 -0.04243755  0.64383664 -0.14546378\n",
      " -0.20315682  0.45391043  0.1896645  -0.17548562]\n",
      "\n",
      "# 15 Gradient out:  [1.61585531 1.97056653 2.24509807 1.35640032 1.02070369 1.25972898\n",
      " 1.82976297 1.89527416 2.11149248 2.11183538]\n",
      "\n",
      "     Weights  out:  [ 0.02401211  0.18680564  0.38372482 -0.08416954 -0.26180588 -0.1281048\n",
      "  0.1162847   0.14763482  0.27388734  0.27412871] [ 0.11071166  0.06654098 -0.51042211 -0.24862279  0.51440642 -0.33088724\n",
      " -0.50717755  0.13545243 -0.18022114 -0.54545724]\n",
      "\n",
      "# 16 Gradient out:  [-1.18254771 -1.52674677 -1.81958216 -0.93935808 -0.59939468 -0.84566512\n",
      " -1.38573877 -1.45041139 -1.67554576 -1.67591552]\n",
      "\n",
      "     Weights  out:  [ 0.02401211  0.18680564  0.38372482 -0.08416954 -0.26180588 -0.1281048\n",
      "  0.1162847   0.14763482  0.27388734  0.27412871] [ 0.43388273  0.46065429 -0.0614025   0.02265728  0.71854716 -0.07894144\n",
      " -0.14122496  0.51450727  0.24207736 -0.12309017]\n",
      "\n",
      "# 17 Gradient out:  [2.28326306 2.87249104 3.29389675 1.8455585  1.30114472 1.684773\n",
      " 2.6421524  2.75015808 3.09445743 3.09498408]\n",
      "\n",
      "     Weights  out:  [ 0.02401211  0.18680564  0.38372482 -0.08416954 -0.26180588 -0.1281048\n",
      "  0.1162847   0.14763482  0.27388734  0.27412871] [ 0.19737318  0.15530493 -0.42531893 -0.16521434  0.59866823 -0.24807446\n",
      " -0.41837271  0.22442499 -0.09303179 -0.45827327]\n",
      "\n",
      "# 18 Gradient out:  [-0.11722877 -0.14669569 -0.17155445 -0.09631288 -0.06735397 -0.08829135\n",
      " -0.13467221 -0.14019612 -0.15931723 -0.15934856]\n",
      "\n",
      "     Weights  out:  [ 0.02401211  0.18680564  0.38372482 -0.08416954 -0.26180588 -0.1281048\n",
      "  0.1162847   0.14763482  0.27388734  0.27412871] [0.6540258  0.72980314 0.23346042 0.20389736 0.85889717 0.08888014\n",
      " 0.11005776 0.7744566  0.52585969 0.16072355]\n",
      "\n",
      "# 19 Gradient out:  [-0.15804952 -0.19859997 -0.23273894 -0.12925669 -0.08942774 -0.11821753\n",
      " -0.18205941 -0.18965995 -0.21594722 -0.21599025]\n",
      "\n",
      "     Weights  out:  [ 0.02401211  0.18680564  0.38372482 -0.08416954 -0.26180588 -0.1281048\n",
      "  0.1162847   0.14763482  0.27388734  0.27412871] [0.63058004 0.70046401 0.19914953 0.18463479 0.84542638 0.07122187\n",
      " 0.08312332 0.74641738 0.49399625 0.12885384]\n",
      "\n",
      "# 20 Gradient out:  [-0.2353552  -0.29737623 -0.34945826 -0.19130023 -0.13042339 -0.17441516\n",
      " -0.27208728 -0.28371022 -0.32386856 -0.32393422]\n",
      "\n",
      "     Weights  out:  [ 0.02401211  0.18680564  0.38372482 -0.08416954 -0.26180588 -0.1281048\n",
      "  0.1162847   0.14763482  0.27388734  0.27412871] [0.59897014 0.66074401 0.15260174 0.15878345 0.82754083 0.04757836\n",
      " 0.04671144 0.70848539 0.4508068  0.08565578]\n",
      "\n",
      "# 21 Gradient out:  [-0.41746499 -0.53167729 -0.62732565 -0.33631665 -0.2242767  -0.30522068\n",
      " -0.48512002 -0.50652178 -0.58039444 -0.58051506]\n",
      "\n",
      "     Weights  out:  [ 0.02401211  0.18680564  0.38372482 -0.08416954 -0.26180588 -0.1281048\n",
      "  0.1162847   0.14763482  0.27388734  0.27412871] [ 0.5518991   0.60126877  0.08271009  0.1205234   0.80145615  0.01269533\n",
      " -0.00770601  0.65174335  0.38603309  0.02086894]\n",
      "\n",
      "# 22 Gradient out:  [-0.9897297  -1.27535363 -1.51609038 -0.78733741 -0.50627018 -0.70957767\n",
      " -1.15864643 -1.21223897 -1.39791776 -1.39822161]\n",
      "\n",
      "     Weights  out:  [ 0.02401211  0.18680564  0.38372482 -0.08416954 -0.26180588 -0.1281048\n",
      "  0.1162847   0.14763482  0.27388734  0.27412871] [ 0.4684061   0.49493331 -0.04275504  0.05326007  0.75660081 -0.04834881\n",
      " -0.10473002  0.55043899  0.2699542  -0.09523407]\n",
      "\n",
      "# 23 Gradient out:  [1.58011612 2.06159682 2.35048777 1.21035293 0.78828248 1.07866423\n",
      " 1.87972413 1.96648234 2.22245822 2.22281815]\n",
      "\n",
      "     Weights  out:  [ 0.02401211  0.18680564  0.38372482 -0.08416954 -0.26180588 -0.1281048\n",
      "  0.1162847   0.14763482  0.27388734  0.27412871] [ 0.27046016  0.23986258 -0.34597312 -0.10420741  0.65534677 -0.19026434\n",
      " -0.3364593   0.3079912  -0.00962935 -0.37487839]\n",
      "\n",
      "# 24 Gradient out:  [-0.2766105  -0.3500856  -0.41175232 -0.22441676 -0.15230558 -0.2044131\n",
      " -0.32012788 -0.3338971  -0.38146169 -0.38153944]\n",
      "\n",
      "     Weights  out:  [ 0.02401211  0.18680564  0.38372482 -0.08416954 -0.26180588 -0.1281048\n",
      "  0.1162847   0.14763482  0.27388734  0.27412871] [0.58648338 0.65218195 0.12412444 0.13786318 0.81300327 0.0254685\n",
      " 0.03948552 0.70128766 0.4348623  0.06968524]\n",
      "\n",
      "# 25 Gradient out:  [-0.53311334 -0.68096844 -0.80477237 -0.42807884 -0.2830189  -0.38782273\n",
      " -0.62068905 -0.64839746 -0.74404547 -0.74420163]\n",
      "\n",
      "     Weights  out:  [ 0.02401211  0.18680564  0.38372482 -0.08416954 -0.26180588 -0.1281048\n",
      "  0.1162847   0.14763482  0.27388734  0.27412871] [ 0.53116128  0.58216483  0.04177397  0.09297983  0.78254215 -0.01541412\n",
      " -0.02454005  0.63450824  0.35856996 -0.00662265]\n",
      "\n",
      "# 26 Gradient out:  [-1.28710777 -1.66321568 -1.98758771 -1.02244953 -0.64902657 -0.92009339\n",
      " -1.50857645 -1.57937953 -1.8275264  -1.82793615]\n",
      "\n",
      "     Weights  out:  [ 0.02401211  0.18680564  0.38372482 -0.08416954 -0.26180588 -0.1281048\n",
      "  0.1162847   0.14763482  0.27388734  0.27412871] [ 0.42453861  0.44597114 -0.1191805   0.00736406  0.72593838 -0.09297866\n",
      " -0.14867787  0.50482875  0.20976086 -0.15546298]\n",
      "\n",
      "# 27 Gradient out:  [2.00016543 2.46675085 2.81923173 1.65723075 1.21892668 1.53001952\n",
      " 2.28241368 2.36838621 2.64911813 2.64955849]\n",
      "\n",
      "     Weights  out:  [ 0.02401211  0.18680564  0.38372482 -0.08416954 -0.26180588 -0.1281048\n",
      "  0.1162847   0.14763482  0.27388734  0.27412871] [ 0.16711706  0.113328   -0.51669804 -0.19712585  0.59613306 -0.27699734\n",
      " -0.45039316  0.18895285 -0.15574441 -0.52105021]\n",
      "\n",
      "# 28 Gradient out:  [-0.41490515 -0.52944019 -0.62527295 -0.33351447 -0.22118682 -0.30233006\n",
      " -0.48275849 -0.50421921 -0.57826737 -0.57838824]\n",
      "\n",
      "     Weights  out:  [ 0.02401211  0.18680564  0.38372482 -0.08416954 -0.26180588 -0.1281048\n",
      "  0.1162847   0.14763482  0.27388734  0.27412871] [0.56715015 0.60667817 0.0471483  0.1343203  0.8399184  0.02900656\n",
      " 0.00608958 0.66263009 0.37407921 0.00886149]\n",
      "\n",
      "# 29 Gradient out:  [-0.98427286 -1.26931898 -1.50963512 -0.7823064  -0.50177905 -0.70470423\n",
      " -1.15283946 -1.20632566 -1.39166031 -1.39196362]\n",
      "\n",
      "     Weights  out:  [ 0.02401211  0.18680564  0.38372482 -0.08416954 -0.26180588 -0.1281048\n",
      "  0.1162847   0.14763482  0.27388734  0.27412871] [ 0.48416912  0.50079014 -0.07790628  0.06761741  0.79568103 -0.03145945\n",
      " -0.09046212  0.56178625  0.25842574 -0.10681616]\n",
      "\n",
      "# 30 Gradient out:  [1.51888938 1.99172723 2.27216462 1.1551     0.74189139 1.02575655\n",
      " 1.81346964 1.89858911 2.14853057 2.14887994]\n",
      "\n",
      "     Weights  out:  [ 0.02401211  0.18680564  0.38372482 -0.08416954 -0.26180588 -0.1281048\n",
      "  0.1162847   0.14763482  0.27388734  0.27412871] [ 0.28731454  0.24692634 -0.37983331 -0.08884387  0.69532522 -0.1724003\n",
      " -0.32103001  0.32052111 -0.01990633 -0.38520888]\n",
      "\n",
      "# 31 Gradient out:  [-0.31055847 -0.3945283  -0.464887   -0.2508944  -0.16851952 -0.22803275\n",
      " -0.36030012 -0.37603432 -0.43035058 -0.43043929]\n",
      "\n",
      "     Weights  out:  [ 0.02401211  0.18680564  0.38372482 -0.08416954 -0.26180588 -0.1281048\n",
      "  0.1162847   0.14763482  0.27388734  0.27412871] [0.59109242 0.64527178 0.07459962 0.14217612 0.8437035  0.03275102\n",
      " 0.04166392 0.70023893 0.40979979 0.04456711]\n",
      "\n",
      "# 32 Gradient out:  [-0.63891455 -0.81941775 -0.97053189 -0.51071242 -0.33359692 -0.46156667\n",
      " -0.74581627 -0.77964662 -0.89643653 -0.89662716]\n",
      "\n",
      "     Weights  out:  [ 0.02401211  0.18680564  0.38372482 -0.08416954 -0.26180588 -0.1281048\n",
      "  0.1162847   0.14763482  0.27388734  0.27412871] [ 0.52898073  0.56636612 -0.01837778  0.09199724  0.8099996  -0.01285553\n",
      " -0.03039611  0.62503207  0.32372967 -0.04152075]\n",
      "\n",
      "# 33 Gradient out:  [-1.25485707 -1.61370604 -1.94475372 -1.00744314 -0.64182713 -0.90989751\n",
      " -1.46350566 -1.53169683 -1.77876804 -1.77918684]\n",
      "\n",
      "     Weights  out:  [ 0.02401211  0.18680564  0.38372482 -0.08416954 -0.26180588 -0.1281048\n",
      "  0.1162847   0.14763482  0.27388734  0.27412871] [ 0.40119782  0.40248258 -0.21248416 -0.01014524  0.74328022 -0.10516887\n",
      " -0.17955936  0.46910275  0.14444237 -0.22084618]\n",
      "\n",
      "# 34 Gradient out:  [1.64155564 1.99242642 2.26778807 1.38556052 1.05214951 1.2899587\n",
      " 1.85279833 1.91767816 2.13308828 2.13343207]\n",
      "\n",
      "     Weights  out:  [ 0.02401211  0.18680564  0.38372482 -0.08416954 -0.26180588 -0.1281048\n",
      "  0.1162847   0.14763482  0.27388734  0.27412871] [ 0.1502264   0.07974137 -0.60143491 -0.21163387  0.61491479 -0.28714837\n",
      " -0.47226049  0.16276338 -0.21131124 -0.57668355]\n",
      "\n",
      "# 35 Gradient out:  [-1.15018766 -1.48595896 -1.7719046  -0.9129886  -0.58126646 -0.82159133\n",
      " -1.34838387 -1.41147713 -1.63119524 -1.63155626]\n",
      "\n",
      "     Weights  out:  [ 0.02401211  0.18680564  0.38372482 -0.08416954 -0.26180588 -0.1281048\n",
      "  0.1162847   0.14763482  0.27388734  0.27412871] [ 0.47853753  0.47822665 -0.14787729  0.06547824  0.82534469 -0.02915663\n",
      " -0.10170083  0.54629901  0.21530642 -0.14999714]\n",
      "\n",
      "# 36 Gradient out:  [2.23279821 2.82735802 3.2471568  1.7899853  1.24283372 1.62771445\n",
      " 2.59553887 2.70438041 3.04935307 3.04987764]\n",
      "\n",
      "     Weights  out:  [ 0.02401211  0.18680564  0.38372482 -0.08416954 -0.26180588 -0.1281048\n",
      "  0.1162847   0.14763482  0.27388734  0.27412871] [ 0.2485      0.18103486 -0.50225821 -0.11711948  0.7090914  -0.1934749\n",
      " -0.3713776   0.26400359 -0.11093263 -0.47630839]\n",
      "\n",
      "# 37 Gradient out:  [-0.11061359 -0.13929269 -0.16340911 -0.09024467 -0.06208655 -0.0824371\n",
      " -0.12759732 -0.13297211 -0.15155157 -0.15158197]\n",
      "\n",
      "     Weights  out:  [ 0.02401211  0.18680564  0.38372482 -0.08416954 -0.26180588 -0.1281048\n",
      "  0.1162847   0.14763482  0.27388734  0.27412871] [0.69505964 0.74650647 0.14717315 0.24087758 0.95765814 0.13206799\n",
      " 0.14773017 0.80487967 0.49893798 0.13366714]\n",
      "\n",
      "# 38 Gradient out:  [-0.14722042 -0.18606665 -0.21867301 -0.11962214 -0.08150068 -0.10904629\n",
      " -0.17022968 -0.17750896 -0.20265299 -0.2026941 ]\n",
      "\n",
      "     Weights  out:  [ 0.02401211  0.18680564  0.38372482 -0.08416954 -0.26180588 -0.1281048\n",
      "  0.1162847   0.14763482  0.27388734  0.27412871] [0.67293692 0.71864793 0.11449133 0.22282864 0.94524083 0.11558057\n",
      " 0.12221071 0.77828525 0.46862767 0.10335075]\n",
      "\n",
      "# 39 Gradient out:  [-0.21458849 -0.27249305 -0.3209901  -0.17343675 -0.11664502 -0.15767156\n",
      " -0.24889385 -0.25974289 -0.29718489 -0.29724603]\n",
      "\n",
      "     Weights  out:  [ 0.02401211  0.18680564  0.38372482 -0.08416954 -0.26180588 -0.1281048\n",
      "  0.1162847   0.14763482  0.27388734  0.27412871] [0.64349284 0.6814346  0.07075673 0.19890421 0.9289407  0.09377131\n",
      " 0.08816477 0.74278346 0.42809707 0.06281193]\n",
      "\n",
      "# 40 Gradient out:  [-0.36615364 -0.46798726 -0.55309003 -0.29376819 -0.19393755 -0.26604138\n",
      " -0.42649328 -0.44557158 -0.51136284 -0.51147017]\n",
      "\n",
      "     Weights  out:  [ 0.02401211  0.18680564  0.38372482 -0.08416954 -0.26180588 -0.1281048\n",
      "  0.1162847   0.14763482  0.27388734  0.27412871] [0.60057514 0.62693599 0.0065587  0.16421686 0.90561169 0.062237\n",
      " 0.038386   0.69083488 0.36866009 0.00336272]\n",
      "\n",
      "# 41 Gradient out:  [-0.82543097 -1.06441027 -1.26498316 -0.65587177 -0.4210954  -0.59080644\n",
      " -0.96687587 -1.01168873 -1.16661609 -1.16686919]\n",
      "\n",
      "     Weights  out:  [ 0.02401211  0.18680564  0.38372482 -0.08416954 -0.26180588 -0.1281048\n",
      "  0.1162847   0.14763482  0.27388734  0.27412871] [ 0.52734441  0.53333854 -0.1040593   0.10546322  0.86682419  0.00902873\n",
      " -0.04691265  0.60172056  0.26638753 -0.09893131]\n",
      "\n",
      "# 42 Gradient out:  [-0.01939668  0.03403114 -0.01897873 -0.080256   -0.09061909 -0.09532166\n",
      "  0.02419486  0.03134909  0.01938835  0.01931871]\n",
      "\n",
      "     Weights  out:  [ 0.02401211  0.18680564  0.38372482 -0.08416954 -0.26180588 -0.1281048\n",
      "  0.1162847   0.14763482  0.27388734  0.27412871] [ 0.36225822  0.32045648 -0.35705593 -0.02571113  0.78260511 -0.10913256\n",
      " -0.24028783  0.39938282  0.03306431 -0.33230515]\n",
      "\n",
      "# 43 Gradient out:  [ 0.00686672  0.06361577  0.01443106 -0.05618832 -0.07016264 -0.07215176\n",
      "  0.05233683  0.06013277  0.05072454  0.05065965]\n",
      "\n",
      "     Weights  out:  [ 0.02401211  0.18680564  0.38372482 -0.08416954 -0.26180588 -0.1281048\n",
      "  0.1162847   0.14763482  0.27388734  0.27412871] [ 0.35837888  0.32726271 -0.36085168 -0.04176233  0.76448129 -0.12819689\n",
      " -0.23544885  0.40565263  0.03694198 -0.32844141]\n",
      "\n",
      "# 44 Gradient out:  [-0.1097586  -0.09328146 -0.17078793 -0.14300594 -0.11971548 -0.14797506\n",
      " -0.08877007 -0.08837251 -0.12133686 -0.12143739]\n",
      "\n",
      "     Weights  out:  [ 0.02401211  0.18680564  0.38372482 -0.08416954 -0.26180588 -0.1281048\n",
      "  0.1162847   0.14763482  0.27388734  0.27412871] [ 0.35975222  0.33998586 -0.35796547 -0.05299999  0.75044876 -0.14262724\n",
      " -0.22498149  0.41767919  0.04708689 -0.31830948]\n",
      "\n",
      "# 45 Gradient out:  [0.5076835  0.70588807 0.76543032 0.34164416 0.1933352  0.28711745\n",
      " 0.63823233 0.67222303 0.74943983 0.74951152]\n",
      "\n",
      "     Weights  out:  [ 0.02401211  0.18680564  0.38372482 -0.08416954 -0.26180588 -0.1281048\n",
      "  0.1162847   0.14763482  0.27388734  0.27412871] [ 0.3378005   0.32132957 -0.39212305 -0.08160118  0.72650566 -0.17222226\n",
      " -0.2427355   0.40000468  0.02281951 -0.34259696]\n",
      "\n",
      "# 46 Gradient out:  [-1.32670503 -1.713158   -2.05254279 -1.0562031  -0.66987994 -0.95106367\n",
      " -1.55351702 -1.62644693 -1.88432578 -1.88475466]\n",
      "\n",
      "     Weights  out:  [ 0.02401211  0.18680564  0.38372482 -0.08416954 -0.26180588 -0.1281048\n",
      "  0.1162847   0.14763482  0.27388734  0.27412871] [ 0.4393372   0.46250718 -0.23903699 -0.01327235  0.7651727  -0.11479877\n",
      " -0.11508904  0.53444929  0.17270748 -0.19269465]\n",
      "\n",
      "# 47 Gradient out:  [1.75096396 2.13433245 2.43227954 1.47072566 1.10750959 1.36625199\n",
      " 1.98205596 2.05288055 2.28702669 2.28739875]\n",
      "\n",
      "     Weights  out:  [ 0.02401211  0.18680564  0.38372482 -0.08416954 -0.26180588 -0.1281048\n",
      "  0.1162847   0.14763482  0.27388734  0.27412871] [ 0.1739962   0.11987559 -0.64954555 -0.22451297  0.63119671 -0.3050115\n",
      " -0.42579244  0.20915991 -0.20415767 -0.56964558]\n",
      "\n",
      "# 48 Gradient out:  [-0.9079269  -1.17137964 -1.39296794 -0.72112535 -0.46208891 -0.6493992\n",
      " -1.06379377 -1.11321118 -1.28424271 -1.28452236]\n",
      "\n",
      "     Weights  out:  [ 0.02401211  0.18680564  0.38372482 -0.08416954 -0.26180588 -0.1281048\n",
      "  0.1162847   0.14763482  0.27388734  0.27412871] [ 0.52418899  0.54674207 -0.16308964  0.06963216  0.85269863 -0.0317611\n",
      " -0.02938125  0.61973602  0.25324766 -0.11216583]\n",
      "\n",
      "# 49 Gradient out:  [0.804776   1.09699743 1.22753165 0.57016084 0.33291432 0.48999466\n",
      " 0.99194546 1.04333257 1.17771152 1.17787255]\n",
      "\n",
      "     Weights  out:  [ 0.02401211  0.18680564  0.38372482 -0.08416954 -0.26180588 -0.1281048\n",
      "  0.1162847   0.14763482  0.27388734  0.27412871] [ 0.34260361  0.31246615 -0.44168323 -0.07459291  0.76028085 -0.16164094\n",
      " -0.24214     0.39709378 -0.00360088 -0.36907031]\n",
      "\n",
      "Lineal error cuadrático medio (ECM): 1.3744512516514913\n",
      "\n",
      "# 0 Gradient out:  [2.4760022  3.43651007 1.09272769 1.47506763 3.12645301 3.04876472\n",
      " 1.42542943 3.40770342 1.01272925 1.1260014 ]\n",
      "\n",
      "     Weights  out:  [ 0.07036619  0.47052796 -0.34078593 -0.17926976  0.27412694  0.24120182\n",
      " -0.19558069  0.44564245 -0.39168355 -0.32211916] [-0.25465792 -0.41329462  0.11260342  0.38953679  0.46476817 -0.29630706\n",
      " -0.43572137 -0.29871959 -0.46267743 -0.27759379]\n",
      "\n",
      "# 1 Gradient out:  [-0.35597073 -0.49338041 -0.17748121 -0.23445681 -0.44270884 -0.43102482\n",
      " -0.22765319 -0.48856644 -0.1642166  -0.18288317]\n",
      "\n",
      "     Weights  out:  [ 0.07036619  0.47052796 -0.34078593 -0.17926976  0.27412694  0.24120182\n",
      " -0.19558069  0.44564245 -0.39168355 -0.32211916] [ 0.24054252  0.27400739  0.33114895  0.68455032  1.09005877  0.31344588\n",
      " -0.15063548  0.3828211  -0.26013158 -0.05239351]\n",
      "\n",
      "# 2 Gradient out:  [-0.65529955 -0.92812631 -0.29955144 -0.41275773 -0.82816018 -0.80494101\n",
      " -0.39920108 -0.91871297 -0.27337672 -0.31023946]\n",
      "\n",
      "     Weights  out:  [ 0.07036619  0.47052796 -0.34078593 -0.17926976  0.27412694  0.24120182\n",
      " -0.19558069  0.44564245 -0.39168355 -0.32211916] [ 0.16934837  0.17533131  0.29565271  0.63765896  1.001517    0.22724092\n",
      " -0.19616612  0.28510781 -0.2929749  -0.08897014]\n",
      "\n",
      "# 3 Gradient out:  [-1.31477833 -1.97710923 -0.448902   -0.72533636 -1.73671419 -1.67994192\n",
      " -0.69217883 -1.95508518 -0.38584039 -0.47483154]\n",
      "\n",
      "     Weights  out:  [ 0.07036619  0.47052796 -0.34078593 -0.17926976  0.27412694  0.24120182\n",
      " -0.19558069  0.44564245 -0.39168355 -0.32211916] [ 0.03828846 -0.01029395  0.23574243  0.55510741  0.83588497  0.06625272\n",
      " -0.27600634  0.10136522 -0.34765024 -0.15101804]\n",
      "\n",
      "# 4 Gradient out:  [2.57213245 3.55969792 1.14539012 1.5377291  3.24216999 3.16248645\n",
      " 1.48665098 3.5301433  1.06352552 1.17944474]\n",
      "\n",
      "     Weights  out:  [ 0.07036619  0.47052796 -0.34078593 -0.17926976  0.27412694  0.24120182\n",
      " -0.19558069  0.44564245 -0.39168355 -0.32211916] [-0.22466721 -0.4057158   0.14596203  0.41004014  0.48854213 -0.26973567\n",
      " -0.4144421  -0.28965182 -0.42481832 -0.24598434]\n",
      "\n",
      "# 5 Gradient out:  [-0.24839957 -0.3427969  -0.12588721 -0.16501927 -0.30793492 -0.2999105\n",
      " -0.16034932 -0.33947745 -0.11676192 -0.12960101]\n",
      "\n",
      "     Weights  out:  [ 0.07036619  0.47052796 -0.34078593 -0.17926976  0.27412694  0.24120182\n",
      " -0.19558069  0.44564245 -0.39168355 -0.32211916] [ 0.28975928  0.30622379  0.37504005  0.71758596  1.13697613  0.36276162\n",
      " -0.11711191  0.41637684 -0.21211322 -0.0100954 ]\n",
      "\n",
      "# 6 Gradient out:  [-0.394324   -0.55094912 -0.19058984 -0.25553599 -0.49331557 -0.47999683\n",
      " -0.24777233 -0.54548662 -0.17550273 -0.19673876]\n",
      "\n",
      "     Weights  out:  [ 0.07036619  0.47052796 -0.34078593 -0.17926976  0.27412694  0.24120182\n",
      " -0.19558069  0.44564245 -0.39168355 -0.32211916] [ 0.24007937  0.23766441  0.34986261  0.68458211  1.07538914  0.30277952\n",
      " -0.14918177  0.34848135 -0.2354656  -0.0360156 ]\n",
      "\n",
      "# 7 Gradient out:  [-0.76464162 -1.0934625  -0.33532842 -0.4718121  -0.97325211 -0.94525655\n",
      " -0.45545204 -1.08217994 -0.30385037 -0.34819463]\n",
      "\n",
      "     Weights  out:  [ 0.07036619  0.47052796 -0.34078593 -0.17926976  0.27412694  0.24120182\n",
      " -0.19558069  0.44564245 -0.39168355 -0.32211916] [ 0.16121457  0.12747458  0.31174464  0.63347491  0.97672603  0.20678016\n",
      " -0.19873624  0.23938403 -0.27056614 -0.07536335]\n",
      "\n",
      "# 8 Gradient out:  [-1.24082749 -1.92403914 -0.35903224 -0.64631823 -1.67304354 -1.61394591\n",
      " -0.61219718 -1.90123301 -0.29303811 -0.38617765]\n",
      "\n",
      "     Weights  out:  [ 0.07036619  0.47052796 -0.34078593 -0.17926976  0.27412694  0.24120182\n",
      " -0.19558069  0.44564245 -0.39168355 -0.32211916] [ 0.00828624 -0.09121792  0.24467896  0.53911249  0.78207561  0.01772885\n",
      " -0.28982664  0.02294804 -0.33133622 -0.14500228]\n",
      "\n",
      "# 9 Gradient out:  [2.40702881 3.32893503 1.09319532 1.46094498 3.02615134 2.95139158\n",
      " 1.41360671 3.30044846 1.01500277 1.12555955]\n",
      "\n",
      "     Weights  out:  [ 0.07036619  0.47052796 -0.34078593 -0.17926976  0.27412694  0.24120182\n",
      " -0.19558069  0.44564245 -0.39168355 -0.32211916] [-0.23987925 -0.47602575  0.17287251  0.40984885  0.4474669  -0.30506033\n",
      " -0.41226608 -0.35729856 -0.38994384 -0.22223781]\n",
      "\n",
      "# 10 Gradient out:  [-0.43733131 -0.61843958 -0.20125042 -0.27635111 -0.55201282 -0.53661145\n",
      " -0.26735896 -0.61216523 -0.18386175 -0.20834547]\n",
      "\n",
      "     Weights  out:  [ 0.07036619  0.47052796 -0.34078593 -0.17926976  0.27412694  0.24120182\n",
      " -0.19558069  0.44564245 -0.39168355 -0.32211916] [ 0.24152651  0.18976126  0.39151157  0.70203784  1.05269717  0.28521798\n",
      " -0.12954474  0.30279113 -0.18694329  0.0028741 ]\n",
      "\n",
      "# 11 Gradient out:  [-0.89891665 -1.30035714 -0.37436005 -0.54106911 -1.15385707 -1.11965769\n",
      " -0.52107325 -1.28665098 -0.33599318 -0.39005645]\n",
      "\n",
      "     Weights  out:  [ 0.07036619  0.47052796 -0.34078593 -0.17926976  0.27412694  0.24120182\n",
      " -0.19558069  0.44564245 -0.39168355 -0.32211916] [ 0.15406025  0.06607334  0.35126149  0.64676762  0.9422946   0.17789569\n",
      " -0.18301653  0.18035808 -0.22371564 -0.03879499]\n",
      "\n",
      "# 12 Gradient out:  [-0.62747865 -1.06747739 -0.10666935 -0.29793894 -0.89115098 -0.85152553\n",
      " -0.27659929 -1.05139428 -0.05987471 -0.12572476]\n",
      "\n",
      "     Weights  out:  [ 0.07036619  0.47052796 -0.34078593 -0.17926976  0.27412694  0.24120182\n",
      " -0.19558069  0.44564245 -0.39168355 -0.32211916] [-0.02572308 -0.19399808  0.27638948  0.5385538   0.71152319 -0.04603584\n",
      " -0.28723118 -0.07697211 -0.29091427 -0.11680628]\n",
      "\n",
      "# 13 Gradient out:  [2.42268728 3.33813462 1.07850828 1.43937215 3.05059537 2.97743766\n",
      " 1.39172479 3.31139721 1.0046087  1.10935218]\n",
      "\n",
      "     Weights  out:  [ 0.07036619  0.47052796 -0.34078593 -0.17926976  0.27412694  0.24120182\n",
      " -0.19558069  0.44564245 -0.39168355 -0.32211916] [-0.15121881 -0.40749356  0.25505561  0.47896601  0.53329299 -0.21634095\n",
      " -0.34255104 -0.28725097 -0.30288921 -0.14195124]\n",
      "\n",
      "# 14 Gradient out:  [-0.21434073 -0.30100513 -0.10148514 -0.13740536 -0.26915612 -0.26179054\n",
      " -0.13310758 -0.29798634 -0.09314931 -0.10488307]\n",
      "\n",
      "     Weights  out:  [ 0.07036619  0.47052796 -0.34078593 -0.17926976  0.27412694  0.24120182\n",
      " -0.19558069  0.44564245 -0.39168355 -0.32211916] [ 0.33331864  0.26013336  0.47075726  0.76684044  1.14341207  0.37914658\n",
      " -0.06420608  0.37502847 -0.10196747  0.0799192 ]\n",
      "\n",
      "# 15 Gradient out:  [-0.32531747 -0.46068694 -0.14878497 -0.20490328 -0.41105548 -0.39954765\n",
      " -0.19818161 -0.45599686 -0.13579424 -0.15408533]\n",
      "\n",
      "     Weights  out:  [ 0.07036619  0.47052796 -0.34078593 -0.17926976  0.27412694  0.24120182\n",
      " -0.19558069  0.44564245 -0.39168355 -0.32211916] [ 0.2904505   0.19993234  0.45046024  0.73935937  1.08958084  0.32678847\n",
      " -0.0908276   0.31543121 -0.12059734  0.05894259]\n",
      "\n",
      "# 16 Gradient out:  [-0.59080201 -0.84798361 -0.25482477 -0.36150822 -0.7539994  -0.73211932\n",
      " -0.34871329 -0.83914884 -0.23022038 -0.26487899]\n",
      "\n",
      "     Weights  out:  [ 0.07036619  0.47052796 -0.34078593 -0.17926976  0.27412694  0.24120182\n",
      " -0.19558069  0.44564245 -0.39168355 -0.32211916] [ 0.225387    0.10779495  0.42070324  0.69837871  1.00736975  0.24687894\n",
      " -0.13046392  0.22423183 -0.14775618  0.02812552]\n",
      "\n",
      "# 17 Gradient out:  [-1.30983218 -1.93578488 -0.49564917 -0.75649336 -1.70653586 -1.65298397\n",
      " -0.72531737 -1.91446256 -0.43554876 -0.52025915]\n",
      "\n",
      "     Weights  out:  [ 0.07036619  0.47052796 -0.34078593 -0.17926976  0.27412694  0.24120182\n",
      " -0.19558069  0.44564245 -0.39168355 -0.32211916] [ 0.1072266  -0.06180177  0.36973829  0.62607707  0.85656987  0.10045508\n",
      " -0.20020658  0.05640207 -0.19380026 -0.02485028]\n",
      "\n",
      "# 18 Gradient out:  [2.5297807  3.49658169 1.12652024 1.50934706 3.18745352 3.10976158\n",
      " 1.45929714 3.46768088 1.0469237  1.15962553]\n",
      "\n",
      "     Weights  out:  [ 0.07036619  0.47052796 -0.34078593 -0.17926976  0.27412694  0.24120182\n",
      " -0.19558069  0.44564245 -0.39168355 -0.32211916] [-0.15473983 -0.44895875  0.27060845  0.4747784   0.51526269 -0.23014171\n",
      " -0.34527005 -0.32649045 -0.28091001 -0.12890211]\n",
      "\n",
      "# 19 Gradient out:  [-0.19987607 -0.28162794 -0.09334661 -0.12722907 -0.2516133  -0.24466563\n",
      " -0.12317301 -0.27878546 -0.08549108 -0.09654973]\n",
      "\n",
      "     Weights  out:  [ 0.07036619  0.47052796 -0.34078593 -0.17926976  0.27412694  0.24120182\n",
      " -0.19558069  0.44564245 -0.39168355 -0.32211916] [ 0.35121631  0.25035759  0.4959125   0.77664781  1.1527534   0.3918106\n",
      " -0.05341062  0.36704573 -0.07152527  0.103023  ]\n",
      "\n",
      "# 20 Gradient out:  [-0.29628739 -0.42055889 -0.13414713 -0.18566063 -0.37502751 -0.36446413\n",
      " -0.17948811 -0.41625829 -0.12222996 -0.13901037]\n",
      "\n",
      "     Weights  out:  [ 0.07036619  0.47052796 -0.34078593 -0.17926976  0.27412694  0.24120182\n",
      " -0.19558069  0.44564245 -0.39168355 -0.32211916] [ 0.31124109  0.194032    0.47724318  0.751202    1.10243074  0.34287748\n",
      " -0.07804523  0.31128864 -0.08862349  0.08371305]\n",
      "\n",
      "# 21 Gradient out:  [-0.51698805 -0.74219066 -0.2227177  -0.31611582 -0.65990677 -0.64075232\n",
      " -0.30491199 -0.73445217 -0.20117801 -0.23151893]\n",
      "\n",
      "     Weights  out:  [ 0.07036619  0.47052796 -0.34078593 -0.17926976  0.27412694  0.24120182\n",
      " -0.19558069  0.44564245 -0.39168355 -0.32211916] [ 0.25198361  0.10992022  0.45041375  0.71406987  1.02742523  0.26998465\n",
      " -0.11394285  0.22803698 -0.11306948  0.05591098]\n",
      "\n",
      "# 22 Gradient out:  [-1.14322682 -1.67749423 -0.44645526 -0.66866839 -1.48225203 -1.4366504\n",
      " -0.64205401 -1.65927776 -0.3953012  -0.4673931 ]\n",
      "\n",
      "     Weights  out:  [ 0.07036619  0.47052796 -0.34078593 -0.17926976  0.27412694  0.24120182\n",
      " -0.19558069  0.44564245 -0.39168355 -0.32211916] [ 0.148586   -0.03851791  0.40587021  0.65084671  0.89544388  0.14183419\n",
      " -0.17492525  0.08114655 -0.15330508  0.00960719]\n",
      "\n",
      "# 23 Gradient out:  [1.55513103 2.10855542 0.68245537 0.89381364 1.95444705 1.91192155\n",
      " 0.86406435 2.09464326 0.64343198 0.69912917]\n",
      "\n",
      "     Weights  out:  [ 0.07036619  0.47052796 -0.34078593 -0.17926976  0.27412694  0.24120182\n",
      " -0.19558069  0.44564245 -0.39168355 -0.32211916] [-0.08005936 -0.37401676  0.31657916  0.51711303  0.59899347 -0.14549589\n",
      " -0.30333605 -0.250709   -0.23236532 -0.08387143]\n",
      "\n",
      "# 24 Gradient out:  [-0.68331083 -0.98921915 -0.2833427  -0.4102744  -0.87760349 -0.8515689\n",
      " -0.39504117 -0.97875209 -0.25412065 -0.29529245]\n",
      "\n",
      "     Weights  out:  [ 0.07036619  0.47052796 -0.34078593 -0.17926976  0.27412694  0.24120182\n",
      " -0.19558069  0.44564245 -0.39168355 -0.32211916] [ 0.23096684  0.04769433  0.45307023  0.69587576  0.98988288  0.23688842\n",
      " -0.13052318  0.16821965 -0.10367893  0.05595441]\n",
      "\n",
      "# 25 Gradient out:  [-1.42323508 -2.12603037 -0.52069935 -0.8149684  -1.86491943 -1.804442\n",
      " -0.78013559 -2.10169087 -0.4521488  -0.54871209]\n",
      "\n",
      "     Weights  out:  [ 0.07036619  0.47052796 -0.34078593 -0.17926976  0.27412694  0.24120182\n",
      " -0.19558069  0.44564245 -0.39168355 -0.32211916] [ 0.09430468 -0.1501495   0.39640169  0.61382088  0.81436219  0.06657464\n",
      " -0.20953141 -0.02753077 -0.15450306 -0.00310408]\n",
      "\n",
      "# 26 Gradient out:  [2.36725945 3.25419377 1.12337533 1.47797449 2.95509419 2.88297107\n",
      " 1.43291152 3.2254408  1.04605667 1.15512265]\n",
      "\n",
      "     Weights  out:  [ 0.07036619  0.47052796 -0.34078593 -0.17926976  0.27412694  0.24120182\n",
      " -0.19558069  0.44564245 -0.39168355 -0.32211916] [-0.19034234 -0.57535558  0.29226182  0.4508272   0.4413783  -0.29431376\n",
      " -0.36555853 -0.44786895 -0.24493281 -0.1128465 ]\n",
      "\n",
      "# 27 Gradient out:  [-0.48109414 -0.69625279 -0.19949559 -0.28871678 -0.61782416 -0.59952676\n",
      " -0.27800052 -0.68889198 -0.1789666  -0.20789006]\n",
      "\n",
      "     Weights  out:  [ 0.07036619  0.47052796 -0.34078593 -0.17926976  0.27412694  0.24120182\n",
      " -0.19558069  0.44564245 -0.39168355 -0.32211916] [ 0.28310955  0.07548318  0.51693689  0.74642209  1.03239714  0.28228045\n",
      " -0.07897623  0.19721921 -0.03572148  0.11817803]\n",
      "\n",
      "# 28 Gradient out:  [-1.05469133 -1.54993662 -0.40870816 -0.61459257 -1.3689252  -1.32667886\n",
      " -0.58992908 -1.53301888 -0.36129164 -0.42810926]\n",
      "\n",
      "     Weights  out:  [ 0.07036619  0.47052796 -0.34078593 -0.17926976  0.27412694  0.24120182\n",
      " -0.19558069  0.44564245 -0.39168355 -0.32211916] [ 0.18689072 -0.06376738  0.47703777  0.68867874  0.90883231  0.1623751\n",
      " -0.13457633  0.05944082 -0.0715148   0.07660002]\n",
      "\n",
      "# 29 Gradient out:  [0.67740416 0.88671144 0.25943829 0.32998962 0.85775916 0.84402402\n",
      " 0.31729215 0.8849719  0.25303239 0.26287489]\n",
      "\n",
      "     Weights  out:  [ 0.07036619  0.47052796 -0.34078593 -0.17926976  0.27412694  0.24120182\n",
      " -0.19558069  0.44564245 -0.39168355 -0.32211916] [-0.02404754 -0.3737547   0.39529614  0.56576022  0.63504727 -0.10296067\n",
      " -0.25256215 -0.24716296 -0.14377313 -0.00902183]\n",
      "\n",
      "# 30 Gradient out:  [-1.43241587 -2.13888077 -0.53287292 -0.82920855 -1.87360556 -1.81267979\n",
      " -0.79435198 -2.11398729 -0.46319615 -0.56127242]\n",
      "\n",
      "     Weights  out:  [ 0.07036619  0.47052796 -0.34078593 -0.17926976  0.27412694  0.24120182\n",
      " -0.19558069  0.44564245 -0.39168355 -0.32211916] [ 0.11143329 -0.19641241  0.4471838   0.63175815  0.8065991   0.06584413\n",
      " -0.18910372 -0.07016858 -0.09316665  0.04355314]\n",
      "\n",
      "# 31 Gradient out:  [2.2975122  3.1485879  1.11516529 1.45591753 2.8572493  2.7879201\n",
      " 1.41294031 3.12026117 1.03980705 1.14597246]\n",
      "\n",
      "     Weights  out:  [ 0.07036619  0.47052796 -0.34078593 -0.17926976  0.27412694  0.24120182\n",
      " -0.19558069  0.44564245 -0.39168355 -0.32211916] [-0.17504988 -0.62418857  0.34060921  0.46591644  0.43187798 -0.29669183\n",
      " -0.34797411 -0.49296604 -0.18580588 -0.06870134]\n",
      "\n",
      "# 32 Gradient out:  [-0.57951335 -0.8473625  -0.22853728 -0.33964131 -0.74993621 -0.72714945\n",
      " -0.32628439 -0.83824644 -0.20303408 -0.23897546]\n",
      "\n",
      "     Weights  out:  [ 0.07036619  0.47052796 -0.34078593 -0.17926976  0.27412694  0.24120182\n",
      " -0.19558069  0.44564245 -0.39168355 -0.32211916] [ 0.28445256  0.00552901  0.56364227  0.75709995  1.00332785  0.26089219\n",
      " -0.06538605  0.1310862   0.02215553  0.16049315]\n",
      "\n",
      "# 33 Gradient out:  [-1.31625993 -1.95201701 -0.4950583  -0.76023762 -1.71699237 -1.66253915\n",
      " -0.72870474 -1.92998322 -0.43343307 -0.52022517]\n",
      "\n",
      "     Weights  out:  [ 0.07036619  0.47052796 -0.34078593 -0.17926976  0.27412694  0.24120182\n",
      " -0.19558069  0.44564245 -0.39168355 -0.32211916] [ 0.16854989 -0.16394349  0.51793482  0.68917168  0.8533406   0.1154623\n",
      " -0.13064293 -0.03656309 -0.01845129  0.11269806]\n",
      "\n",
      "# 34 Gradient out:  [2.38293372 3.32798214 1.01800024 1.39270182 3.02338562 2.94731952\n",
      " 1.34391236 3.29936636 0.93952956 1.05057048]\n",
      "\n",
      "     Weights  out:  [ 0.07036619  0.47052796 -0.34078593 -0.17926976  0.27412694  0.24120182\n",
      " -0.19558069  0.44564245 -0.39168355 -0.32211916] [-0.0947021  -0.55434689  0.41892315  0.53712416  0.50994213 -0.21704553\n",
      " -0.27638388 -0.42255974 -0.1051379   0.00865302]\n",
      "\n",
      "# 35 Gradient out:  [-0.25239366 -0.36611786 -0.10328375 -0.1504134  -0.32475398 -0.31508996\n",
      " -0.1447447  -0.3622377  -0.09245892 -0.10771177]\n",
      "\n",
      "     Weights  out:  [ 0.07036619  0.47052796 -0.34078593 -0.17926976  0.27412694  0.24120182\n",
      " -0.19558069  0.44564245 -0.39168355 -0.32211916] [ 0.38188464  0.11124954  0.6225232   0.81566452  1.11461925  0.37241838\n",
      " -0.00760141  0.23731354  0.08276801  0.21876712]\n",
      "\n",
      "# 36 Gradient out:  [-0.41593122 -0.60726995 -0.16495862 -0.24428534 -0.53774579 -0.52147821\n",
      " -0.23474121 -0.60076242 -0.1467623  -0.17240652]\n",
      "\n",
      "     Weights  out:  [ 0.07036619  0.47052796 -0.34078593 -0.17926976  0.27412694  0.24120182\n",
      " -0.19558069  0.44564245 -0.39168355 -0.32211916] [ 0.33140591  0.03802597  0.60186645  0.78558184  1.04966846  0.30940038\n",
      " -0.03655035  0.164866    0.06427623  0.19722477]\n",
      "\n",
      "# 37 Gradient out:  [-0.86733235 -1.27871405 -0.32951325 -0.50037762 -1.12875007 -1.09369659\n",
      " -0.47987228 -1.26469939 -0.2902424  -0.34558706]\n",
      "\n",
      "     Weights  out:  [ 0.07036619  0.47052796 -0.34078593 -0.17926976  0.27412694  0.24120182\n",
      " -0.19558069  0.44564245 -0.39168355 -0.32211916] [ 0.24821967 -0.08342802  0.56887473  0.73672477  0.9421193   0.20510474\n",
      " -0.08349859  0.04471351  0.03492377  0.16274346]\n",
      "\n",
      "# 38 Gradient out:  [-0.88693647 -1.34509036 -0.38432673 -0.58417205 -1.14533901 -1.10390997\n",
      " -0.5630006  -1.32532766 -0.33133939 -0.40534761]\n",
      "\n",
      "     Weights  out:  [ 0.07036619  0.47052796 -0.34078593 -0.17926976  0.27412694  0.24120182\n",
      " -0.19558069  0.44564245 -0.39168355 -0.32211916] [ 0.0747532  -0.33917083  0.50297208  0.63664925  0.71636928 -0.01363458\n",
      " -0.17947304 -0.20822636 -0.02312471  0.09362605]\n",
      "\n",
      "# 39 Gradient out:  [2.43985547 3.39047305 1.09321715 1.47199853 3.074522   2.99754002\n",
      " 1.4234599  3.36025636 1.01165053 1.12680866]\n",
      "\n",
      "     Weights  out:  [ 0.07036619  0.47052796 -0.34078593 -0.17926976  0.27412694  0.24120182\n",
      " -0.19558069  0.44564245 -0.39168355 -0.32211916] [-0.1026341  -0.60818891  0.42610673  0.51981484  0.48730148 -0.23441657\n",
      " -0.29207317 -0.4732919  -0.08939259  0.01255653]\n",
      "\n",
      "# 40 Gradient out:  [-0.28422013 -0.41452223 -0.11319093 -0.16719318 -0.36720911 -0.35613571\n",
      " -0.16069249 -0.41009244 -0.10080924 -0.11825895]\n",
      "\n",
      "     Weights  out:  [ 0.07036619  0.47052796 -0.34078593 -0.17926976  0.27412694  0.24120182\n",
      " -0.19558069  0.44564245 -0.39168355 -0.32211916] [ 0.385337    0.0699057   0.64475016  0.81421455  1.10220588  0.36509144\n",
      " -0.00738119  0.19875937  0.11293752  0.23791826]\n",
      "\n",
      "# 41 Gradient out:  [-0.49518274 -0.7270011  -0.19096989 -0.28710202 -0.64285105 -0.62313606\n",
      " -0.27553169 -0.7191378  -0.16894434 -0.19998955]\n",
      "\n",
      "     Weights  out:  [ 0.07036619  0.47052796 -0.34078593 -0.17926976  0.27412694  0.24120182\n",
      " -0.19558069  0.44564245 -0.39168355 -0.32211916] [ 0.32849297 -0.01299874  0.62211198  0.78077591  1.02876406  0.29386429\n",
      " -0.03951968  0.11674089  0.09277567  0.21426647]\n",
      "\n",
      "# 42 Gradient out:  [-1.10488356 -1.63802659 -0.4115397  -0.63338798 -1.44247985 -1.39694817\n",
      " -0.6068703  -1.61972478 -0.36030417 -0.43248994]\n",
      "\n",
      "     Weights  out:  [ 0.07036619  0.47052796 -0.34078593 -0.17926976  0.27412694  0.24120182\n",
      " -0.19558069  0.44564245 -0.39168355 -0.32211916] [ 0.22945642 -0.15839896  0.583918    0.7233555   0.90019385  0.16923708\n",
      " -0.09462602 -0.02708668  0.0589868   0.17426856]\n",
      "\n",
      "# 43 Gradient out:  [1.06876563 1.51331629 0.31370684 0.48000956 1.40947141 1.37617949\n",
      " 0.45496373 1.50522602 0.28782755 0.32540638]\n",
      "\n",
      "     Weights  out:  [ 0.07036619  0.47052796 -0.34078593 -0.17926976  0.27412694  0.24120182\n",
      " -0.19558069  0.44564245 -0.39168355 -0.32211916] [ 0.00847971 -0.48600428  0.50161006  0.59667791  0.61169788 -0.11015255\n",
      " -0.21600008 -0.35103163 -0.01307403  0.08777057]\n",
      "\n",
      "# 44 Gradient out:  [-1.20695897 -1.78983861 -0.45106672 -0.69383786 -1.57533494 -1.52549893\n",
      " -0.66488182 -1.76974374 -0.39484777 -0.47404129]\n",
      "\n",
      "     Weights  out:  [ 0.07036619  0.47052796 -0.34078593 -0.17926976  0.27412694  0.24120182\n",
      " -0.19558069  0.44564245 -0.39168355 -0.32211916] [ 0.22223284 -0.18334102  0.56435143  0.69267982  0.89359216  0.16508335\n",
      " -0.12500734 -0.04998643  0.04449148  0.15285185]\n",
      "\n",
      "# 45 Gradient out:  [1.85207967 2.6175945  0.68840061 0.9872128  2.39145897 2.33101973\n",
      " 0.94656984 2.59733781 0.63056253 0.71293604]\n",
      "\n",
      "     Weights  out:  [ 0.07036619  0.47052796 -0.34078593 -0.17926976  0.27412694  0.24120182\n",
      " -0.19558069  0.44564245 -0.39168355 -0.32211916] [-0.01915896 -0.54130874  0.47413808  0.55391225  0.57852517 -0.14001644\n",
      " -0.2579837  -0.40393518 -0.03447808  0.05804359]\n",
      "\n",
      "# 46 Gradient out:  [-0.47195509 -0.69101777 -0.18462766 -0.27546368 -0.61142908 -0.59280056\n",
      " -0.26453514 -0.68357259 -0.16379617 -0.19315538]\n",
      "\n",
      "     Weights  out:  [ 0.07036619  0.47052796 -0.34078593 -0.17926976  0.27412694  0.24120182\n",
      " -0.19558069  0.44564245 -0.39168355 -0.32211916] [ 0.35125698 -0.01778984  0.61181821  0.75135481  1.05681697  0.32618751\n",
      " -0.06866973  0.11553239  0.09163443  0.2006308 ]\n",
      "\n",
      "# 47 Gradient out:  [-1.03725656 -1.53443788 -0.38920785 -0.59593593 -1.35256868 -1.31014732\n",
      " -0.57118321 -1.51743042 -0.34156619 -0.40869766]\n",
      "\n",
      "     Weights  out:  [ 0.07036619  0.47052796 -0.34078593 -0.17926976  0.27412694  0.24120182\n",
      " -0.19558069  0.44564245 -0.39168355 -0.32211916] [ 0.25686596 -0.1559934   0.57489267  0.69626207  0.93453115  0.20762739\n",
      " -0.12157676 -0.02118213  0.0588752   0.16199972]\n",
      "\n",
      "# 48 Gradient out:  [0.46890828 0.63622316 0.09068678 0.14375292 0.62893613 0.61878888\n",
      " 0.13272217 0.63715193 0.08995281 0.09203742]\n",
      "\n",
      "     Weights  out:  [ 0.07036619  0.47052796 -0.34078593 -0.17926976  0.27412694  0.24120182\n",
      " -0.19558069  0.44564245 -0.39168355 -0.32211916] [ 0.04941465 -0.46288097  0.4970511   0.57707489  0.66401741 -0.05440207\n",
      " -0.2358134  -0.32466822 -0.00943804  0.08026019]\n",
      "\n",
      "# 49 Gradient out:  [-1.27449948 -1.91293419 -0.49994053 -0.77123023 -1.65992053 -1.60398874\n",
      " -0.74042731 -1.8886574  -0.43324976 -0.52683598]\n",
      "\n",
      "     Weights  out:  [ 0.07036619  0.47052796 -0.34078593 -0.17926976  0.27412694  0.24120182\n",
      " -0.19558069  0.44564245 -0.39168355 -0.32211916] [ 0.1431963  -0.33563634  0.51518846  0.60582547  0.78980464  0.06935571\n",
      " -0.20926897 -0.19723783  0.00855252  0.09866767]\n",
      "\n",
      "Tanh error cuadrático medio (ECM): 1.1624124788744625\n",
      "\n",
      "# 0 Gradient out:  [-0.72832131 -2.08155576 -2.06553252 -0.43094383 -2.11486835 -1.63494025\n",
      " -1.78186983 -0.31142151 -2.11929178 -0.4817339 ]\n",
      "\n",
      "     Weights  out:  [-0.20643456  0.43580045  0.42122149 -0.38152059  0.46891848  0.16725625\n",
      "  0.23690567 -0.49345469  0.47364968 -0.34470237] [-0.08231293  0.18072119  0.04259057  0.08834867  0.37331259  0.21654617\n",
      "  0.47998328  0.47549966 -0.44291015 -0.10506518]\n",
      "\n",
      "# 1 Gradient out:  [0.98431442 1.98603646 1.97453864 0.783093   2.01071666 1.67935966\n",
      " 1.78247834 0.69626074 2.01409104 0.81749951]\n",
      "\n",
      "     Weights  out:  [-0.20643456  0.43580045  0.42122149 -0.38152059  0.46891848  0.16725625\n",
      "  0.23690567 -0.49345469  0.47364968 -0.34470237] [-0.2279772  -0.23558996 -0.37051594  0.0021599  -0.04966108 -0.11044188\n",
      "  0.12360931  0.41321536 -0.8667685  -0.20141196]\n",
      "\n",
      "# 2 Gradient out:  [-0.75721173 -2.10139793 -2.0850566  -0.45822237 -2.13545604 -1.65244904\n",
      " -1.79888419 -0.33625353 -2.13998784 -0.50967839]\n",
      "\n",
      "     Weights  out:  [-0.20643456  0.43580045  0.42122149 -0.38152059  0.46891848  0.16725625\n",
      "  0.23690567 -0.49345469  0.47364968 -0.34470237] [-0.03111431  0.16161733  0.02439179  0.1587785   0.35248226  0.22543005\n",
      "  0.48010498  0.55246751 -0.46395029 -0.03791206]\n",
      "\n",
      "# 3 Gradient out:  [0.99185779 1.93600115 1.92468991 0.79874027 1.9603711  1.6416979\n",
      " 1.73924604 0.71324191 1.96371266 0.83221085]\n",
      "\n",
      "     Weights  out:  [-0.20643456  0.43580045  0.42122149 -0.38152059  0.46891848  0.16725625\n",
      "  0.23690567 -0.49345469  0.47364968 -0.34470237] [-0.18255666 -0.25866226 -0.39261953  0.06713403 -0.07460895 -0.10505976\n",
      "  0.12032814  0.4852168  -0.89194786 -0.13984774]\n",
      "\n",
      "# 4 Gradient out:  [-0.74686253 -2.00856635 -1.9923209  -0.45706998 -2.04251933 -1.57402143\n",
      " -1.7129928  -0.33577138 -2.04704662 -0.5077314 ]\n",
      "\n",
      "     Weights  out:  [-0.20643456  0.43580045  0.42122149 -0.38152059  0.46891848  0.16725625\n",
      "  0.23690567 -0.49345469  0.47364968 -0.34470237] [ 0.0158149   0.12853797 -0.00768154  0.22688208  0.31746527  0.22327982\n",
      "  0.46817735  0.62786519 -0.49920533  0.02659443]\n",
      "\n",
      "# 5 Gradient out:  [1.0356661  1.98784112 1.97606043 0.83820975 2.01329204 1.68693466\n",
      " 1.78558375 0.74910428 2.01678919 0.8727807 ]\n",
      "\n",
      "     Weights  out:  [-0.20643456  0.43580045  0.42122149 -0.38152059  0.46891848  0.16725625\n",
      "  0.23690567 -0.49345469  0.47364968 -0.34470237] [-0.13355761 -0.2731753  -0.40614573  0.13546809 -0.0910386  -0.09152447\n",
      "  0.12557879  0.56071091 -0.90861466 -0.07495185]\n",
      "\n",
      "# 6 Gradient out:  [-0.78226188 -2.08414776 -2.06740417 -0.48473782 -2.11921036 -1.63763715\n",
      " -1.78057426 -0.35962656 -2.12389413 -0.53676981]\n",
      "\n",
      "     Weights  out:  [-0.20643456  0.43580045  0.42122149 -0.38152059  0.46891848  0.16725625\n",
      "  0.23690567 -0.49345469  0.47364968 -0.34470237] [ 0.07357561  0.12439293 -0.01093364  0.30311004  0.31161981  0.24586247\n",
      "  0.48269554  0.71053177 -0.50525682  0.09960429]\n",
      "\n",
      "# 7 Gradient out:  [1.05330975 1.98666921 1.97468784 0.85661427 2.01263128 1.68693555\n",
      " 1.78395333 0.76592584 2.01620688 0.89145082]\n",
      "\n",
      "     Weights  out:  [-0.20643456  0.43580045  0.42122149 -0.38152059  0.46891848  0.16725625\n",
      "  0.23690567 -0.49345469  0.47364968 -0.34470237] [-0.08287676 -0.29243663 -0.42441447  0.20616247 -0.11222226 -0.08166496\n",
      "  0.12658069  0.63860646 -0.93003564 -0.00774967]\n",
      "\n",
      "# 8 Gradient out:  [-0.77976527 -2.04353646 -2.02672459 -0.48591126 -2.07882659 -1.60273067\n",
      " -1.74222417 -0.36022427 -2.08354987 -0.53778755]\n",
      "\n",
      "     Weights  out:  [-0.20643456  0.43580045  0.42122149 -0.38152059  0.46891848  0.16725625\n",
      "  0.23690567 -0.49345469  0.47364968 -0.34470237] [ 0.12778519  0.10489721 -0.02947691  0.37748533  0.29030399  0.25572215\n",
      "  0.48337136  0.79179162 -0.52679427  0.17054049]\n",
      "\n",
      "# 9 Gradient out:  [1.08739192 2.05585865 2.04311809 0.8810693  2.08351916 1.74146212\n",
      " 1.84235252 0.7845898  2.0873343  0.9178907 ]\n",
      "\n",
      "     Weights  out:  [-0.20643456  0.43580045  0.42122149 -0.38152059  0.46891848  0.16725625\n",
      "  0.23690567 -0.49345469  0.47364968 -0.34470237] [-0.02816787 -0.30381008 -0.43482182  0.28030308 -0.12546133 -0.06482399\n",
      "  0.13492652  0.71974677 -0.94350424  0.06298298]\n",
      "\n",
      "# 10 Gradient out:  [-0.77619764 -2.07615732 -2.05928393 -0.47884161 -2.11157709 -1.62963991\n",
      " -1.77220713 -0.35265866 -2.11631877 -0.53100742]\n",
      "\n",
      "     Weights  out:  [-0.20643456  0.43580045  0.42122149 -0.38152059  0.46891848  0.16725625\n",
      "  0.23690567 -0.49345469  0.47364968 -0.34470237] [ 0.18931052  0.10736165 -0.0261982   0.45651693  0.29124251  0.28346844\n",
      "  0.50339703  0.87666473 -0.52603738  0.24656112]\n",
      "\n",
      "# 11 Gradient out:  [1.11685749 2.1676627  2.15368868 0.89189849 2.19802534 1.82487475\n",
      " 1.93445314 0.78605935 2.20221568 0.93218101]\n",
      "\n",
      "     Weights  out:  [-0.20643456  0.43580045  0.42122149 -0.38152059  0.46891848  0.16725625\n",
      "  0.23690567 -0.49345469  0.47364968 -0.34470237] [ 0.03407099 -0.30786981 -0.43805499  0.36074861 -0.13107291 -0.04245955\n",
      "  0.1489556   0.806133   -0.94930114  0.14035964]\n",
      "\n",
      "# 12 Gradient out:  [-0.71141577 -1.97911713 -1.96344591 -0.42952719 -2.011956   -1.5552569\n",
      " -1.69292171 -0.31234344 -2.01634701 -0.47832641]\n",
      "\n",
      "     Weights  out:  [-0.20643456  0.43580045  0.42122149 -0.38152059  0.46891848  0.16725625\n",
      "  0.23690567 -0.49345469  0.47364968 -0.34470237] [ 0.25744249  0.12566273 -0.00731725  0.53912831  0.30853215  0.3225154\n",
      "  0.53584623  0.96334487 -0.508858    0.32679584]\n",
      "\n",
      "# 13 Gradient out:  [1.13441064 2.5159909  2.49894187 0.84841678 2.55282204 2.08010507\n",
      " 2.22313978 0.71946455 2.55788275 0.89845681]\n",
      "\n",
      "     Weights  out:  [-0.20643456  0.43580045  0.42122149 -0.38152059  0.46891848  0.16725625\n",
      "  0.23690567 -0.49345469  0.47364968 -0.34470237] [ 0.11515933 -0.2701607  -0.40000644  0.45322287 -0.09385905  0.01146402\n",
      "  0.19726188  0.90087618 -0.9121274   0.23113056]\n",
      "\n",
      "# 14 Gradient out:  [-0.35361399 -1.03377836 -1.02606152 -0.20929966 -1.04988324 -0.81632473\n",
      " -0.88904585 -0.15162188 -1.05203031 -0.23368631]\n",
      "\n",
      "     Weights  out:  [-0.20643456  0.43580045  0.42122149 -0.38152059  0.46891848  0.16725625\n",
      "  0.23690567 -0.49345469  0.47364968 -0.34470237] [ 0.34204146  0.23303748  0.09978194  0.62290623  0.41670536  0.42748504\n",
      "  0.64188984  1.04476909 -0.40055085  0.41082192]\n",
      "\n",
      "# 15 Gradient out:  [-0.68393723 -1.55534317 -1.54009631 -0.44612991 -1.58775871 -1.20049622\n",
      " -1.3022335  -0.33189841 -1.59213932 -0.49118222]\n",
      "\n",
      "     Weights  out:  [-0.20643456  0.43580045  0.42122149 -0.38152059  0.46891848  0.16725625\n",
      "  0.23690567 -0.49345469  0.47364968 -0.34470237] [ 0.27131866  0.02628181 -0.10543037  0.5810463   0.20672871  0.26422009\n",
      "  0.46408067  1.01444471 -0.61095692  0.36408466]\n",
      "\n",
      "# 16 Gradient out:  [1.10065444 2.44592149 2.42905994 0.82019518 2.48238855 2.01850817\n",
      " 2.15800352 0.69263004 2.48740347 0.86950503]\n",
      "\n",
      "     Weights  out:  [-0.20643456  0.43580045  0.42122149 -0.38152059  0.46891848  0.16725625\n",
      "  0.23690567 -0.49345469  0.47364968 -0.34470237] [ 0.13453122 -0.28478682 -0.41344963  0.49182032 -0.11082303  0.02412085\n",
      "  0.20363397  0.94806503 -0.92938478  0.26584821]\n",
      "\n",
      "# 17 Gradient out:  [-0.39969035 -1.17771204 -1.16884116 -0.23409112 -1.19622629 -0.92823882\n",
      " -1.01152528 -0.1677902  -1.19869458 -0.26211131]\n",
      "\n",
      "     Weights  out:  [-0.20643456  0.43580045  0.42122149 -0.38152059  0.46891848  0.16725625\n",
      "  0.23690567 -0.49345469  0.47364968 -0.34470237] [ 0.3546621   0.20439747  0.07236236  0.65585935  0.38565468  0.42782248\n",
      "  0.63523467  1.08659104 -0.43190409  0.43974922]\n",
      "\n",
      "# 18 Gradient out:  [-0.44157337 -0.58428129 -0.57452821 -0.33089483 -0.60553124 -0.42282814\n",
      " -0.45111076 -0.25759753 -0.60845272 -0.35698475]\n",
      "\n",
      "     Weights  out:  [-0.20643456  0.43580045  0.42122149 -0.38152059  0.46891848  0.16725625\n",
      "  0.23690567 -0.49345469  0.47364968 -0.34470237] [ 0.27472403 -0.03114493 -0.16140587  0.60904113  0.14640943  0.24217472\n",
      "  0.43292962  1.053033   -0.671643    0.38732696]\n",
      "\n",
      "# 19 Gradient out:  [0.56837389 2.14589697 2.13451351 0.30840235 2.16948235 1.74699262\n",
      " 1.90178435 0.22292024 2.17262299 0.34665129]\n",
      "\n",
      "     Weights  out:  [-0.20643456  0.43580045  0.42122149 -0.38152059  0.46891848  0.16725625\n",
      "  0.23690567 -0.49345469  0.47364968 -0.34470237] [ 0.18640936 -0.14800119 -0.27631151  0.54286216  0.02530318  0.15760909\n",
      "  0.34270746  1.0015135  -0.79333355  0.31593001]\n",
      "\n",
      "# 20 Gradient out:  [-0.23745186 -0.68344798 -0.67841671 -0.14322192 -0.6939502  -0.54141172\n",
      " -0.58900891 -0.10561189 -0.69535065 -0.15912265]\n",
      "\n",
      "     Weights  out:  [-0.20643456  0.43580045  0.42122149 -0.38152059  0.46891848  0.16725625\n",
      "  0.23690567 -0.49345469  0.47364968 -0.34470237] [ 0.30008414  0.2811782   0.15059119  0.60454263  0.45919965  0.50700761\n",
      "  0.72306433  1.04609754 -0.35880895  0.38526027]\n",
      "\n",
      "# 21 Gradient out:  [-0.61012678 -1.74610923 -1.73263351 -0.3631861  -1.77429244 -1.37443091\n",
      " -1.49686534 -0.26244214 -1.77805555 -0.40544503]\n",
      "\n",
      "     Weights  out:  [-0.20643456  0.43580045  0.42122149 -0.38152059  0.46891848  0.16725625\n",
      "  0.23690567 -0.49345469  0.47364968 -0.34470237] [ 0.25259377  0.14448861  0.01490785  0.57589825  0.32040961  0.39872527\n",
      "  0.60526255  1.02497517 -0.49787908  0.35343574]\n",
      "\n",
      "# 22 Gradient out:  [0.93817717 2.67032231 2.65331666 0.61394466 2.70642423 2.17520896\n",
      " 2.35046826 0.48577855 2.71131965 0.66671968]\n",
      "\n",
      "     Weights  out:  [-0.20643456  0.43580045  0.42122149 -0.38152059  0.46891848  0.16725625\n",
      "  0.23690567 -0.49345469  0.47364968 -0.34470237] [ 0.13056841 -0.20473324 -0.33161885  0.50326102 -0.03444888  0.12383909\n",
      "  0.30588948  0.97248674 -0.85349019  0.27234673]\n",
      "\n",
      "# 23 Gradient out:  [-0.15216999 -0.43294935 -0.42978581 -0.0929339  -0.43955474 -0.34364478\n",
      " -0.37358665 -0.06928298 -0.44043579 -0.10292745]\n",
      "\n",
      "     Weights  out:  [-0.20643456  0.43580045  0.42122149 -0.38152059  0.46891848  0.16725625\n",
      "  0.23690567 -0.49345469  0.47364968 -0.34470237] [ 0.31820384  0.32933122  0.19904448  0.62604996  0.50683597  0.55888088\n",
      "  0.77598314  1.06964245 -0.31122626  0.40569067]\n",
      "\n",
      "# 24 Gradient out:  [-0.31030291 -0.90127238 -0.89458499 -0.18515806 -0.91523008 -0.71267264\n",
      " -0.77580323 -0.13517171 -0.91709108 -0.20629191]\n",
      "\n",
      "     Weights  out:  [-0.20643456  0.43580045  0.42122149 -0.38152059  0.46891848  0.16725625\n",
      "  0.23690567 -0.49345469  0.47364968 -0.34470237] [ 0.28776984  0.24274135  0.11308732  0.60746318  0.41892502  0.49015193\n",
      "  0.70126581  1.05578585 -0.39931341  0.38510518]\n",
      "\n",
      "# 25 Gradient out:  [-0.75836371 -1.99995155 -1.98303468 -0.46683832 -2.0355642  -1.56255925\n",
      " -1.69988375 -0.34026189 -2.04034233 -0.51866652]\n",
      "\n",
      "     Weights  out:  [-0.20643456  0.43580045  0.42122149 -0.38152059  0.46891848  0.16725625\n",
      "  0.23690567 -0.49345469  0.47364968 -0.34470237] [ 0.22570926  0.06248688 -0.06582968  0.57043157  0.235879    0.3476174\n",
      "  0.54610516  1.02875151 -0.58273163  0.3438468 ]\n",
      "\n",
      "# 26 Gradient out:  [1.08135149 2.18061253 2.16572674 0.84402943 2.21299655 1.81901077\n",
      " 1.93385148 0.73125406 2.21747    0.88676397]\n",
      "\n",
      "     Weights  out:  [-0.20643456  0.43580045  0.42122149 -0.38152059  0.46891848  0.16725625\n",
      "  0.23690567 -0.49345469  0.47364968 -0.34470237] [ 0.07403652 -0.33750343 -0.46243662  0.4770639  -0.17123384  0.03510555\n",
      "  0.20612841  0.96069913 -0.9908001   0.24011349]\n",
      "\n",
      "# 27 Gradient out:  [-0.67133729 -1.89975435 -1.88458176 -0.39834836 -1.93155085 -1.48925524\n",
      " -1.62262844 -0.2848874  -1.93580286 -0.44559416]\n",
      "\n",
      "     Weights  out:  [-0.20643456  0.43580045  0.42122149 -0.38152059  0.46891848  0.16725625\n",
      "  0.23690567 -0.49345469  0.47364968 -0.34470237] [ 0.29030682  0.09861907 -0.02929127  0.64586979  0.27136547  0.3989077\n",
      "  0.59289871  1.10694994 -0.5473061   0.41746629]\n",
      "\n",
      "# 28 Gradient out:  [1.01350567 2.484749   2.46742301 0.71514756 2.52203818 2.02993685\n",
      " 2.1815815  0.58421572 2.52714705 0.76659368]\n",
      "\n",
      "     Weights  out:  [-0.20643456  0.43580045  0.42122149 -0.38152059  0.46891848  0.16725625\n",
      "  0.23690567 -0.49345469  0.47364968 -0.34470237] [ 0.15603936 -0.2813318  -0.40620762  0.56620012 -0.1149447   0.10105665\n",
      "  0.26837302  1.04997246 -0.93446667  0.32834745]\n",
      "\n",
      "# 29 Gradient out:  [-0.30998955 -0.92433665 -0.9174163  -0.18004594 -0.93876995 -0.72852764\n",
      " -0.79416322 -0.12832838 -0.94069317 -0.20195619]\n",
      "\n",
      "     Weights  out:  [-0.20643456  0.43580045  0.42122149 -0.38152059  0.46891848  0.16725625\n",
      "  0.23690567 -0.49345469  0.47364968 -0.34470237] [ 0.35874049  0.215618    0.08727698  0.70922963  0.38946294  0.50704403\n",
      "  0.70468932  1.16681561 -0.42903726  0.48166619]\n",
      "\n",
      "# 30 Gradient out:  [-0.7330229  -1.87901811 -1.86230793 -0.45353252 -1.91432066 -1.46021009\n",
      " -1.58858454 -0.32842402 -1.91907007 -0.50413145]\n",
      "\n",
      "     Weights  out:  [-0.20643456  0.43580045  0.42122149 -0.38152059  0.46891848  0.16725625\n",
      "  0.23690567 -0.49345469  0.47364968 -0.34470237] [ 0.29674258  0.03075067 -0.09620628  0.67322044  0.20170895  0.3613385\n",
      "  0.54585668  1.14114993 -0.61717589  0.44127495]\n",
      "\n",
      "# 31 Gradient out:  [1.02222434 2.19277972 2.17701969 0.77005861 2.22704229 1.80859393\n",
      " 1.93084683 0.65068575 2.23177259 0.81538429]\n",
      "\n",
      "     Weights  out:  [-0.20643456  0.43580045  0.42122149 -0.38152059  0.46891848  0.16725625\n",
      "  0.23690567 -0.49345469  0.47364968 -0.34470237] [ 0.150138   -0.34505295 -0.46866786  0.58251394 -0.18115519  0.06929648\n",
      "  0.22813977  1.07546513 -1.00098991  0.34044866]\n",
      "\n",
      "# 32 Gradient out:  [-0.6156787  -1.77792538 -1.76374795 -0.35905264 -1.80761483 -1.39194599\n",
      " -1.51789001 -0.25304502 -1.8115829  -0.40330727]\n",
      "\n",
      "     Weights  out:  [-0.20643456  0.43580045  0.42122149 -0.38152059  0.46891848  0.16725625\n",
      "  0.23690567 -0.49345469  0.47364968 -0.34470237] [ 0.35458287  0.093503   -0.03326392  0.73652566  0.26425327  0.43101527\n",
      "  0.61430913  1.20560228 -0.55463539  0.50352552]\n",
      "\n",
      "# 33 Gradient out:  [0.82833741 2.37218729 2.35619279 0.53219365 2.40626231 1.92034306\n",
      " 2.07751643 0.41158475 2.41089471 0.58122845]\n",
      "\n",
      "     Weights  out:  [-0.20643456  0.43580045  0.42122149 -0.38152059  0.46891848  0.16725625\n",
      "  0.23690567 -0.49345469  0.47364968 -0.34470237] [ 0.23144713 -0.26208208 -0.38601351  0.66471513 -0.09726969  0.15262607\n",
      "  0.31073113  1.15499327 -0.91695197  0.42286407]\n",
      "\n",
      "# 34 Gradient out:  [-0.27742467 -0.83821453 -0.83193518 -0.15911442 -0.85130371 -0.65992917\n",
      " -0.71980681 -0.11219276 -0.85304704 -0.17902658]\n",
      "\n",
      "     Weights  out:  [-0.20643456  0.43580045  0.42122149 -0.38152059  0.46891848  0.16725625\n",
      "  0.23690567 -0.49345469  0.47364968 -0.34470237] [ 0.39711461  0.21235538  0.08522504  0.77115386  0.38398277  0.53669468\n",
      "  0.72623442  1.23731022 -0.43477303  0.53910975]\n",
      "\n",
      "# 35 Gradient out:  [-0.70543421 -1.92235346 -1.90604795 -0.42263577 -1.95666851 -1.4978283\n",
      " -1.63192563 -0.30062339 -1.96127194 -0.47268872]\n",
      "\n",
      "     Weights  out:  [-0.20643456  0.43580045  0.42122149 -0.38152059  0.46891848  0.16725625\n",
      "  0.23690567 -0.49345469  0.47364968 -0.34470237] [ 0.34162968  0.04471247 -0.08116199  0.73933098  0.21372203  0.40470885\n",
      "  0.58227306  1.21487167 -0.60538244  0.50330444]\n",
      "\n",
      "# 36 Gradient out:  [0.96395759 2.22169616 2.20538137 0.6974527  2.25705804 1.81566088\n",
      " 1.94657483 0.57397181 2.26192876 0.74480768]\n",
      "\n",
      "     Weights  out:  [-0.20643456  0.43580045  0.42122149 -0.38152059  0.46891848  0.16725625\n",
      "  0.23690567 -0.49345469  0.47364968 -0.34470237] [ 0.20054284 -0.33975822 -0.46237158  0.65480382 -0.17761167  0.10514319\n",
      "  0.25588793  1.15474699 -0.99763682  0.40876669]\n",
      "\n",
      "# 37 Gradient out:  [-0.53784963 -1.59102919 -1.57853343 -0.30863152 -1.61715385 -1.2460925\n",
      " -1.3597066  -0.2152231  -1.62064108 -0.34784799]\n",
      "\n",
      "     Weights  out:  [-0.20643456  0.43580045  0.42122149 -0.38152059  0.46891848  0.16725625\n",
      "  0.23690567 -0.49345469  0.47364968 -0.34470237] [ 0.39333436  0.10458101 -0.02129531  0.79429436  0.27379993  0.46827536\n",
      "  0.6452029   1.26954135 -0.54525107  0.55772823]\n",
      "\n",
      "# 38 Gradient out:  [0.49475732 1.96294595 1.952302   0.2509672  1.98493488 1.58933065\n",
      " 1.73392768 0.17113073 1.9878548  0.28688839]\n",
      "\n",
      "     Weights  out:  [-0.20643456  0.43580045  0.42122149 -0.38152059  0.46891848  0.16725625\n",
      "  0.23690567 -0.49345469  0.47364968 -0.34470237] [ 0.28576443 -0.21362483 -0.33700199  0.73256806 -0.04963084  0.21905686\n",
      "  0.37326158  1.22649673 -0.86937929  0.48815863]\n",
      "\n",
      "# 39 Gradient out:  [-0.33630216 -1.01793452 -1.01024833 -0.19192088 -1.03396033 -0.80040935\n",
      " -0.87329282 -0.1344863  -1.03609517 -0.21626847]\n",
      "\n",
      "     Weights  out:  [-0.20643456  0.43580045  0.42122149 -0.38152059  0.46891848  0.16725625\n",
      "  0.23690567 -0.49345469  0.47364968 -0.34470237] [ 0.38471589  0.17896436  0.05345841  0.7827615   0.34735614  0.53692299\n",
      "  0.72004711  1.26072288 -0.47180833  0.54553631]\n",
      "\n",
      "# 40 Gradient out:  [-0.66681145 -1.49695289 -1.48158458 -0.43296178 -1.52974257 -1.14816229\n",
      " -1.24609454 -0.31771899 -1.53418636 -0.47790233]\n",
      "\n",
      "     Weights  out:  [-0.20643456  0.43580045  0.42122149 -0.38152059  0.46891848  0.16725625\n",
      "  0.23690567 -0.49345469  0.47364968 -0.34470237] [ 0.31745546 -0.02462254 -0.14859126  0.74437732  0.14056407  0.37684112\n",
      "  0.54538855  1.23382562 -0.67902736  0.50228262]\n",
      "\n",
      "# 41 Gradient out:  [0.94526421 2.30762661 2.29093587 0.66392931 2.34364743 1.87889963\n",
      " 2.01991065 0.5377282  2.34859264 0.71304137]\n",
      "\n",
      "     Weights  out:  [-0.20643456  0.43580045  0.42122149 -0.38152059  0.46891848  0.16725625\n",
      "  0.23690567 -0.49345469  0.47364968 -0.34470237] [ 0.18409317 -0.32401312 -0.44490818  0.65778497 -0.16538444  0.14720866\n",
      "  0.29616964  1.17028182 -0.98586463  0.40670215]\n",
      "\n",
      "# 42 Gradient out:  [-0.43084931 -1.29719932 -1.28725818 -0.24560834 -1.31794496 -1.01823519\n",
      " -1.11115871 -0.17131593 -1.32071034 -0.27700144]\n",
      "\n",
      "     Weights  out:  [-0.20643456  0.43580045  0.42122149 -0.38152059  0.46891848  0.16725625\n",
      "  0.23690567 -0.49345469  0.47364968 -0.34470237] [ 0.37314601  0.1375122   0.013279    0.79057083  0.30334505  0.52298859\n",
      "  0.70015177  1.27782746 -0.51614611  0.54931042]\n",
      "\n",
      "# 43 Gradient out:  [-0.17019288  0.38001189  0.38369337 -0.18938394  0.37121139  0.34475306\n",
      "  0.38788486 -0.16134898  0.36992718 -0.19517111]\n",
      "\n",
      "     Weights  out:  [-0.20643456  0.43580045  0.42122149 -0.38152059  0.46891848  0.16725625\n",
      "  0.23690567 -0.49345469  0.47364968 -0.34470237] [ 0.28697615 -0.12192766 -0.24417264  0.74144916  0.03975605  0.31934155\n",
      "  0.47792003  1.24356428 -0.78028817  0.49391014]\n",
      "\n",
      "# 44 Gradient out:  [-0.60487446 -1.20780417 -1.19403012 -0.40945715 -1.23737022 -0.91758486\n",
      " -0.99281011 -0.30608936 -1.24139433 -0.44879774]\n",
      "\n",
      "     Weights  out:  [-0.20643456  0.43580045  0.42122149 -0.38152059  0.46891848  0.16725625\n",
      "  0.23690567 -0.49345469  0.47364968 -0.34470237] [ 0.25293757 -0.04592528 -0.16743396  0.70357237  0.11399833  0.38829216\n",
      "  0.555497    1.21129448 -0.70630274  0.45487592]\n",
      "\n",
      "# 45 Gradient out:  [0.91992695 2.46817997 2.45122611 0.61576533 2.50445863 2.00431208\n",
      " 2.16277936 0.48780639 2.50940722 0.66702347]\n",
      "\n",
      "     Weights  out:  [-0.20643456  0.43580045  0.42122149 -0.38152059  0.46891848  0.16725625\n",
      "  0.23690567 -0.49345469  0.47364968 -0.34470237] [ 0.13196268 -0.28748612 -0.40623999  0.62168094 -0.13347571  0.20477519\n",
      "  0.35693498  1.15007661 -0.9545816   0.36511637]\n",
      "\n",
      "# 46 Gradient out:  [-0.26874394 -0.80668999 -0.80065981 -0.15524951 -0.81926323 -0.63564945\n",
      " -0.69307587 -0.11018623 -0.82093824 -0.17435928]\n",
      "\n",
      "     Weights  out:  [-0.20643456  0.43580045  0.42122149 -0.38152059  0.46891848  0.16725625\n",
      "  0.23690567 -0.49345469  0.47364968 -0.34470237] [ 0.31594807  0.20614988  0.08400523  0.74483401  0.36741601  0.60563761\n",
      "  0.78949085  1.24763789 -0.45270016  0.49852106]\n",
      "\n",
      "# 47 Gradient out:  [-0.69769806 -1.94480336 -1.92876279 -0.41436792 -1.97849048 -1.51911876\n",
      " -1.65550939 -0.29437717 -1.98300256 -0.46395774]\n",
      "\n",
      "     Weights  out:  [-0.20643456  0.43580045  0.42122149 -0.38152059  0.46891848  0.16725625\n",
      "  0.23690567 -0.49345469  0.47364968 -0.34470237] [ 0.26219929  0.04481188 -0.07612673  0.71378411  0.20356337  0.47850772\n",
      "  0.65087568  1.22560064 -0.61688781  0.46364921]\n",
      "\n",
      "# 48 Gradient out:  [0.98190138 2.29501564 2.27840051 0.70682167 2.33096436 1.87586469\n",
      " 2.01219516 0.58111741 2.33590928 0.75532672]\n",
      "\n",
      "     Weights  out:  [-0.20643456  0.43580045  0.42122149 -0.38152059  0.46891848  0.16725625\n",
      "  0.23690567 -0.49345469  0.47364968 -0.34470237] [ 0.12265967 -0.34414879 -0.46187929  0.63091052 -0.19213473  0.17468397\n",
      "  0.3197738   1.16672521 -1.01348832  0.37085766]\n",
      "\n",
      "# 49 Gradient out:  [-0.47882367 -1.43082694 -1.41979044 -0.27420826 -1.45387326 -1.12274432\n",
      " -1.22501662 -0.19172134 -1.45694683 -0.30898801]\n",
      "\n",
      "     Weights  out:  [-0.20643456  0.43580045  0.42122149 -0.38152059  0.46891848  0.16725625\n",
      "  0.23690567 -0.49345469  0.47364968 -0.34470237] [ 0.31903995  0.11485434 -0.00619919  0.77227486  0.27405815  0.5498569\n",
      "  0.72221283  1.28294869 -0.54630646  0.521923  ]\n",
      "\n",
      "Lineal error cuadrático medio (ECM): 0.6410252009894142\n",
      "\n",
      "# 0 Gradient out:  [-1.50932195 -0.29682343 -0.9296294  -0.44716946 -0.16130171 -1.55257388\n",
      " -1.50087316 -1.20954824 -1.6025676  -0.27005915]\n",
      "\n",
      "     Weights  out:  [ 0.3816174  -0.33074029  0.02960086 -0.22949198 -0.45780204  0.42344085\n",
      "  0.37411371  0.17834464  0.48186826 -0.35191327] [-0.07798663  0.25891496  0.40001644 -0.24692207 -0.070324    0.21690893\n",
      "  0.03917102 -0.31437812  0.356351    0.1134612 ]\n",
      "\n",
      "# 1 Gradient out:  [2.99908057 1.10870835 2.12263715 1.30474168 0.93813895 3.05338802\n",
      " 2.98858979 2.5998153  3.11926381 1.07544906]\n",
      "\n",
      "     Weights  out:  [ 0.3816174  -0.33074029  0.02960086 -0.22949198 -0.45780204  0.42344085\n",
      "  0.37411371  0.17834464  0.48186826 -0.35191327] [-0.37985102  0.19955027  0.21409056 -0.33635596 -0.10258434 -0.09360585\n",
      " -0.26100361 -0.55628776  0.03583748  0.05944937]\n",
      "\n",
      "# 2 Gradient out:  [-0.48948712 -0.17219795 -0.33986453 -0.20862383 -0.13970509 -0.49985221\n",
      " -0.4874737  -0.41620549 -0.51210229 -0.16582724]\n",
      "\n",
      "     Weights  out:  [ 0.3816174  -0.33074029  0.02960086 -0.22949198 -0.45780204  0.42344085\n",
      "  0.37411371  0.17834464  0.48186826 -0.35191327] [ 0.2199651   0.42129194  0.63861799 -0.07540762  0.08504345  0.51707176\n",
      "  0.33671435 -0.0363247   0.65969024  0.27453918]\n",
      "\n",
      "# 3 Gradient out:  [-1.02712549 -0.32330425 -0.69532873 -0.40400191 -0.25178615 -1.04992228\n",
      " -1.02268868 -0.86480786 -1.07674033 -0.30923135]\n",
      "\n",
      "     Weights  out:  [ 0.3816174  -0.33074029  0.02960086 -0.22949198 -0.45780204  0.42344085\n",
      "  0.37411371  0.17834464  0.48186826 -0.35191327] [ 0.12206768  0.38685235  0.57064509 -0.11713239  0.05710243  0.41710131\n",
      "  0.23921961 -0.1195658   0.55726978  0.24137373]\n",
      "\n",
      "# 4 Gradient out:  [-1.70533653 -0.41218513 -1.08723174 -0.57218234 -0.2664521  -1.7519041\n",
      " -1.69627345 -1.38604827 -1.80625942 -0.38359484]\n",
      "\n",
      "     Weights  out:  [ 0.3816174  -0.33074029  0.02960086 -0.22949198 -0.45780204  0.42344085\n",
      "  0.37411371  0.17834464  0.48186826 -0.35191327] [-0.08335742  0.3221915   0.43157934 -0.19793277  0.0067452   0.20711686\n",
      "  0.03468187 -0.29252737  0.34192171  0.17952746]\n",
      "\n",
      "# 5 Gradient out:  [2.65490371 1.04296594 1.90464374 1.2139138  0.89024756 2.7036661\n",
      " 2.64553406 2.30731891 2.76340456 1.01351394]\n",
      "\n",
      "     Weights  out:  [ 0.3816174  -0.33074029  0.02960086 -0.22949198 -0.45780204  0.42344085\n",
      "  0.37411371  0.17834464  0.48186826 -0.35191327] [-0.42442473  0.23975448  0.21413299 -0.31236924 -0.04654522 -0.14326396\n",
      " -0.30457282 -0.56973703 -0.01933017  0.1028085 ]\n",
      "\n",
      "# 6 Gradient out:  [-1.01536964 -0.3078786  -0.6818905  -0.3889349  -0.23612517 -1.0382384\n",
      " -1.01091772 -0.85232397 -1.06512826 -0.29375227]\n",
      "\n",
      "     Weights  out:  [ 0.3816174  -0.33074029  0.02960086 -0.22949198 -0.45780204  0.42344085\n",
      "  0.37411371  0.17834464  0.48186826 -0.35191327] [ 0.10655601  0.44834766  0.59506174 -0.06958648  0.13150429  0.39746926\n",
      "  0.224534   -0.10827325  0.53335074  0.30551128]\n",
      "\n",
      "# 7 Gradient out:  [-1.77464527 -0.47489138 -1.15214471 -0.63728153 -0.32494025 -1.82263114\n",
      " -1.76533764 -1.4507053  -1.87907762 -0.44566625]\n",
      "\n",
      "     Weights  out:  [ 0.3816174  -0.33074029  0.02960086 -0.22949198 -0.45780204  0.42344085\n",
      "  0.37411371  0.17834464  0.48186826 -0.35191327] [-0.09651792  0.38677194  0.45868364 -0.14737346  0.08427925  0.18982158\n",
      "  0.02235045 -0.27873804  0.32032509  0.24676083]\n",
      "\n",
      "# 8 Gradient out:  [2.52793101 1.03360597 1.83049147 1.1945635  0.88703035 2.57482711\n",
      " 2.51895564 2.20100726 2.63271492 1.00556925]\n",
      "\n",
      "     Weights  out:  [ 0.3816174  -0.33074029  0.02960086 -0.22949198 -0.45780204  0.42344085\n",
      "  0.37411371  0.17834464  0.48186826 -0.35191327] [-0.45144697  0.29179367  0.2282547  -0.27482976  0.0192912  -0.17470465\n",
      " -0.33071708 -0.5688791  -0.05549043  0.15762758]\n",
      "\n",
      "# 9 Gradient out:  [-1.28321971 -0.36520068 -0.85034373 -0.47061193 -0.27194402 -1.31293883\n",
      " -1.27743169 -1.07124985 -1.34783974 -0.3468277 ]\n",
      "\n",
      "     Weights  out:  [ 0.3816174  -0.33074029  0.02960086 -0.22949198 -0.45780204  0.42344085\n",
      "  0.37411371  0.17834464  0.48186826 -0.35191327] [ 0.05413923  0.49851486  0.59435299 -0.03591706  0.19669727  0.34026077\n",
      "  0.17307405 -0.12867765  0.47105255  0.35874143]\n",
      "\n",
      "# 10 Gradient out:  [ 0.14072439 -0.03347235  0.08576658 -0.05036349  0.00192175  0.12877971\n",
      "  0.14288771  0.16771469  0.11363532 -0.02771977]\n",
      "\n",
      "     Weights  out:  [ 0.3816174  -0.33074029  0.02960086 -0.22949198 -0.45780204  0.42344085\n",
      "  0.37411371  0.17834464  0.48186826 -0.35191327] [-0.20250471  0.42547473  0.42428424 -0.13003945  0.14230847  0.07767301\n",
      " -0.08241229 -0.34292762  0.2014846   0.28937589]\n",
      "\n",
      "# 11 Gradient out:  [-0.56791707 -0.20772202 -0.37556755 -0.27962615 -0.12718361 -0.59413352\n",
      " -0.56294624 -0.42910114 -0.625849   -0.19286534]\n",
      "\n",
      "     Weights  out:  [ 0.3816174  -0.33074029  0.02960086 -0.22949198 -0.45780204  0.42344085\n",
      "  0.37411371  0.17834464  0.48186826 -0.35191327] [-0.17435983  0.41878026  0.44143756 -0.14011215  0.14269282  0.10342895\n",
      " -0.05383474 -0.30938468  0.22421167  0.28383194]\n",
      "\n",
      "# 12 Gradient out:  [2.57207304 0.74355955 1.74002652 0.91216843 0.61257381 2.61324733\n",
      " 2.56396808 2.22461221 2.66183194 0.71694781]\n",
      "\n",
      "     Weights  out:  [ 0.3816174  -0.33074029  0.02960086 -0.22949198 -0.45780204  0.42344085\n",
      "  0.37411371  0.17834464  0.48186826 -0.35191327] [-0.28794325  0.37723585  0.36632405 -0.19603738  0.1172561  -0.01539775\n",
      " -0.16642399 -0.39520491  0.09904187  0.24525887]\n",
      "\n",
      "# 13 Gradient out:  [-0.35915173 -0.11849088 -0.24572485 -0.14604033 -0.09399951 -0.36696144\n",
      " -0.35763361 -0.30371727 -0.37617855 -0.113682  ]\n",
      "\n",
      "     Weights  out:  [ 0.3816174  -0.33074029  0.02960086 -0.22949198 -0.45780204  0.42344085\n",
      "  0.37411371  0.17834464  0.48186826 -0.35191327] [ 0.22647136  0.52594776  0.71432935 -0.01360369  0.23977086  0.50725171\n",
      "  0.34636963  0.04971753  0.63140825  0.38864843]\n",
      "\n",
      "# 14 Gradient out:  [-0.64655951 -0.20018968 -0.43622343 -0.25123789 -0.15496783 -0.66097378\n",
      " -0.64375473 -0.54384779 -0.67794545 -0.1912935 ]\n",
      "\n",
      "     Weights  out:  [ 0.3816174  -0.33074029  0.02960086 -0.22949198 -0.45780204  0.42344085\n",
      "  0.37411371  0.17834464  0.48186826 -0.35191327] [ 0.15464102  0.50224959  0.66518438 -0.04281176  0.22097096  0.43385942\n",
      "  0.2748429  -0.01102592  0.55617254  0.36591203]\n",
      "\n",
      "# 15 Gradient out:  [-1.55648623 -0.43752212 -1.02843684 -0.56658748 -0.32312009 -1.59295057\n",
      " -1.549385   -1.29707321 -1.63575849 -0.41499021]\n",
      "\n",
      "     Weights  out:  [ 0.3816174  -0.33074029  0.02960086 -0.22949198 -0.45780204  0.42344085\n",
      "  0.37411371  0.17834464  0.48186826 -0.35191327] [ 0.02532911  0.46221165  0.5779397  -0.09305933  0.18997739  0.30166467\n",
      "  0.14609196 -0.11979548  0.42058345  0.32765333]\n",
      "\n",
      "# 16 Gradient out:  [2.53412928 0.72894497 1.71323465 0.8946809  0.60073632 2.57441029\n",
      " 2.52619483 2.19242484 2.62189987 0.70285876]\n",
      "\n",
      "     Weights  out:  [ 0.3816174  -0.33074029  0.02960086 -0.22949198 -0.45780204  0.42344085\n",
      "  0.37411371  0.17834464  0.48186826 -0.35191327] [-0.28596813  0.37470722  0.37225233 -0.20637683  0.12535338 -0.01692544\n",
      " -0.16378504 -0.37921012  0.09343176  0.24465529]\n",
      "\n",
      "# 17 Gradient out:  [-0.37667803 -0.12370245 -0.25744811 -0.1526605  -0.09796349 -0.38488539\n",
      " -0.37508252 -0.31840948 -0.39457057 -0.11864809]\n",
      "\n",
      "     Weights  out:  [ 0.3816174  -0.33074029  0.02960086 -0.22949198 -0.45780204  0.42344085\n",
      "  0.37411371  0.17834464  0.48186826 -0.35191327] [ 0.22085773  0.52049622  0.71489926 -0.02744065  0.24550064  0.49795661\n",
      "  0.34145392  0.05927485  0.61781173  0.38522704]\n",
      "\n",
      "# 18 Gradient out:  [-0.69466647 -0.21348584 -0.46792411 -0.26852033 -0.16474515 -0.71020186\n",
      " -0.69164324 -0.58393699 -0.72848914 -0.2038958 ]\n",
      "\n",
      "     Weights  out:  [ 0.3816174  -0.33074029  0.02960086 -0.22949198 -0.45780204  0.42344085\n",
      "  0.37411371  0.17834464  0.48186826 -0.35191327] [ 0.14552212  0.49575573  0.66340964 -0.05797275  0.22590794  0.42097954\n",
      "  0.26643742 -0.00440705  0.53889762  0.36149742]\n",
      "\n",
      "# 19 Gradient out:  [-1.70389407 -0.47427994 -1.12323091 -0.61665205 -0.34784718 -1.74420037\n",
      " -1.69604577 -1.41784544 -1.79151621 -0.449389  ]\n",
      "\n",
      "     Weights  out:  [ 0.3816174  -0.33074029  0.02960086 -0.22949198 -0.45780204  0.42344085\n",
      "  0.37411371  0.17834464  0.48186826 -0.35191327] [ 0.00658883  0.45305856  0.56982482 -0.11167682  0.19295891  0.27893916\n",
      "  0.12810877 -0.12119445  0.39319979  0.32071826]\n",
      "\n",
      "# 20 Gradient out:  [3.15637118 1.0129418  2.17040348 1.2248105  0.83666752 3.21222153\n",
      " 3.14550195 2.72288104 3.27921335 0.97800698]\n",
      "\n",
      "     Weights  out:  [ 0.3816174  -0.33074029  0.02960086 -0.22949198 -0.45780204  0.42344085\n",
      "  0.37411371  0.17834464  0.48186826 -0.35191327] [-0.33418999  0.35820257  0.34517864 -0.23500723  0.12338948 -0.06990091\n",
      " -0.21110038 -0.40476354  0.03489655  0.23084046]\n",
      "\n",
      "# 21 Gradient out:  [-0.18816109 -0.06540776 -0.1302863  -0.07948367 -0.05285003 -0.19216705\n",
      " -0.1873831  -0.15983861 -0.19690476 -0.06294642]\n",
      "\n",
      "     Weights  out:  [ 0.3816174  -0.33074029  0.02960086 -0.22949198 -0.45780204  0.42344085\n",
      "  0.37411371  0.17834464  0.48186826 -0.35191327] [0.29708425 0.56079093 0.77925933 0.00995487 0.29072298 0.5725434\n",
      " 0.41800001 0.13981267 0.69073922 0.42644186]\n",
      "\n",
      "# 22 Gradient out:  [-0.2607545  -0.08766357 -0.17916571 -0.10748816 -0.07001565 -0.26638287\n",
      " -0.25966082 -0.22086357 -0.27303124 -0.08420085]\n",
      "\n",
      "     Weights  out:  [ 0.3816174  -0.33074029  0.02960086 -0.22949198 -0.45780204  0.42344085\n",
      "  0.37411371  0.17834464  0.48186826 -0.35191327] [ 0.25945203  0.54770938  0.75320207 -0.00594186  0.28015298  0.53410999\n",
      "  0.38052339  0.10784495  0.65135826  0.41385257]\n",
      "\n",
      "# 23 Gradient out:  [-0.40604249 -0.13038543 -0.27614119 -0.16191639 -0.10239618 -0.41496609\n",
      " -0.40430719 -0.34259518 -0.42548899 -0.12488573]\n",
      "\n",
      "     Weights  out:  [ 0.3816174  -0.33074029  0.02960086 -0.22949198 -0.45780204  0.42344085\n",
      "  0.37411371  0.17834464  0.48186826 -0.35191327] [ 0.20730113  0.53017667  0.71736893 -0.02743949  0.26614985  0.48083341\n",
      "  0.32859122  0.06367223  0.59675202  0.3970124 ]\n",
      "\n",
      "# 24 Gradient out:  [-0.77916292 -0.23369284 -0.52213715 -0.29606921 -0.17850737 -0.79675033\n",
      " -0.77573932 -0.65366554 -0.81743814 -0.22282852]\n",
      "\n",
      "     Weights  out:  [ 0.3816174  -0.33074029  0.02960086 -0.22949198 -0.45780204  0.42344085\n",
      "  0.37411371  0.17834464  0.48186826 -0.35191327] [ 0.12609263  0.50409958  0.66214069 -0.05982277  0.24567061  0.39784019\n",
      "  0.24772979 -0.0048468   0.51165422  0.37203526]\n",
      "\n",
      "# 25 Gradient out:  [-1.91375493 -0.52579469 -1.25700654 -0.68827894 -0.38055499 -1.9600872\n",
      " -1.90474057 -1.58763399 -2.01452509 -0.49725639]\n",
      "\n",
      "     Weights  out:  [ 0.3816174  -0.33074029  0.02960086 -0.22949198 -0.45780204  0.42344085\n",
      "  0.37411371  0.17834464  0.48186826 -0.35191327] [-0.02973995  0.45736101  0.55771326 -0.11903661  0.20996914  0.23849013\n",
      "  0.09258192 -0.13557991  0.34816659  0.32746955]\n",
      "\n",
      "# 26 Gradient out:  [3.04445323 1.1370604  2.15778403 1.33784254 0.95857399 3.10141504\n",
      " 3.03350213 2.63589721 3.17117265 1.10259125]\n",
      "\n",
      "     Weights  out:  [ 0.3816174  -0.33074029  0.02960086 -0.22949198 -0.45780204  0.42344085\n",
      "  0.37411371  0.17834464  0.48186826 -0.35191327] [-0.41249094  0.35220207  0.30631195 -0.2566924   0.13385814 -0.15352731\n",
      " -0.28836619 -0.45310671 -0.05473843  0.22801828]\n",
      "\n",
      "# 27 Gradient out:  [-0.39417266 -0.12121555 -0.26558919 -0.15237915 -0.0936277  -0.40296559\n",
      " -0.39246169 -0.33145847 -0.41332064 -0.11578793]\n",
      "\n",
      "     Weights  out:  [ 0.3816174  -0.33074029  0.02960086 -0.22949198 -0.45780204  0.42344085\n",
      "  0.37411371  0.17834464  0.48186826 -0.35191327] [0.19639971 0.57961415 0.73786876 0.01087611 0.32557294 0.46675569\n",
      " 0.31833423 0.07407273 0.5794961  0.44853653]\n",
      "\n",
      "# 28 Gradient out:  [-0.74598654 -0.21630754 -0.49646333 -0.27679795 -0.16288831 -0.76300755\n",
      " -0.74267185 -0.62427444 -0.78301239 -0.20578234]\n",
      "\n",
      "     Weights  out:  [ 0.3816174  -0.33074029  0.02960086 -0.22949198 -0.45780204  0.42344085\n",
      "  0.37411371  0.17834464  0.48186826 -0.35191327] [ 0.11756518  0.55537104  0.68475092 -0.01959972  0.3068474   0.38616258\n",
      "  0.2398419   0.00778104  0.49683197  0.42537894]\n",
      "\n",
      "# 29 Gradient out:  [-1.83864494 -0.50212682 -1.20640332 -0.65834784 -0.36245254 -1.88320354\n",
      " -1.82997811 -1.52502884 -1.93560571 -0.47469433]\n",
      "\n",
      "     Weights  out:  [ 0.3816174  -0.33074029  0.02960086 -0.22949198 -0.45780204  0.42344085\n",
      "  0.37411371  0.17834464  0.48186826 -0.35191327] [-0.03163213  0.51210954  0.58545825 -0.07495931  0.27426973  0.23356107\n",
      "  0.09130753 -0.11707385  0.34022949  0.38422247]\n",
      "\n",
      "# 30 Gradient out:  [3.10818258 1.08297229 2.16918403 1.29294216 0.89921239 3.16672987\n",
      " 3.09689448 2.68038491 3.23808032 1.04726784]\n",
      "\n",
      "     Weights  out:  [ 0.3816174  -0.33074029  0.02960086 -0.22949198 -0.45780204  0.42344085\n",
      "  0.37411371  0.17834464  0.48186826 -0.35191327] [-0.39936112  0.41168417  0.34417759 -0.20662888  0.20177923 -0.14307964\n",
      " -0.27468809 -0.42207962 -0.04689165  0.28928361]\n",
      "\n",
      "# 31 Gradient out:  [-0.28749625 -0.08780779 -0.19344349 -0.11058515 -0.0676575  -0.29391819\n",
      " -0.28624651 -0.24165485 -0.30148005 -0.08384256]\n",
      "\n",
      "     Weights  out:  [ 0.3816174  -0.33074029  0.02960086 -0.22949198 -0.45780204  0.42344085\n",
      "  0.37411371  0.17834464  0.48186826 -0.35191327] [0.22227539 0.62827863 0.7780144  0.05195955 0.3816217  0.49026633\n",
      " 0.3446908  0.11399737 0.60072442 0.49873717]\n",
      "\n",
      "# 32 Gradient out:  [-0.46758261 -0.13683929 -0.31182522 -0.17454046 -0.10356176 -0.47818543\n",
      " -0.46551795 -0.39170832 -0.49065178 -0.13028294]\n",
      "\n",
      "     Weights  out:  [ 0.3816174  -0.33074029  0.02960086 -0.22949198 -0.45780204  0.42344085\n",
      "  0.37411371  0.17834464  0.48186826 -0.35191327] [0.16477615 0.61071707 0.7393257  0.02984253 0.3680902  0.4314827\n",
      " 0.2874415  0.0656664  0.54042841 0.48196866]\n",
      "\n",
      "# 33 Gradient out:  [-0.96943203 -0.26791664 -0.63890333 -0.34811606 -0.1971658  -0.9919723\n",
      " -0.96504057 -0.80809384 -1.018433   -0.25396573]\n",
      "\n",
      "     Weights  out:  [ 0.3816174  -0.33074029  0.02960086 -0.22949198 -0.45780204  0.42344085\n",
      "  0.37411371  0.17834464  0.48186826 -0.35191327] [ 0.07125962  0.58334922  0.67696065 -0.00506557  0.34737785  0.33584561\n",
      "  0.19433791 -0.01267527  0.44229805  0.45591207]\n",
      "\n",
      "# 34 Gradient out:  [-1.89213424 -0.55408314 -1.25015166 -0.72262014 -0.39567314 -1.94292382\n",
      " -1.88233036 -1.55590825 -2.00336373 -0.52349367]\n",
      "\n",
      "     Weights  out:  [ 0.3816174  -0.33074029  0.02960086 -0.22949198 -0.45780204  0.42344085\n",
      "  0.37411371  0.17834464  0.48186826 -0.35191327] [-0.12262678  0.52976589  0.54917999 -0.07468878  0.30794469  0.13745115\n",
      "  0.00132979 -0.17429404  0.23861145  0.40511893]\n",
      "\n",
      "# 35 Gradient out:  [2.4267299  1.04219304 1.77708781 1.19575901 0.89726304 2.47327048\n",
      " 2.41788765 2.1154073  2.53152494 1.01489062]\n",
      "\n",
      "     Weights  out:  [ 0.3816174  -0.33074029  0.02960086 -0.22949198 -0.45780204  0.42344085\n",
      "  0.37411371  0.17834464  0.48186826 -0.35191327] [-0.50105363  0.41894926  0.29914965 -0.21921281  0.22881006 -0.25113361\n",
      " -0.37513628 -0.48547569 -0.1620613   0.30042019]\n",
      "\n",
      "# 36 Gradient out:  [-1.50129594 -0.39541969 -0.97914714 -0.52334081 -0.28180363 -1.53751681\n",
      " -1.49424479 -1.24424485 -1.5800729  -0.37306001]\n",
      "\n",
      "     Weights  out:  [ 0.3816174  -0.33074029  0.02960086 -0.22949198 -0.45780204  0.42344085\n",
      "  0.37411371  0.17834464  0.48186826 -0.35191327] [-0.01570765  0.62738787  0.65456722  0.01993899  0.40826267  0.24352048\n",
      "  0.10844125 -0.06239422  0.34424369  0.50339832]\n",
      "\n",
      "# 37 Gradient out:  [1.85822662 0.34609855 1.17865671 0.47430735 0.2577426  1.88556839\n",
      " 1.85270399 1.59180497 1.91635886 0.32720494]\n",
      "\n",
      "     Weights  out:  [ 0.3816174  -0.33074029  0.02960086 -0.22949198 -0.45780204  0.42344085\n",
      "  0.37411371  0.17834464  0.48186826 -0.35191327] [-0.31596684  0.54830393  0.45873779 -0.08472917  0.35190194 -0.06398288\n",
      " -0.1904077  -0.31124319  0.02822911  0.42878632]\n",
      "\n",
      "# 38 Gradient out:  [-1.001882   -0.26962211 -0.65687143 -0.35333482 -0.19582794 -1.02538965\n",
      " -0.99730102 -0.83348203 -1.05297132 -0.2550649 ]\n",
      "\n",
      "     Weights  out:  [ 0.3816174  -0.33074029  0.02960086 -0.22949198 -0.45780204  0.42344085\n",
      "  0.37411371  0.17834464  0.48186826 -0.35191327] [0.05567849 0.61752364 0.69446913 0.0101323  0.40345046 0.3131308\n",
      " 0.18013309 0.0071178  0.41150088 0.4942273 ]\n",
      "\n",
      "# 39 Gradient out:  [-1.77891409 -0.54440916 -1.18281647 -0.70497993 -0.39010156 -1.82849589\n",
      " -1.76937847 -1.45935347 -1.88787686 -0.5148491 ]\n",
      "\n",
      "     Weights  out:  [ 0.3816174  -0.33074029  0.02960086 -0.22949198 -0.45780204  0.42344085\n",
      "  0.37411371  0.17834464  0.48186826 -0.35191327] [-0.14469791  0.56359922  0.56309484 -0.06053467  0.36428488  0.10805287\n",
      " -0.01932711 -0.15957861  0.20090662  0.44321432]\n",
      "\n",
      "# 40 Gradient out:  [2.50943642 1.05437752 1.82676788 1.2156782  0.90217815 2.5583106\n",
      " 2.50015085 2.18241373 2.61948958 1.02570521]\n",
      "\n",
      "     Weights  out:  [ 0.3816174  -0.33074029  0.02960086 -0.22949198 -0.45780204  0.42344085\n",
      "  0.37411371  0.17834464  0.48186826 -0.35191327] [-0.50048073  0.45471738  0.32653155 -0.20153065  0.28626457 -0.25764631\n",
      " -0.3732028  -0.4514493  -0.17666875  0.3402445 ]\n",
      "\n",
      "# 41 Gradient out:  [-1.287207   -0.33329293 -0.83730029 -0.44297491 -0.23632174 -1.3181066\n",
      " -1.28118725 -1.06669228 -1.35436686 -0.31417789]\n",
      "\n",
      "     Weights  out:  [ 0.3816174  -0.33074029  0.02960086 -0.22949198 -0.45780204  0.42344085\n",
      "  0.37411371  0.17834464  0.48186826 -0.35191327] [ 0.00140655  0.66559289  0.69188513  0.04160499  0.4667002   0.25401581\n",
      "  0.12682736 -0.01496655  0.34722916  0.54538555]\n",
      "\n",
      "# 42 Gradient out:  [ 0.11951889 -0.18846865  0.00777489 -0.19815568 -0.15032557  0.10637834\n",
      "  0.121786    0.13104571  0.08816572 -0.18298668]\n",
      "\n",
      "     Weights  out:  [ 0.3816174  -0.33074029  0.02960086 -0.22949198 -0.45780204  0.42344085\n",
      "  0.37411371  0.17834464  0.48186826 -0.35191327] [-0.25603485  0.5989343   0.52442507 -0.04698999  0.41943585 -0.00960551\n",
      " -0.12941009 -0.22830501  0.07635579  0.48254997]\n",
      "\n",
      "# 43 Gradient out:  [-0.24613685 -0.25234623 -0.21874103 -0.29246019 -0.19038997 -0.26676117\n",
      " -0.24237832 -0.1727185  -0.29340542 -0.24194388]\n",
      "\n",
      "     Weights  out:  [ 0.3816174  -0.33074029  0.02960086 -0.22949198 -0.45780204  0.42344085\n",
      "  0.37411371  0.17834464  0.48186826 -0.35191327] [-0.23213107  0.56124057  0.52598005 -0.08662113  0.38937073  0.01167016\n",
      " -0.10505289 -0.20209587  0.09398894  0.44595263]\n",
      "\n",
      "# 44 Gradient out:  [1.30759999 0.18584585 0.81274614 0.26847236 0.13949861 1.32149359\n",
      " 1.30466724 1.13285407 1.33606943 0.17505308]\n",
      "\n",
      "     Weights  out:  [ 0.3816174  -0.33074029  0.02960086 -0.22949198 -0.45780204  0.42344085\n",
      "  0.37411371  0.17834464  0.48186826 -0.35191327] [-0.28135844  0.51077133  0.48223184 -0.14511317  0.35129274 -0.04168207\n",
      " -0.15352855 -0.23663957  0.03530785  0.39756386]\n",
      "\n",
      "# 45 Gradient out:  [-1.66584704 -0.44796542 -1.09045943 -0.58932286 -0.32210965 -1.70598024\n",
      " -1.65803701 -1.38188268 -1.75315462 -0.42321672]\n",
      "\n",
      "     Weights  out:  [ 0.3816174  -0.33074029  0.02960086 -0.22949198 -0.45780204  0.42344085\n",
      "  0.37411371  0.17834464  0.48186826 -0.35191327] [-0.01983844  0.5479405   0.64478107 -0.09141869  0.37919246  0.22261664\n",
      "  0.1074049  -0.01006875  0.30252174  0.43257447]\n",
      "\n",
      "# 46 Gradient out:  [2.82006777 0.79472272 1.89113748 0.99133667 0.63460155 2.87067546\n",
      " 2.81017738 2.41714317 2.93093006 0.76271184]\n",
      "\n",
      "     Weights  out:  [ 0.3816174  -0.33074029  0.02960086 -0.22949198 -0.45780204  0.42344085\n",
      "  0.37411371  0.17834464  0.48186826 -0.35191327] [-0.35300785  0.45834741  0.42668918 -0.20928327  0.31477053 -0.11857941\n",
      " -0.2242025  -0.28644529 -0.04810919  0.34793113]\n",
      "\n",
      "# 47 Gradient out:  [-0.30835945 -0.0926659  -0.20677128 -0.11726545 -0.07091252 -0.31529196\n",
      " -0.3070102  -0.25885092 -0.32345283 -0.08838427]\n",
      "\n",
      "     Weights  out:  [ 0.3816174  -0.33074029  0.02960086 -0.22949198 -0.45780204  0.42344085\n",
      "  0.37411371  0.17834464  0.48186826 -0.35191327] [ 0.21100571  0.61729196  0.80491668 -0.01101593  0.44169084  0.45555569\n",
      "  0.33783297  0.19698334  0.53807683  0.5004735 ]\n",
      "\n",
      "# 48 Gradient out:  [-0.51772217 -0.14887681 -0.3440214  -0.19092235 -0.11177918 -0.52954159\n",
      " -0.51542031 -0.43310682 -0.54343393 -0.1415661 ]\n",
      "\n",
      "     Weights  out:  [ 0.3816174  -0.33074029  0.02960086 -0.22949198 -0.45780204  0.42344085\n",
      "  0.37411371  0.17834464  0.48186826 -0.35191327] [ 0.14933382  0.59875878  0.76356242 -0.03446902  0.42750834  0.39249729\n",
      "  0.27643093  0.14521316  0.47338626  0.48279664]\n",
      "\n",
      "# 49 Gradient out:  [-1.13303102 -0.30788218 -0.74411004 -0.40240873 -0.22441271 -1.1596257\n",
      " -1.12784998 -0.94291068 -1.19084414 -0.29142662]\n",
      "\n",
      "     Weights  out:  [ 0.3816174  -0.33074029  0.02960086 -0.22949198 -0.45780204  0.42344085\n",
      "  0.37411371  0.17834464  0.48186826 -0.35191327] [ 0.04578938  0.56898342  0.69475814 -0.07265349  0.4051525   0.28658897\n",
      "  0.17334687  0.0585918   0.36469947  0.45448342]\n",
      "\n",
      "Tanh error cuadrático medio (ECM): 0.515526670793126\n",
      "Promedio MSE salida lineal: 1.3992546531446195 ± 0.4690064460527766\n",
      "Promedio MSE salida tanh: 0.8434145350587672 ± 0.22090628428428055\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "errors_linear = []\n",
    "errors_tanh = []\n",
    "\n",
    "for _ in range(10):\n",
    "    train(X, t)\n",
    "    _, mse_lin = recall(X, t)\n",
    "    errors_linear.append(mse_lin)\n",
    "\n",
    "    train_tanh(X, t)\n",
    "    _, mse_tanh = recall_tanh(X, t)\n",
    "    errors_tanh.append(mse_tanh)\n",
    "\n",
    "print(\"Promedio MSE salida lineal:\", np.mean(errors_linear), \"±\", np.std(errors_linear))\n",
    "print(\"Promedio MSE salida tanh:\", np.mean(errors_tanh), \"±\", np.std(errors_tanh))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa que el Error Cuadrático Medio (MSE) obtenido con activación lineal en la capa de salida es consistentemente más alto, con un promedio cercano a 1.422. En contraste, al emplear la función tangente hiperbólica (tanh) como activación de salida, el modelo logra un MSE promedio de aproximadamente 0.794.\n",
    "\n",
    "Este resultado sugiere que la red con activación tanh en la salida ofrece un mejor desempeño predictivo, al adaptarse más eficazmente a la distribución del conjunto de datos.\n",
    "La función tanh, al ser no lineal y acotada en [-1, 1], proporciona una salida más adecuada para problemas donde las variables objetivo también están dentro de ese rango, favoreciendo una mejor aproximación a la función real y facilitando la convergencia del modelo.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
