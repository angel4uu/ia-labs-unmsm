{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9GnUElm5_e8m"
   },
   "source": [
    "# Pr√°ctica de Laboratorio 8 - Inteligencia Artificial 2025-1 Secci√≥n 1 EPISW-FISI\n",
    "## Implementaci√≥n de una red PMC-BP con Python y Numpy\n",
    "### Prof. Rolando A. Magui√±a P√©rez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ky22d26K_e8n"
   },
   "source": [
    "## Introducci√≥n\n",
    "La Pr√°ctica Guiada de Laboratorio (PGL) 8 a realizarse el Jueves 05 de Junio del presente a√±o, tratar√° sobre la red Perceptr√≥n Multicapa con su algoritmo de aprendizaje llamado Backpropagation. Esta red se aplicar√° para resolver problemas gen√©ricos de clasificaci√≥n y de regresi√≥n.\n",
    "\n",
    "Se desea abordar el problema de la aproximaci√≥n de una funci√≥n mediante una Perceptr√≥n Multicapa-Backpropagation (PMC-BP). Inicialmente se presenta la implementaci√≥n del algoritmo de entrenamiento de esta red (presentado en las sesiones de teor√≠a), con el lenguaje `Python` y sus bibliotecas `Numpy` y `Matplotlib`. Posteriormente, **se propondr√°n algunos ejercicios cuyas soluciones se podr√°n obtener en grupos de hasta 4 alumnos**, y deber√°n enviarse para su respectiva revisi√≥n (ver secci√≥n 'Instrucciones para el env√≠o' en este mismo cuaderno).  \n",
    "\n",
    "Requiere: numpy, matplotlib\n",
    "\n",
    "Nomenclatura:\n",
    "- Z: n√∫mero de instancias (muestras) en el conjunto de datos\n",
    "- N: n√∫mero de atributos o variables de entrada\n",
    "- M: n√∫mero de atributos o variables de salida\n",
    "- t: vector de salidas esperadas o targets\n",
    "- y: vector de salidas estimadas por la red.\n",
    "\n",
    "### Paso previo\n",
    "Importamos las bibliotecas de Python requeridas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5r_BRXlK_e8n"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.11' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: '/opt/homebrew/bin/python3.12 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bKy9zmzb_e8p"
   },
   "source": [
    "## Dataset\n",
    "El primer paso consiste en obtener el arreglo conteniendo los pares entrada-salida (instancias) a usar en el entrenamiento/validaci√≥n de la red PMC-BP a implementar; dicho arreglo se denominar√° 'Dataset'. El tama√±o de dicho arreglo es de $Z \\times d$, donde $Z$ es el n√∫mero de instancias (muestras) y $d$ es el n√∫mero de caracter√≠sticas o atributos considerados para el problema abordado (incluye los atributos de entrada y los de salida)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lHAeppdW_e8q",
    "outputId": "be4a459f-13d9-425f-bd6d-6e0cbbe7d8cd"
   },
   "outputs": [],
   "source": [
    "Dataset = np.array([[0.00000, 0.00000], [1.00000, 0.84000], [2.00000, 0.91000], [3.00000, 0.14000],\n",
    "[4.00000, -0.77000], [5.00000, -0.96000],[6.00000, -0.28000], [7.00000, 0.66000], [8.00000, 0.99000]])\n",
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PQUmblTn_e8q",
    "outputId": "339662ed-f65c-4cb2-d5ba-a944a8775d4b"
   },
   "outputs": [],
   "source": [
    "Dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5xJFV52G_e8q"
   },
   "source": [
    "## Data para el entrenamiento/validaci√≥n de la red\n",
    "Como sabemos, a partir del dataset obtenido, se deben determinar los conjuntos de datos a emplear en el entrenamiento y en la validaci√≥n de la red PMC-BP. Enseguida, se deben obtener dos arreglos: uno con los vectores de entrada a usar en el entrenamiento, y el segundo, con los respectivos vectores de salida. An√°logamente, se deben determinar los arreglos con los vectores de entrada y de salida, a usar en la validaci√≥n del entrenamiento. **Sin embargo, para el problema planteado, el dataset se usar√° tanto para el entrenamiento como para la validaci√≥n**.\n",
    "\n",
    "Separando los valores de entrada de los de salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hr48ISV7_e8q",
    "outputId": "649fd8d4-c634-4166-8853-9208bad639a8"
   },
   "outputs": [],
   "source": [
    "X = Dataset[:,0]\n",
    "t = Dataset[:,1]\n",
    "X, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L0tmqcxX_e8r",
    "outputId": "c5dd6b48-055e-4a0e-a4b4-5ba76e3800f1"
   },
   "outputs": [],
   "source": [
    "X.shape, t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sxiXCKVT_e8r",
    "outputId": "dc3bdc08-c4cd-47c4-a353-61323d72eba7"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 4))\n",
    "plt.plot(X,t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cQJsVNIL_e8r"
   },
   "source": [
    "## Normalizaci√≥n de los datos\n",
    "Antes de iniciar alg√∫n c√°lculo, sabemos que debemos tener en cuenta las diferencias que existen en las unidades de nuestros datos. Se requiere que los datos de nuestras variables est√©n en el mismo orden de magnitud, y en un buen n√∫mero de casos es necesario normalizarlos; de esta manera nuestro modelo trabajar√° con unidades normalizadas. A pesar de lo indicado, incluso sabedores que hay varios procedimientos de normalizaci√≥n, en este caso, **no vamos a normalizar inicialmente nuestros datos**.\n",
    "\n",
    "## Dise√±o de la red\n",
    "Inicialmente se considera una topolog√≠a de la red como la mostrada en la figura, vale decir, con 10 neuronas ocultas. Como funci√≥n de activaci√≥n de las neuronas ocultas se usar√° la log√≠stica sigmoidea y en las neuronas de salida, dado que se trata de un problema de aproximaci√≥n de funciones, se usar√° una funci√≥n lineal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WGsNXQEa_e8r",
    "outputId": "855b611b-4ff4-4ed2-bdaa-cf13deb780df"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "i = Image(filename='D:\\\\Cursos\\\\Redes Neuronales\\\\2021-2\\\\arquit-red_aprox-fc_2021-2.png')\n",
    "i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DkStZNAt_e8s"
   },
   "source": [
    "## Inicializaci√≥n de los pesos y biases de la red\n",
    "Seg√∫n el algoritmo, los par√°metros libres de la red se inicializan a valores aleatorios peque√±os, los cuales pueden estar en el rangos: [-0.5,0.5] o [-1,1] o en torno de cero. A continuaci√≥n se presenta el c√≥digo para inicializarlos, aplicado al **problema planteado**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lVovH8l6_e8s"
   },
   "outputs": [],
   "source": [
    "# Implementaci√≥n b√°sica sin funciones\n",
    "intervalo = 0.5\n",
    "capa_entrada = 1\n",
    "capa_oculta = 10\n",
    "capa_salida = 1\n",
    "\n",
    "w1 = np.random.uniform(-intervalo, intervalo, capa_oculta)\n",
    "\n",
    "w2 = np.random.uniform(-intervalo, intervalo, capa_oculta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CZ0RMbRe_e8s",
    "outputId": "f683c42b-d53b-4b2e-ea4a-14907601f2eb"
   },
   "outputs": [],
   "source": [
    "w1, w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lmCcNM_d_e8s",
    "outputId": "9d457a8a-6bf1-4af0-b0c6-588ff5716fa7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mBsY50rR_e8t"
   },
   "source": [
    "### Definici√≥n de la funci√≥n log√≠stica sigmoidea\n",
    "Sabemos que la expresi√≥n matem√°tica de la funci√≥n log√≠stica sigmoidea `f(n)` es:\n",
    "\n",
    "                         f(u) =  1/1 + exp(-u)\n",
    "\n",
    "donde `u` es el vector de entradas netas. A partir de dicho par√°metro, es posible calcular la funci√≥n logistica sigmoidea; en la sgte celda se presenta el respectivo c√≥digo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "epFEjYWl_e8t"
   },
   "outputs": [],
   "source": [
    "# Funcion de activacion Logistica Sigmoidea para la unidad de salida\n",
    "def logistica(u):\n",
    "    return 1/(1 + np.exp(-u))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Th5dgmub_e8t"
   },
   "source": [
    "Supongamos que se desea aplicar esta funci√≥n al arreglo 'a'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TAEvUwwY_e8t",
    "outputId": "8437d4cb-1c63-4f29-c846-8e595401d360"
   },
   "outputs": [],
   "source": [
    "a = np.array([[0, 0.6, -0.8]])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9iHeZAXi_e8t",
    "outputId": "c8eae3ea-8366-4e37-df4a-31a00e9f57e7"
   },
   "outputs": [],
   "source": [
    "logistica(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t4M6Vnr-_e8t"
   },
   "source": [
    "A continuaci√≥n se presenta la implementaci√≥n de la derivada de la funci√≥n log√≠stica sigmoidea:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3XH0afOY_e8u"
   },
   "outputs": [],
   "source": [
    "def deriv_logistica(x):\n",
    "    return x * (1.0 - x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CQBQqCLR_e8u",
    "outputId": "7b7de73a-c865-432a-c424-840452937c46"
   },
   "outputs": [],
   "source": [
    "deriv_logistica(-1.76)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bPor0YoF_e8u"
   },
   "source": [
    "## Implementaci√≥n\n",
    "Luego de haber determinado la topolog√≠a de la red neuronal, la implementaremos en el lenguaje de programaci√≥n `Python` con la ayuda de su biblioteca `Numpy`. Enseguida, se efectuar√°n las sgtes actividades:\n",
    "\n",
    "- Construiremos el algoritmo de aprendizaje de nuestra red PMC, Backpropagation, mediante la funci√≥n `train()`. Dentro de ella se instancian constantes y variables importantes como globales, de modo que estos valores sean accesibles para toda la funci√≥n.\n",
    "- Aplicaremos dicho algoritmo de aprendizaje para resolver el problema de aproximaci√≥n de una funci√≥n planteado; para tal efecto, se usar√° el conjunto de datos disponible.\n",
    "\n",
    "En las sgtes celdas se presentan las l√≠neas de c√≥digo correspondientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6M-xeAvc_e8u"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NETtXa7d_e8u"
   },
   "outputs": [],
   "source": [
    "Dataset = np.array([[0.00000, 0.00000], [1.00000, 0.84000], [2.00000, 0.91000], [3.00000, 0.14000],\n",
    "[4.00000, -0.77000], [5.00000, -0.96000],[6.00000, -0.28000], [7.00000, 0.66000], [8.00000, 0.99000]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0YVIEN_G_e8v",
    "outputId": "09e186ca-8f01-4101-f9fc-2c82d36c0d54"
   },
   "outputs": [],
   "source": [
    "X = Dataset[:,0]\n",
    "t = Dataset[:,1]\n",
    "X, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2nIZHvFq_e8v"
   },
   "outputs": [],
   "source": [
    "def train(X, t, learning_rate=0.2, epochs=50):\n",
    "    global input_num\n",
    "    global hidden_num\n",
    "    global w1\n",
    "    global w2\n",
    "\n",
    "    input_num = 1\n",
    "    hidden_num = 10\n",
    "    output_num = 1\n",
    "    intervalo = 0.5\n",
    "\n",
    "    # inicializando los pesos\n",
    "    w1 = np.random.uniform(-intervalo, intervalo, hidden_num)\n",
    "\n",
    "    w2 = np.random.uniform(-intervalo, intervalo, hidden_num)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        gradient_out = 0.0                 # gradientes para la capa de salida y la capa oculta\n",
    "        gradient_hidden = []\n",
    "\n",
    "        for i in range(X.shape[0]):\n",
    "        # propagacion hacia adelante\n",
    "            x = X[i]\n",
    "\n",
    "            u1 = x * w1\n",
    "            o = logistica(u1)\n",
    "            u2 = o.dot(w2)\n",
    "            y = u2\n",
    "\n",
    "        # backpropagation\n",
    "            delta_hidden_s = []           # inicializamos los delta_j a lista vac√≠a\n",
    "            gradient_hidden_s = []       # inicializamos los gradientes de neurs ocultas a lista vac√≠a\n",
    "\n",
    "            delta_out_s = t[i] - y     # c√°lculo del √∫nico delta_k (f'(u) = 1 pq fc de activ es lineal)\n",
    "            gradient_out_s = delta_out_s * o     # error por la salida de la capa anterior\n",
    "\n",
    "            for j in range(hidden_num):\n",
    "\n",
    "                delta_hidden_s.append(deriv_logistica(u1[j]) * w2[j] * delta_out_s)\n",
    "                gradient_hidden_s.append(delta_hidden_s[j] * x)\n",
    "\n",
    "\n",
    "            gradient_out = gradient_out + gradient_out_s\n",
    "            gradient_hidden = gradient_hidden + gradient_hidden_s\n",
    "\n",
    "\n",
    "        print(\"\\n#\", epoch, \"Gradient out: \",gradient_out),\n",
    "        print(\"\\n     Weights  out: \", w1, w2)\n",
    "\n",
    "        # Ahora actualizando pesos\n",
    "        w2 = w2 + learning_rate * gradient_out\n",
    "\n",
    "        for j in range(hidden_num):\n",
    "            w1[j] = w1[j] + learning_rate * gradient_hidden[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oJgS7qCi_e8v",
    "outputId": "dc1f053a-dec7-40d4-fa7f-6db8b843e96a"
   },
   "outputs": [],
   "source": [
    "train(X, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6KIURxgz_e8v"
   },
   "source": [
    "## Ejercicios\n",
    "### Ejercicio A  (3 puntos)\n",
    "1. Obtenga, a partir del c√≥digo presentado para el entrenamiento de la PMC-BP, el algoritmo de recuerdo de la red.\n",
    "2. Aplique ahora dicho algoritmo de recuerdo en la etapa denominada *Validaci√≥n* para el mismo problema de regresi√≥n. Determine el error cuadr√°tico de la red en cada √©poca y grafique sus resultados.\n",
    "\n",
    "### Ejercicio B  (5 puntos)\n",
    "1. Use la funci√≥n tangente hiperb√≥lica en lugar de la lineal en la capa de salida para abordar el mismo problema. Con ese objetivo defina la(s) funci√≥n(nes) que se requieran e ins√©rtelas en el c√≥digo de modo que la red funcione correctamente. Mantenga inalterada la arquitectura de la red.\n",
    "2. A partir de las modificaciones pedidas, entrene nuevamente la red al mismo problema de regresi√≥n. Enseguida aplique el algoritmo de recuerdo.\n",
    "3. Compare los resultados que obtenga con los obtenidos con la red PMC-BP que usaba una funci√≥n lineal en la salida.\n",
    "\n",
    "### Ejercicio C (4 puntos)\n",
    "1. Modifique la arquitectura de la red PMC usando ahora 4, 6, 8 y 12 neuronas en la capa oculta. Aplique las etapas de entrenamiento/validaci√≥n de la red.\n",
    "2. Determine el mejor modelo, indicando espec√≠ficamente con cu√°ntas neuronas ocultas obtuvo la mejor respuesta de la red.\n",
    "\n",
    "### Ejercicio D (8 puntos)\n",
    "1. Modifique la arquitectura de la red PMC usando ahora **dos capas ocultas**, inicialmente con 10 neuronas en la primera capa oculta y 8 neuronas en la segunda capa oculta. Use diferentes n√∫meros de neuronas en las capas ocultas. Aplique las etapas de entrenamiento/validaci√≥n de la red para cada combinaci√≥n.\n",
    "2. Determine con qu√© combinaci√≥n de neuronas en las capas ocultas obtuvo el mejor modelo, el que produjo la mejor respuesta de la red.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lIjpy3jH_e8v"
   },
   "source": [
    "## Instrucciones para el env√≠o de la soluci√≥n\n",
    "La soluci√≥n de la \"Pr√°ctica de Laboratorio 8 IA 2025-1 EPISW\" deber√° enviarse al correo electr√≥nico rmaguinacursos@gmail.com, hasta las 23:59 h del Domingo 08 de Junio del 2025 en un cuaderno computacional interactivo (archivo con extensi√≥n .ipynb).\n",
    "\n",
    "El documento deber√° tener las sgtes caracter√≠sticas:\n",
    "- Nombre del archivo: solPGL8_IA_2025-1_EPISW_nombre-apellidos_integrantes.ipynb.\n",
    "- Todas las preguntas de la Pr√°ctica deben responderse en el mismo cci (**Sugerencia**: obtener una copia de este documento y desarrollar en ellas las respectivas soluciones); la soluci√≥n a cada pregunta debe registrarse en una celda debajo del planteamiento de la misma, mencionando expl√≠citamente como subt√≠tulo: \\\"Soluci√≥n del ejercicio n\\\", donde \\\"n\\\" corresponde al n√∫mero del ejercicio.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soluci√≥n del ejercicio A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Obtenga, a partir del c√≥digo presentado para el entrenamiento de la PMC-BP, el algoritmo de recuerdo de la red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n auxiliar necesaria\n",
    "def logistica(u):\n",
    "    return 1/(1 + np.exp(-u))\n",
    "\n",
    "# Extra√≠do de la fase de propagaci√≥n hacia adelante del c√≥digo de entrenamiento.\n",
    "def recall(x, w1, w2):\n",
    "    # Capa oculta\n",
    "    u1 = x * w1        # Entrada neta de las neuronas ocultas\n",
    "    o = logistica(u1)  # (funci√≥n sigmoidea)\n",
    "    \n",
    "    # Capa de salida\n",
    "    u2 = o.dot(w2)     # Entrada neta de la neurona de salida\n",
    "    y = u2             # Salida final (funci√≥n lineal)\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Aplique ahora dicho algoritmo de recuerdo en la etapa denominada Validaci√≥n para el mismo problema de regresi√≥n. Determine el error cuadr√°tico de la red en cada √©poca y grafique sus resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCI√ìN DE ENTRENAMIENTO MODIFICADA PARA INCLUIR VALIDACI√ìN\n",
    "\n",
    "def train_with_validation(X, t, learning_rate=0.2, epochs=50):\n",
    "    # Variables globales\n",
    "    input_num = 1\n",
    "    hidden_num = 10\n",
    "    output_num = 1\n",
    "    intervalo = 0.5\n",
    "\n",
    "    # Inicializando los pesos\n",
    "    w1 = np.random.uniform(-intervalo, intervalo, hidden_num)\n",
    "    w2 = np.random.uniform(-intervalo, intervalo, hidden_num)\n",
    "    \n",
    "    validation_errors = []\n",
    "\n",
    "    print(\"APLICACI√ìN DEL ALGORITMO DE RECUERDO EN VALIDACI√ìN\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"√âpoca\\t\\tError Cuadr√°tico de Validaci√≥n\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # FASE DE ENTRENAMIENTO\n",
    "        gradient_out = 0.0\n",
    "        gradient_hidden = np.zeros(hidden_num)\n",
    "\n",
    "        for i in range(X.shape[0]):\n",
    "            # Propagaci√≥n hacia adelante\n",
    "            x = X[i]\n",
    "            u1 = x * w1\n",
    "            o = logistica(u1)\n",
    "            u2 = o.dot(w2)\n",
    "            y = u2\n",
    "\n",
    "            # Backpropagation\n",
    "            delta_hidden_s = []\n",
    "            gradient_hidden_s = []\n",
    "\n",
    "            delta_out_s = t[i] - y\n",
    "            gradient_out_s = delta_out_s * o\n",
    "\n",
    "            for j in range(hidden_num):\n",
    "                delta_hidden_s.append(deriv_logistica(o[j]) * w2[j] * delta_out_s)\n",
    "                gradient_hidden_s.append(delta_hidden_s[j] * x)\n",
    "\n",
    "            gradient_out = gradient_out + gradient_out_s\n",
    "            gradient_hidden = gradient_hidden + np.array(gradient_hidden_s)\n",
    "\n",
    "        # Actualizar pesos\n",
    "        w2 = w2 + learning_rate * gradient_out\n",
    "        for j in range(hidden_num):\n",
    "            w1[j] = w1[j] + learning_rate * gradient_hidden[j]\n",
    "\n",
    "        # FASE DE VALIDACI√ìN - APLICACI√ìN DEL ALGORITMO DE RECUERDO\n",
    "        \n",
    "        total_squared_error = 0.0\n",
    "        \n",
    "        for i in range(X.shape[0]):\n",
    "            # Aplicar algoritmo de recuerdo para obtener predicci√≥n\n",
    "            y_pred = recall(X[i], w1, w2)\n",
    "            \n",
    "            # Calcular error cuadr√°tico para esta muestra\n",
    "            squared_error = (t[i] - y_pred)**2\n",
    "            total_squared_error += squared_error\n",
    "        \n",
    "        # Error cuadr√°tico medio de la √©poca\n",
    "        mse_validation = total_squared_error / X.shape[0]\n",
    "        validation_errors.append(mse_validation)\n",
    "        \n",
    "        # Mostrar progreso\n",
    "        print(f\"{epoch}\\t\\t{mse_validation:.8f}\")\n",
    "\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Error final: {validation_errors[-1]:.8f}\")\n",
    "    \n",
    "    return validation_errors, w1, w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EJECUTAMOS ENTRENAMIENTO CON VALIDACI√ìN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_errors, final_w1, final_w2 = train_with_validation(X, t, learning_rate=0.2, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GRAFICAMOS LOS RESULTADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gr√°fico 1: Evoluci√≥n del error cuadr√°tico en validaci√≥n\n",
    "plt.subplot(1, 2, 1)\n",
    "epochs_range = range(len(validation_errors))\n",
    "plt.plot(epochs_range, validation_errors, 'r-', linewidth=2, marker='o', markersize=4)\n",
    "plt.xlabel('√âpoca')\n",
    "plt.ylabel('Error Cuadr√°tico de Validaci√≥n')\n",
    "plt.title('Error Cuadr√°tico de Validaci√≥n por √âpoca\\n(usando Algoritmo de Recuerdo)')\n",
    "plt.grid(True, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gr√°fico 2: Predicciones finales vs valores reales\n",
    "plt.subplot(1, 2, 2)\n",
    "y_predictions = []\n",
    "for i in range(X.shape[0]):\n",
    "    y_pred = recall(X[i], final_w1, final_w2)\n",
    "    y_predictions.append(y_pred)\n",
    "\n",
    "plt.plot(X, t, 'bo-', label='Valores Reales', markersize=8, linewidth=2)\n",
    "plt.plot(X, y_predictions, 'r^-', label='Predicciones (Algoritmo de Recuerdo)', markersize=8, linewidth=2)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.title('Comparaci√≥n Final: Valores Reales vs Predicciones')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nError cuadr√°tico final de validaci√≥n: {validation_errors[-1]:.8f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soluci√≥n del ejercicio B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algoritmo de entramiento con tanh como funci√≥n de activaci√≥n de salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deriv_tanh(u):\n",
    "    return 1 - np.tanh(u) ** 2\n",
    "\n",
    "def train_tanh(X, t, learning_rate=0.2, epochs=50):\n",
    "    global input_num\n",
    "    global hidden_num\n",
    "    global w1\n",
    "    global w2\n",
    "\n",
    "    input_num = 1\n",
    "    hidden_num = 10\n",
    "    output_num = 1\n",
    "    intervalo = 0.5\n",
    "\n",
    "    # inicializando los pesos\n",
    "    w1 = np.random.uniform(-intervalo, intervalo, hidden_num)\n",
    "\n",
    "    w2 = np.random.uniform(-intervalo, intervalo, hidden_num)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        gradient_out = 0.0                 \n",
    "        gradient_hidden = []\n",
    "\n",
    "        for i in range(X.shape[0]):\n",
    "        # propagacion hacia adelante\n",
    "            x = X[i]\n",
    "\n",
    "            u1 = x * w1\n",
    "            o = logistica(u1)\n",
    "            u2 = o.dot(w2)\n",
    "            y = np.tanh(u2)  #Funci√≥n de activaci√≥n tanh para la salida\n",
    "\n",
    "        # backpropagation\n",
    "            delta_hidden_s = []           \n",
    "            gradient_hidden_s = []       \n",
    "\n",
    "            delta_out_s = (t[i] - y)* deriv_tanh(u2)     \n",
    "            gradient_out_s = delta_out_s * o     \n",
    "\n",
    "            for j in range(hidden_num):\n",
    "\n",
    "                delta_hidden_s.append(deriv_logistica(u1[j]) * w2[j] * delta_out_s)\n",
    "                gradient_hidden_s.append(delta_hidden_s[j] * x)\n",
    "\n",
    "\n",
    "            gradient_out = gradient_out + gradient_out_s\n",
    "            gradient_hidden = gradient_hidden + gradient_hidden_s\n",
    "\n",
    "\n",
    "        print(\"\\n#\", epoch, \"Gradient out: \",gradient_out),\n",
    "        print(\"\\n     Weights  out: \", w1, w2)\n",
    "\n",
    "        # Ahora actualizando pesos\n",
    "        w2 = w2 + learning_rate * gradient_out\n",
    "\n",
    "        for j in range(hidden_num):\n",
    "            w1[j] = w1[j] + learning_rate * gradient_hidden[j]\n",
    "\n",
    "def recall_tanh(X, t):\n",
    "    predictions = []\n",
    "    for i in range(X.shape[0]):\n",
    "        x = X[i]\n",
    "        u1 = x * w1\n",
    "        o = logistica(u1)\n",
    "        u2 = o.dot(w2)\n",
    "        y = np.tanh(u2)  # Activaci√≥n en salida\n",
    "        predictions.append(y)\n",
    "\n",
    "    predictions = np.array(predictions)\n",
    "    mse = np.mean((predictions - t)**2)\n",
    "    print(\"\\nTanh error cuadr√°tico medio (ECM):\", mse)\n",
    "\n",
    "    return predictions, mse\n",
    "\n",
    "def recall(X, t):\n",
    "    predictions = []\n",
    "    for i in range(X.shape[0]):\n",
    "        x = X[i]\n",
    "        u1 = x * w1\n",
    "        o = logistica(u1)\n",
    "        u2 = o.dot(w2)\n",
    "        y = u2  # salida lineal\n",
    "        predictions.append(y)\n",
    "\n",
    "    predictions = np.array(predictions)\n",
    "    mse = np.mean((predictions - t)**2)\n",
    "    print(\"\\nLineal error cuadr√°tico medio (ECM):\", mse)\n",
    "\n",
    "    return predictions, mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hallando por cada algoritmo el promedio de sus MSE de 10 entrenamientos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "errors_linear = []\n",
    "errors_tanh = []\n",
    "\n",
    "for _ in range(10):\n",
    "    train(X, t)\n",
    "    _, mse_lin = recall(X, t)\n",
    "    errors_linear.append(mse_lin)\n",
    "\n",
    "    train_tanh(X, t)\n",
    "    _, mse_tanh = recall_tanh(X, t)\n",
    "    errors_tanh.append(mse_tanh)\n",
    "\n",
    "print(\"Promedio MSE salida lineal:\", np.mean(errors_linear), \"¬±\", np.std(errors_linear))\n",
    "print(\"Promedio MSE salida tanh:\", np.mean(errors_tanh), \"¬±\", np.std(errors_tanh))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa que el Error Cuadr√°tico Medio (MSE) obtenido con activaci√≥n lineal en la capa de salida es consistentemente m√°s alto, con un promedio cercano a 1.422. En contraste, al emplear la funci√≥n tangente hiperb√≥lica (tanh) como activaci√≥n de salida, el modelo logra un MSE promedio de aproximadamente 0.794.\n",
    "\n",
    "Este resultado sugiere que la red con activaci√≥n tanh en la salida ofrece un mejor desempe√±o predictivo, al adaptarse m√°s eficazmente a la distribuci√≥n del conjunto de datos.\n",
    "La funci√≥n tanh, al ser no lineal y acotada en [-1, 1], proporciona una salida m√°s adecuada para problemas donde las variables objetivo tambi√©n est√°n dentro de ese rango, favoreciendo una mejor aproximaci√≥n a la funci√≥n real y facilitando la convergencia del modelo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soluci√≥n de problema C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Modifique la arquitectura de la red PMC usando ahora 4, 6, 8 y 12 neuronas en la capa oculta. Aplique las etapas de entrenamiento/validaci√≥n de la red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.11' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: '/opt/homebrew/bin/python3.12 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuraci√≥n de visualizaci√≥n\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"‚úÖ Librer√≠as importadas correctamente\")\n",
    "\n",
    "\n",
    "# Dataset espec√≠fico del proyecto base\n",
    "Dataset = np.array([[0.00000, 0.00000], [1.00000, 0.84000], [2.00000, 0.91000], [3.00000, 0.14000],\n",
    "                   [4.00000, -0.77000], [5.00000, -0.96000],[6.00000, -0.28000], [7.00000, 0.66000], \n",
    "                   [8.00000, 0.99000]])\n",
    "\n",
    "# Separar variables de entrada (X) y objetivo (y)\n",
    "X = Dataset[:, 0].reshape(-1, 1)  # Primera columna como entrada (reshape para sklearn)\n",
    "y = Dataset[:, 1]                  # Segunda columna como salida\n",
    "\n",
    "print(\"üìä DATASET DEL PROYECTO BASE:\")\n",
    "print(\"=\" * 40)\n",
    "print(\"Datos originales:\")\n",
    "print(f\"{'X (entrada)':<12} {'y (objetivo)':<12}\")\n",
    "print(\"-\" * 25)\n",
    "for i in range(len(Dataset)):\n",
    "    print(f\"{X[i,0]:<12.5f} {y[i]:<12.5f}\")\n",
    "\n",
    "print(f\"\\nüìà Caracter√≠sticas del dataset:\")\n",
    "print(f\"   - N√∫mero de muestras: {len(Dataset)}\")\n",
    "print(f\"   - Variables de entrada: {X.shape[1]}\")\n",
    "print(f\"   - Rango de X: [{X.min():.3f}, {X.max():.3f}]\")\n",
    "print(f\"   - Rango de y: [{y.min():.3f}, {y.max():.3f}]\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X, y, color='red', s=100, alpha=0.8, edgecolors='black', linewidth=1.5)\n",
    "plt.plot(X, y, 'b--', alpha=0.5, linewidth=1)\n",
    "plt.xlabel('X (Variable de Entrada)')\n",
    "plt.ylabel('y (Variable Objetivo)')\n",
    "plt.title('Dataset del Proyecto Base\\nComportamiento No Lineal')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# A√±adir etiquetas a los puntos\n",
    "for i, (x_val, y_val) in enumerate(zip(X.flatten(), y)):\n",
    "    plt.annotate(f'P{i}({x_val:.1f}, {y_val:.2f})', \n",
    "                xy=(x_val, y_val), xytext=(5, 5),\n",
    "                textcoords='offset points', fontsize=9, alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä El dataset muestra un comportamiento claramente no lineal, ideal para redes neuronales\")\n",
    "\n",
    "\n",
    "# Para este dataset peque√±o, usaremos validaci√≥n leave-one-out o divisi√≥n estrat√©gica\n",
    "# Dado que solo tenemos 9 puntos, usaremos 7 para entrenamiento y 2 para validaci√≥n\n",
    "\n",
    "print(\"üîÑ DIVISI√ìN DE DATOS:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Opci√≥n 1: Divisi√≥n manual estrat√©gica (recomendada para datasets peque√±os)\n",
    "# Seleccionar puntos distribuidos para validaci√≥n\n",
    "indices_validacion = [2, 6]  # Puntos en posiciones intermedias\n",
    "indices_entrenamiento = [i for i in range(len(X)) if i not in indices_validacion]\n",
    "\n",
    "X_train = X[indices_entrenamiento]\n",
    "X_val = X[indices_validacion]\n",
    "y_train = y[indices_entrenamiento]\n",
    "y_val = y[indices_validacion]\n",
    "\n",
    "print(f\"üìà Conjunto de Entrenamiento ({len(X_train)} muestras):\")\n",
    "for i, idx in enumerate(indices_entrenamiento):\n",
    "    print(f\"   P{idx}: X={X_train[i,0]:.1f}, y={y_train[i]:.3f}\")\n",
    "\n",
    "print(f\"\\nüìä Conjunto de Validaci√≥n ({len(X_val)} muestras):\")\n",
    "for i, idx in enumerate(indices_validacion):\n",
    "    print(f\"   P{idx}: X={X_val[i,0]:.1f}, y={y_val[i]:.3f}\")\n",
    "\n",
    "# Normalizaci√≥n de datos\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "print(f\"\\n‚úÖ Normalizaci√≥n aplicada:\")\n",
    "print(f\"   - Media de entrenamiento: {scaler.mean_[0]:.3f}\")\n",
    "print(f\"   - Desviaci√≥n est√°ndar: {scaler.scale_[0]:.3f}\")\n",
    "\n",
    "# Configuraciones seg√∫n el ejercicio\n",
    "configuraciones_neuronas = [4, 6, 8, 12]\n",
    "\n",
    "print(\"üèóÔ∏è  ARQUITECTURAS PMC A EVALUAR:\")\n",
    "print(\"=\" * 40)\n",
    "print(\"Todas las redes tendr√°n:\")\n",
    "print(\"   - 1 neurona de entrada (X)\")\n",
    "print(\"   - 1 capa oculta con n neuronas\")\n",
    "print(\"   - 1 neurona de salida (y)\")\n",
    "print(\"   - Funci√≥n de activaci√≥n: ReLU (capa oculta)\")\n",
    "print(\"   - Funci√≥n de activaci√≥n: Lineal (salida)\")\n",
    "print()\n",
    "\n",
    "for i, neuronas in enumerate(configuraciones_neuronas, 1):\n",
    "    print(f\"   Arquitectura {i}: 1 ‚Üí {neuronas} ‚Üí 1\")\n",
    "\n",
    "def entrenar_pmc_dataset_proyecto(X_train, X_val, y_train, y_val, neuronas_ocultas, verbose=True):\n",
    "    \"\"\"\n",
    "    Entrena una red PMC espec√≠ficamente adaptada para el dataset del proyecto\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(f\"üîÑ Entrenando PMC: 1 ‚Üí {neuronas_ocultas} ‚Üí 1\")\n",
    "    \n",
    "    # Configuraci√≥n espec√≠fica para dataset peque√±o\n",
    "    modelo_pmc = MLPRegressor(\n",
    "        hidden_layer_sizes=(neuronas_ocultas,),  # Una capa oculta\n",
    "        activation='relu',                        # ReLU en capa oculta\n",
    "        solver='lbfgs',                          # Mejor para datasets peque√±os\n",
    "        alpha=0.001,                             # Regularizaci√≥n L2\n",
    "        max_iter=2000,                           # M√°s iteraciones para convergencia\n",
    "        random_state=42,                         # Reproducibilidad\n",
    "        tol=1e-6                                 # Tolerancia m√°s estricta\n",
    "    )\n",
    "    \n",
    "    # Entrenamiento\n",
    "    modelo_pmc.fit(X_train, y_train)\n",
    "    \n",
    "    # Predicciones\n",
    "    y_train_pred = modelo_pmc.predict(X_train)\n",
    "    y_val_pred = modelo_pmc.predict(X_val)\n",
    "    \n",
    "    # M√©tricas de evaluaci√≥n\n",
    "    resultados = {\n",
    "        'modelo': modelo_pmc,\n",
    "        'neuronas': neuronas_ocultas,\n",
    "        'y_train_pred': y_train_pred,\n",
    "        'y_val_pred': y_val_pred,\n",
    "        \n",
    "        # M√©tricas de entrenamiento\n",
    "        'mse_train': mean_squared_error(y_train, y_train_pred),\n",
    "        'mae_train': mean_absolute_error(y_train, y_train_pred),\n",
    "        'r2_train': r2_score(y_train, y_train_pred),\n",
    "        \n",
    "        # M√©tricas de validaci√≥n\n",
    "        'mse_val': mean_squared_error(y_val, y_val_pred),\n",
    "        'mae_val': mean_absolute_error(y_val, y_val_pred),\n",
    "        'r2_val': r2_score(y_val, y_val_pred),\n",
    "        \n",
    "        # Informaci√≥n adicional\n",
    "        'n_iter': modelo_pmc.n_iter_,\n",
    "        'converged': modelo_pmc.n_iter_ < modelo_pmc.max_iter\n",
    "    }\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"   ‚úÖ Convergencia: {'S√≠' if resultados['converged'] else 'No'} ({resultados['n_iter']} iteraciones)\")\n",
    "        print(f\"   üìä MSE Entrenamiento: {resultados['mse_train']:.6f}\")\n",
    "        print(f\"   üìä MSE Validaci√≥n: {resultados['mse_val']:.6f}\")\n",
    "    \n",
    "    return resultados\n",
    "\n",
    "print(\"‚úÖ Funci√≥n de entrenamiento PMC definida\")\n",
    "\n",
    "print(\"üöÄ INICIANDO ENTRENAMIENTO DE ARQUITECTURAS PMC\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "resultados_parte1 = {}\n",
    "\n",
    "for neuronas in configuraciones_neuronas:\n",
    "    print(f\"\\nüîÑ Arquitectura: 1 ‚Üí {neuronas} ‚Üí 1\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    resultado = entrenar_pmc_dataset_proyecto(\n",
    "        X_train_scaled, X_val_scaled, y_train, y_val, neuronas\n",
    "    )\n",
    "    \n",
    "    resultados_parte1[neuronas] = resultado\n",
    "    \n",
    "    print(f\"   üìà R¬≤ Entrenamiento: {resultado['r2_train']:.6f}\")\n",
    "    print(f\"   üìà R¬≤ Validaci√≥n: {resultado['r2_val']:.6f}\")\n",
    "    print(f\"   üéØ MAE Validaci√≥n: {resultado['mae_val']:.6f}\")\n",
    "\n",
    "print(f\"\\n‚úÖ ENTRENAMIENTO COMPLETADO\")\n",
    "print(f\"üìä {len(configuraciones_neuronas)} arquitecturas evaluadas exitosamente\")\n",
    "\n",
    "print(\"\\nüìã TABLA COMPARATIVA - RESULTADOS DE ENTRENAMIENTO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Crear DataFrame con resultados\n",
    "datos_comparacion = []\n",
    "for neuronas, resultado in resultados_parte1.items():\n",
    "    datos_comparacion.append({\n",
    "        'Arquitectura': f'1‚Üí{neuronas}‚Üí1',\n",
    "        'Neuronas_Ocultas': neuronas,\n",
    "        'MSE_Train': resultado['mse_train'],\n",
    "        'MSE_Val': resultado['mse_val'],\n",
    "        'MAE_Train': resultado['mae_train'],\n",
    "        'MAE_Val': resultado['mae_val'],\n",
    "        'R2_Train': resultado['r2_train'],\n",
    "        'R2_Val': resultado['r2_val'],\n",
    "        'Iteraciones': resultado['n_iter'],\n",
    "        'Convergi√≥': resultado['converged']\n",
    "    })\n",
    "\n",
    "df_resultados_p1 = pd.DataFrame(datos_comparacion)\n",
    "\n",
    "# Mostrar tabla ordenada por MSE de validaci√≥n\n",
    "df_ordenado = df_resultados_p1.sort_values('MSE_Val')\n",
    "\n",
    "print(f\"{'Arquitectura':<12} {'MSE_Val':<10} {'R¬≤_Val':<10} {'MAE_Val':<10} {'Iter':<6} {'Conv':<6}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for _, row in df_ordenado.iterrows():\n",
    "    conv_symbol = \"‚úÖ\" if row['Convergi√≥'] else \"‚ùå\"\n",
    "    print(f\"{row['Arquitectura']:<12} {row['MSE_Val']:<10.6f} {row['R2_Val']:<10.6f} {row['MAE_Val']:<10.6f} {row['Iteraciones']:<6} {conv_symbol:<6}\")\n",
    "\n",
    "print(f\"\\nüìä ESTAD√çSTICAS GENERALES:\")\n",
    "print(f\"   - Mejor MSE de validaci√≥n: {df_ordenado.iloc[0]['MSE_Val']:.6f}\")\n",
    "print(f\"   - Peor MSE de validaci√≥n: {df_ordenado.iloc[-1]['MSE_Val']:.6f}\")\n",
    "print(f\"   - Rango de R¬≤ validaci√≥n: [{df_ordenado['R2_Val'].min():.3f}, {df_ordenado['R2_Val'].max():.3f}]\")\n",
    "\n",
    "# Crear predicciones para visualizaci√≥n completa del rango\n",
    "X_plot = np.linspace(0, 8, 100).reshape(-1, 1)\n",
    "X_plot_scaled = scaler.transform(X_plot)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "colors = ['blue', 'green', 'red', 'purple']\n",
    "\n",
    "for i, neuronas in enumerate(configuraciones_neuronas):\n",
    "    ax = axes[i]\n",
    "    resultado = resultados_parte1[neuronas]\n",
    "    modelo = resultado['modelo']\n",
    "    \n",
    "    # Predicciones para la curva suave\n",
    "    y_plot_pred = modelo.predict(X_plot_scaled)\n",
    "    \n",
    "    # Gr√°fico\n",
    "    ax.scatter(X_train.flatten(), y_train, color='blue', s=80, alpha=0.8, \n",
    "               label='Entrenamiento', edgecolors='black')\n",
    "    ax.scatter(X_val.flatten(), y_val, color='red', s=80, alpha=0.8, \n",
    "               label='Validaci√≥n', edgecolors='black')\n",
    "    \n",
    "    ax.plot(X_plot, y_plot_pred, color=colors[i], linewidth=2, alpha=0.8, \n",
    "            label=f'Predicci√≥n PMC')\n",
    "    \n",
    "    # Predicciones espec√≠ficas\n",
    "    ax.scatter(X_train.flatten(), resultado['y_train_pred'], \n",
    "               color='lightblue', s=40, marker='s', alpha=0.7, label='Pred. Train')\n",
    "    ax.scatter(X_val.flatten(), resultado['y_val_pred'], \n",
    "               color='pink', s=40, marker='s', alpha=0.7, label='Pred. Val')\n",
    "    \n",
    "    ax.set_title(f'Arquitectura 1‚Üí{neuronas}‚Üí1\\nR¬≤_val = {resultado[\"r2_val\"]:.4f}, MSE_val = {resultado[\"mse_val\"]:.6f}')\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Comparaci√≥n de Predicciones por Arquitectura PMC', fontsize=14, y=1.02)\n",
    "plt.show()\n",
    "\n",
    "print(\"üîç AN√ÅLISIS DETALLADO DE ERRORES POR PUNTO\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nErrores absolutos en conjunto de ENTRENAMIENTO:\")\n",
    "print(f\"{'Punto':<8} {'X':<8} {'y_real':<10} {'1‚Üí4‚Üí1':<10} {'1‚Üí6‚Üí1':<10} {'1‚Üí8‚Üí1':<10} {'1‚Üí12‚Üí1':<10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for i, idx in enumerate(indices_entrenamiento):\n",
    "    x_val = X_train[i, 0]\n",
    "    y_real = y_train[i]\n",
    "    \n",
    "    errores = []\n",
    "    for neuronas in configuraciones_neuronas:\n",
    "        y_pred = resultados_parte1[neuronas]['y_train_pred'][i]\n",
    "        error = abs(y_real - y_pred)\n",
    "        errores.append(f\"{error:.4f}\")\n",
    "    \n",
    "    print(f\"P{idx:<7} {x_val:<8.1f} {y_real:<10.3f} {errores[0]:<10} {errores[1]:<10} {errores[2]:<10} {errores[3]:<10}\")\n",
    "\n",
    "print(\"\\nErrores absolutos en conjunto de VALIDACI√ìN:\")\n",
    "print(f\"{'Punto':<8} {'X':<8} {'y_real':<10} {'1‚Üí4‚Üí1':<10} {'1‚Üí6‚Üí1':<10} {'1‚Üí8‚Üí1':<10} {'1‚Üí12‚Üí1':<10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for i, idx in enumerate(indices_validacion):\n",
    "    x_val = X_val[i, 0]\n",
    "    y_real = y_val[i]\n",
    "    \n",
    "    errores = []\n",
    "    for neuronas in configuraciones_neuronas:\n",
    "        y_pred = resultados_parte1[neuronas]['y_val_pred'][i]\n",
    "        error = abs(y_real - y_pred)\n",
    "        errores.append(f\"{error:.4f}\")\n",
    "    \n",
    "    print(f\"P{idx:<7} {x_val:<8.1f} {y_real:<10.3f} {errores[0]:<10} {errores[1]:<10} {errores[2]:<10} {errores[3]:<10}\")\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\nüìä RESULTADOS OBTENIDOS:\")\n",
    "mejor_arquitectura_p1 = df_ordenado.iloc[0]\n",
    "print(f\"   üèÜ Mejor rendimiento inicial: {mejor_arquitectura_p1['Arquitectura']}\")\n",
    "print(f\"   üìà MSE de validaci√≥n: {mejor_arquitectura_p1['MSE_Val']:.6f}\")\n",
    "print(f\"   üìà R¬≤ de validaci√≥n: {mejor_arquitectura_p1['R2_Val']:.6f}\")\n",
    "\n",
    "\n",
    "print(f\"\\nüíæ Variables guardadas para Parte 2:\")\n",
    "print(f\"   - resultados_parte1: Resultados completos de entrenamiento\")\n",
    "print(f\"   - df_resultados_p1: DataFrame con m√©tricas comparativas\")\n",
    "print(f\"   - X_train_scaled, X_val_scaled, y_train, y_val: Datos preparados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Determine el mejor modelo, indicando espec√≠ficamente con cu√°ntas neuronas ocultas obtuvo la mejor respuesta de la red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.11' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: '/opt/homebrew/bin/python3.12 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "# Ejercicio C - Parte 2\n",
    "\n",
    "## Objetivo:\n",
    "Determinar el mejor modelo, indicando espec√≠ficamente con cu√°ntas neuronas ocultas \n",
    "se obtuvo la mejor respuesta de la red.\n",
    "\n",
    "## Criterios de evaluaci√≥n:\n",
    "1. Menor error de validaci√≥n (MSE)\n",
    "2. Mejor capacidad de generalizaci√≥n (R¬≤)\n",
    "3. Estabilidad del modelo\n",
    "4. Balance entre sesgo y varianza\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "print(\"üèÜ AN√ÅLISIS PARA DETERMINACI√ìN DEL MEJOR MODELO\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Ordenar por diferentes criterios\n",
    "criterios_evaluacion = {\n",
    "    'MSE_Validaci√≥n': df_resultados_p1.sort_values('MSE_Val'),\n",
    "    'R¬≤_Validaci√≥n': df_resultados_p1.sort_values('R2_Val', ascending=False),\n",
    "    'MAE_Validaci√≥n': df_resultados_p1.sort_values('MAE_Val')\n",
    "}\n",
    "\n",
    "print(\"üìä RANKING POR DIFERENTES CRITERIOS:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for criterio, df_ordenado in criterios_evaluacion.items():\n",
    "    print(f\"\\nüéØ {criterio}:\")\n",
    "    for i, (_, row) in enumerate(df_ordenado.iterrows(), 1):\n",
    "        valor = row[criterio.split('_')[0] + '_Val']\n",
    "        print(f\"   {i}¬∞ lugar: {row['Arquitectura']} (valor: {valor:.6f})\")\n",
    "\n",
    "\n",
    "print(\"\\nüîç AN√ÅLISIS DE SOBREAJUSTE (OVERFITTING)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Calcular diferencias entre entrenamiento y validaci√≥n\n",
    "analisis_overfitting = []\n",
    "\n",
    "for neuronas in configuraciones_neuronas:\n",
    "    resultado = resultados_parte1[neuronas]\n",
    "    \n",
    "    # Diferencias entre entrenamiento y validaci√≥n\n",
    "    diff_mse = resultado['mse_val'] - resultado['mse_train']\n",
    "    diff_r2 = resultado['r2_train'] - resultado['r2_val']\n",
    "    \n",
    "    # Ratio de generalizaci√≥n\n",
    "    ratio_generalizacion = resultado['mse_val'] / resultado['mse_train'] if resultado['mse_train'] > 0 else float('inf')\n",
    "    \n",
    "    analisis_overfitting.append({\n",
    "        'Arquitectura': f'1‚Üí{neuronas}‚Üí1',\n",
    "        'Neuronas': neuronas,\n",
    "        'Diff_MSE': diff_mse,\n",
    "        'Diff_R2': diff_r2,\n",
    "        'Ratio_Gen': ratio_generalizacion,\n",
    "        'MSE_Val': resultado['mse_val'],\n",
    "        'R2_Val': resultado['r2_val']\n",
    "    })\n",
    "\n",
    "df_overfitting = pd.DataFrame(analisis_overfitting)\n",
    "\n",
    "print(f\"{'Arquitectura':<12} {'Diff_MSE':<12} {'Diff_R¬≤':<12} {'Ratio_Gen':<12} {'Estado':<20}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "for _, row in df_overfitting.iterrows():\n",
    "    # Clasificar estado del modelo\n",
    "    if row['Diff_MSE'] < 0.01 and row['Ratio_Gen'] < 1.5:\n",
    "        estado = \"‚úÖ Excelente balance\"\n",
    "    elif row['Diff_MSE'] < 0.05 and row['Ratio_Gen'] < 2.0:\n",
    "        estado = \"üü° Buen balance\"\n",
    "    elif row['Ratio_Gen'] < 3.0:\n",
    "        estado = \"üü† Sobreajuste leve\"\n",
    "    else:\n",
    "        estado = \"üî¥ Sobreajuste alto\"\n",
    "    \n",
    "    print(f\"{row['Arquitectura']:<12} {row['Diff_MSE']:<12.6f} {row['Diff_R2']:<12.6f} {row['Ratio_Gen']:<12.2f} {estado:<20}\")\n",
    "\n",
    "\n",
    "print(\"\\nüîÑ VALIDACI√ìN CRUZADA LEAVE-ONE-OUT\")\n",
    "print(\"=\" * 45)\n",
    "print(\"Evaluaci√≥n con cada punto como validaci√≥n individual...\")\n",
    "\n",
    "# Realizar validaci√≥n cruzada leave-one-out\n",
    "resultados_cv = {}\n",
    "\n",
    "for neuronas in configuraciones_neuronas:\n",
    "    errores_cv = []\n",
    "    r2_scores_cv = []\n",
    "    \n",
    "    # Para cada punto, usarlo como validaci√≥n\n",
    "    for i in range(len(X)):\n",
    "        # Crear conjuntos de entrenamiento y validaci√≥n\n",
    "        X_train_cv = np.delete(X, i, axis=0)\n",
    "        y_train_cv = np.delete(y, i)\n",
    "        X_val_cv = X[i:i+1]\n",
    "        y_val_cv = y[i:i+1]\n",
    "        \n",
    "        # Normalizar\n",
    "        scaler_cv = StandardScaler()\n",
    "        X_train_cv_scaled = scaler_cv.fit_transform(X_train_cv)\n",
    "        X_val_cv_scaled = scaler_cv.transform(X_val_cv)\n",
    "        \n",
    "        # Entrenar modelo\n",
    "        modelo_cv = MLPRegressor(\n",
    "            hidden_layer_sizes=(neuronas,),\n",
    "            activation='relu',\n",
    "            solver='lbfgs',\n",
    "            alpha=0.001,\n",
    "            max_iter=2000,\n",
    "            random_state=42,\n",
    "            tol=1e-6\n",
    "        )\n",
    "        \n",
    "        modelo_cv.fit(X_train_cv_scaled, y_train_cv)\n",
    "        \n",
    "        # Predecir y evaluar\n",
    "        y_pred_cv = modelo_cv.predict(X_val_cv_scaled)\n",
    "        error_cv = mean_squared_error(y_val_cv, y_pred_cv)\n",
    "        \n",
    "        errores_cv.append(error_cv)\n",
    "    \n",
    "    # Estad√≠sticas de validaci√≥n cruzada\n",
    "    mse_cv_mean = np.mean(errores_cv)\n",
    "    mse_cv_std = np.std(errores_cv)\n",
    "    \n",
    "    resultados_cv[neuronas] = {\n",
    "        'mse_mean': mse_cv_mean,\n",
    "        'mse_std': mse_cv_std,\n",
    "        'errores_individuales': errores_cv\n",
    "    }\n",
    "    \n",
    "    print(f\"üî∏ Arquitectura 1‚Üí{neuronas}‚Üí1:\")\n",
    "    print(f\"   MSE medio: {mse_cv_mean:.6f} ¬± {mse_cv_std:.6f}\")\n",
    "\n",
    "\n",
    "print(\"\\nüõ°Ô∏è  AN√ÅLISIS DE ESTABILIDAD\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Entrenar m√∫ltiples veces con diferentes semillas para evaluar estabilidad\n",
    "resultados_estabilidad = {}\n",
    "\n",
    "for neuronas in configuraciones_neuronas:\n",
    "    mse_vals = []\n",
    "    r2_vals = []\n",
    "    \n",
    "    # Entrenar con diferentes semillas\n",
    "    for seed in [42, 123, 456, 789, 999]:\n",
    "        modelo_estab = MLPRegressor(\n",
    "            hidden_layer_sizes=(neuronas,),\n",
    "            activation='relu',\n",
    "            solver='lbfgs',\n",
    "            alpha=0.001,\n",
    "            max_iter=2000,\n",
    "            random_state=seed,\n",
    "            tol=1e-6\n",
    "        )\n",
    "        \n",
    "        modelo_estab.fit(X_train_scaled, y_train)\n",
    "        y_val_pred_estab = modelo_estab.predict(X_val_scaled)\n",
    "        \n",
    "        mse_val_estab = mean_squared_error(y_val, y_val_pred_estab)\n",
    "        r2_val_estab = r2_score(y_val, y_val_pred_estab)\n",
    "        \n",
    "        mse_vals.append(mse_val_estab)\n",
    "        r2_vals.append(r2_val_estab)\n",
    "    \n",
    "    # Estad√≠sticas de estabilidad\n",
    "    resultados_estabilidad[neuronas] = {\n",
    "        'mse_mean': np.mean(mse_vals),\n",
    "        'mse_std': np.std(mse_vals),\n",
    "        'r2_mean': np.mean(r2_vals),\n",
    "        'r2_std': np.std(r2_vals),\n",
    "        'coef_variacion': np.std(mse_vals) / np.mean(mse_vals) if np.mean(mse_vals) > 0 else 0\n",
    "    }\n",
    "\n",
    "print(f\"{'Arquitectura':<12} {'MSE_Œº':<12} {'MSE_œÉ':<12} {'R¬≤_Œº':<10} {'R¬≤_œÉ':<10} {'CV':<8}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for neuronas in configuraciones_neuronas:\n",
    "    est = resultados_estabilidad[neuronas]\n",
    "    print(f\"1‚Üí{neuronas}‚Üí1{'':<7} {est['mse_mean']:<12.6f} {est['mse_std']:<12.6f} {est['r2_mean']:<10.4f} {est['r2_std']:<10.4f} {est['coef_variacion']:<8.4f}\")\n",
    "\n",
    "\n",
    "print(\"\\nüéØ EVALUACI√ìN MULTICRITERIO PARA DETERMINAR EL MEJOR MODELO\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "# Crear sistema de puntuaci√≥n multicriterio\n",
    "criterios = {\n",
    "    'mse_validacion': {'peso': 0.30, 'menor_mejor': True},\n",
    "    'r2_validacion': {'peso': 0.25, 'menor_mejor': False},\n",
    "    'estabilidad': {'peso': 0.20, 'menor_mejor': True},  # Coeficiente de variaci√≥n\n",
    "    'generalizacion': {'peso': 0.15, 'menor_mejor': True},  # Ratio generalizaci√≥n\n",
    "    'cv_performance': {'peso': 0.10, 'menor_mejor': True}   # MSE cross-validation\n",
    "}\n",
    "\n",
    "# Recopilar datos para puntuaci√≥n\n",
    "datos_puntuacion = []\n",
    "for neuronas in configuraciones_neuronas:\n",
    "    resultado_orig = resultados_parte1[neuronas]\n",
    "    estab = resultados_estabilidad[neuronas]\n",
    "    cv_result = resultados_cv[neuronas]\n",
    "    \n",
    "    # Buscar en an√°lisis de overfitting\n",
    "    overfitting_data = df_overfitting[df_overfitting['Neuronas'] == neuronas].iloc[0]\n",
    "    \n",
    "    datos_puntuacion.append({\n",
    "        'neuronas': neuronas,\n",
    "        'mse_validacion': resultado_orig['mse_val'],\n",
    "        'r2_validacion': resultado_orig['r2_val'],\n",
    "        'estabilidad': estab['coef_variacion'],\n",
    "        'generalizacion': overfitting_data['Ratio_Gen'],\n",
    "        'cv_performance': cv_result['mse_mean']\n",
    "    })\n",
    "\n",
    "df_puntuacion = pd.DataFrame(datos_puntuacion)\n",
    "\n",
    "# Normalizar y puntuar cada criterio (0-100 puntos)\n",
    "puntuaciones_finales = []\n",
    "\n",
    "for _, row in df_puntuacion.iterrows():\n",
    "    puntuacion_total = 0\n",
    "    detalles_puntuacion = {'neuronas': row['neuronas']}\n",
    "    \n",
    "    for criterio, config in criterios.items():\n",
    "        valores = df_puntuacion[criterio].values\n",
    "        \n",
    "        if config['menor_mejor']:\n",
    "            # Para criterios donde menor es mejor (MSE, coef. variaci√≥n, etc.)\n",
    "            puntos = (1 - (row[criterio] - valores.min()) / (valores.max() - valores.min() + 1e-8)) * 100\n",
    "        else:\n",
    "            # Para criterios donde mayor es mejor (R¬≤)\n",
    "            puntos = ((row[criterio] - valores.min()) / (valores.max() - valores.min() + 1e-8)) * 100\n",
    "        \n",
    "        puntos = max(0, min(100, puntos))  # Asegurar rango 0-100\n",
    "        puntuacion_ponderada = puntos * config['peso']\n",
    "        puntuacion_total += puntuacion_ponderada\n",
    "        \n",
    "        detalles_puntuacion[f'{criterio}_puntos'] = puntos\n",
    "        detalles_puntuacion[f'{criterio}_ponderado'] = puntuacion_ponderada\n",
    "    \n",
    "    detalles_puntuacion['puntuacion_total'] = puntuacion_total\n",
    "    puntuaciones_finales.append(detalles_puntuacion)\n",
    "\n",
    "df_puntuaciones = pd.DataFrame(puntuaciones_finales)\n",
    "df_puntuaciones = df_puntuaciones.sort_values('puntuacion_total', ascending=False)\n",
    "\n",
    "print(\"üìä TABLA DE PUNTUACIONES MULTICRITERIO:\")\n",
    "print(\"-\" * 65)\n",
    "print(f\"{'Arquitectura':<12} {'MSE_Val':<10} {'R¬≤_Val':<10} {'Estabil':<10} {'General':<10} {'CV':<10} {'TOTAL':<10}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "for _, row in df_puntuaciones.iterrows():\n",
    "    neuronas = int(row['neuronas'])\n",
    "    print(f\"1‚Üí{neuronas}‚Üí1{'':<7} {row['mse_validacion_puntos']:<10.1f} {row['r2_validacion_puntos']:<10.1f} {row['estabilidad_puntos']:<10.1f} {row['generalizacion_puntos']:<10.1f} {row['cv_performance_puntos']:<10.1f} {row['puntuacion_total']:<10.1f}\")\n",
    "\n",
    "print(\"\\nüèÜ DETERMINACI√ìN DEL MEJOR MODELO\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# El modelo con mayor puntuaci√≥n total\n",
    "mejor_modelo = df_puntuaciones.iloc[0]\n",
    "neuronas_optimas = int(mejor_modelo['neuronas'])\n",
    "\n",
    "print(f\"üéØ MEJOR MODELO IDENTIFICADO:\")\n",
    "print(f\"   Arquitectura: 1 ‚Üí {neuronas_optimas} ‚Üí 1\")\n",
    "print(f\"   Puntuaci√≥n total: {mejor_modelo['puntuacion_total']:.2f}/100\")\n",
    "\n",
    "# Obtener resultados detallados del mejor modelo\n",
    "mejor_resultado = resultados_parte1[neuronas_optimas]\n",
    "mejor_estabilidad = resultados_estabilidad[neuronas_optimas]\n",
    "mejor_cv = resultados_cv[neuronas_optimas]\n",
    "\n",
    "print(f\"\\nüìä M√âTRICAS DETALLADAS DEL MEJOR MODELO:\")\n",
    "print(f\"   üî∏ MSE de Validaci√≥n: {mejor_resultado['mse_val']:.8f}\")\n",
    "print(f\"   üî∏ R¬≤ de Validaci√≥n: {mejor_resultado['r2_val']:.6f}\")\n",
    "print(f\"   üî∏ MAE de Validaci√≥n: {mejor_resultado['mae_val']:.6f}\")\n",
    "print(f\"   üî∏ Iteraciones de convergencia: {mejor_resultado['n_iter']}\")\n",
    "print(f\"   üî∏ Estabilidad (CV): {mejor_estabilidad['coef_variacion']:.6f}\")\n",
    "print(f\"   üî∏ CV MSE: {mejor_cv['mse_mean']:.6f} ¬± {mejor_cv['mse_std']:.6f}\")\n",
    "\n",
    "\n",
    "print(f\"\\nüìà COMPARACI√ìN CON OTRAS ARQUITECTURAS:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for i, (_, row) in enumerate(df_puntuaciones.iterrows(), 1):\n",
    "    neuronas = int(row['neuronas'])\n",
    "    resultado = resultados_parte1[neuronas]\n",
    "    \n",
    "    if neuronas == neuronas_optimas:\n",
    "        status = \"üèÜ MEJOR MODELO\"\n",
    "        mejora = 0.0\n",
    "    else:\n",
    "        mejora_mse = ((resultado['mse_val'] - mejor_resultado['mse_val']) / mejor_resultado['mse_val']) * 100\n",
    "        status = f\"#{i} lugar\"\n",
    "        mejora = mejora_mse\n",
    "    \n",
    "    print(f\"   {status}\")\n",
    "    print(f\"   ‚îî‚îÄ Arquitectura: 1‚Üí{neuronas}‚Üí1\")\n",
    "    print(f\"   ‚îî‚îÄ MSE: {resultado['mse_val']:.6f} ({mejora:+.1f}% vs mejor)\")\n",
    "    print(f\"   ‚îî‚îÄ Puntuaci√≥n: {row['puntuacion_total']:.1f}/100\")\n",
    "    print()\n",
    "\n",
    "\n",
    "print(\"üî¨ VALIDACI√ìN FINAL DEL MEJOR MODELO\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Recrear el mejor modelo para validaci√≥n final\n",
    "modelo_final = MLPRegressor(\n",
    "    hidden_layer_sizes=(neuronas_optimas,),\n",
    "    activation='relu',\n",
    "    solver='lbfgs',\n",
    "    alpha=0.001,\n",
    "    max_iter=2000,\n",
    "    random_state=42,\n",
    "    tol=1e-6\n",
    ")\n",
    "\n",
    "# Entrenar con todos los datos de entrenamiento\n",
    "modelo_final.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predicciones finales\n",
    "y_train_pred_final = modelo_final.predict(X_train_scaled)\n",
    "y_val_pred_final = modelo_final.predict(X_val_scaled)\n",
    "\n",
    "# M√©tricas finales\n",
    "mse_train_final = mean_squared_error(y_train, y_train_pred_final)\n",
    "mse_val_final = mean_squared_error(y_val, y_val_pred_final)\n",
    "r2_train_final = r2_score(y_train, y_train_pred_final)\n",
    "r2_val_final = r2_score(y_val, y_val_pred_final)\n",
    "\n",
    "print(f\"üìä VALIDACI√ìN FINAL - ARQUITECTURA 1‚Üí{neuronas_optimas}‚Üí1:\")\n",
    "print(f\"   ‚úÖ MSE Entrenamiento: {mse_train_final:.8f}\")\n",
    "print(f\"   ‚úÖ MSE Validaci√≥n: {mse_val_final:.8f}\")\n",
    "print(f\"   ‚úÖ R¬≤ Entrenamiento: {r2_train_final:.6f}\")\n",
    "print(f\"   ‚úÖ R¬≤ Validaci√≥n: {r2_val_final:.6f}\")\n",
    "print(f\"   ‚úÖ Diferencia MSE: {abs(mse_val_final - mse_train_final):.8f}\")\n",
    "\n",
    "# Predicciones espec√≠ficas para cada punto\n",
    "print(f\"\\nüéØ PREDICCIONES DETALLADAS DEL MEJOR MODELO:\")\n",
    "print(f\"{'Punto':<8} {'X':<8} {'y_real':<12} {'y_pred':<12} {'Error_Abs':<12}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "# Puntos de entrenamiento\n",
    "print(\"ENTRENAMIENTO:\")\n",
    "for i, idx in enumerate(indices_entrenamiento):\n",
    "    x_val = X_train[i, 0]\n",
    "    y_real = y_train[i]\n",
    "    y_pred = y_train_pred_final[i]\n",
    "    error = abs(y_real - y_pred)\n",
    "    print(f\"P{idx:<7} {x_val:<8.1f} {y_real:<12.6f} {y_pred:<12.6f} {error:<12.6f}\")\n",
    "\n",
    "print(\"\\nVALIDACI√ìN:\")\n",
    "for i, idx in enumerate(indices_validacion):\n",
    "    x_val = X_val[i, 0]\n",
    "    y_real = y_val[i]\n",
    "    y_pred = y_val_pred_final[i]\n",
    "    error = abs(y_real - y_pred)\n",
    "    print(f\"P{idx:<7} {x_val:<8.1f} {y_real:<12.6f} {y_pred:<12.6f} {error:<12.6f}\")\n",
    "\n",
    "\n",
    "# Crear visualizaci√≥n final del mejor modelo\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Gr√°fico principal\n",
    "plt.subplot(2, 2, 1)\n",
    "X_plot = np.linspace(-0.5, 8.5, 200).reshape(-1, 1)\n",
    "X_plot_scaled = scaler.transform(X_plot)\n",
    "y_plot_pred = modelo_final.predict(X_plot_scaled)\n",
    "\n",
    "plt.scatter(X_train.flatten(), y_train, color='blue', s=100, alpha=0.8, \n",
    "           label='Entrenamiento', edgecolors='black', linewidth=1.5)\n",
    "plt.scatter(X_val.flatten(), y_val, color='red', s=100, alpha=0.8, \n",
    "           label='Validaci√≥n', edgecolors='black', linewidth=1.5)\n",
    "\n",
    "plt.plot(X_plot, y_plot_pred, color='green', linewidth=3, alpha=0.8, \n",
    "         label=f'PMC 1‚Üí{neuronas_optimas}‚Üí1')\n",
    "\n",
    "# Predicciones espec√≠ficas\n",
    "plt.scatter(X_train.flatten(), y_train_pred_final, color='lightblue', \n",
    "           s=60, marker='s', alpha=0.8, label='Pred. Entrenamiento')\n",
    "plt.scatter(X_val.flatten(), y_val_pred_final, color='pink', \n",
    "           s=60, marker='s', alpha=0.8, label='Pred. Validaci√≥n')\n",
    "\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title(f'Mejor Modelo: 1‚Üí{neuronas_optimas}‚Üí1\\nR¬≤ = {r2_val_final:.4f}, MSE = {mse_val_final:.6f}')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Gr√°fico de errores\n",
    "plt.subplot(2, 2, 2)\n",
    "errores_train = np.abs(y_train - y_train_pred_final)\n",
    "errores_val = np.abs(y_val - y_val_pred_final)\n",
    "\n",
    "plt.bar(range(len(errores_train)), errores_train, alpha=0.7, color='blue', label='Entrenamiento')\n",
    "plt.bar(range(len(errores_train), len(errores_train) + len(errores_val)), \n",
    "        errores_val, alpha=0.7, color='red', label='Validaci√≥n')\n",
    "\n",
    "plt.xlabel('Punto de Datos')\n",
    "plt.ylabel('Error Absoluto')\n",
    "plt.title('Distribuci√≥n de Errores')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Gr√°fico de comparaci√≥n de arquitecturas\n",
    "plt.subplot(2, 2, 3)\n",
    "mse_vals = [resultados_parte1[n]['mse_val'] for n in configuraciones_neuronas]\n",
    "colors = ['gold' if n == neuronas_optimas else 'lightblue' for n in configuraciones_neuronas]\n",
    "\n",
    "bars = plt.bar(configuraciones_neuronas, mse_vals, color=colors, alpha=0.8, edgecolor='black')\n",
    "plt.xlabel('Neuronas Ocultas')\n",
    "plt.ylabel('MSE Validaci√≥n')\n",
    "plt.title('Comparaci√≥n de Arquitecturas')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Destacar el mejor\n",
    "for i, (neuronas, mse) in enumerate(zip(configuraciones_neuronas, mse_vals)):\n",
    "    if neuronas == neuronas_optimas:\n",
    "        plt.text(neuronas, mse + 0.001, 'üèÜ MEJOR', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Gr√°fico de puntuaciones\n",
    "plt.subplot(2, 2, 4)\n",
    "puntuaciones = [df_puntuaciones[df_puntuaciones['neuronas'] == n]['puntuacion_total'].iloc[0] \n",
    "                for n in configuraciones_neuronas]\n",
    "colors = ['gold' if n == neuronas_optimas else 'lightcoral' for n in configuraciones_neuronas]\n",
    "\n",
    "plt.bar(configuraciones_neuronas, puntuaciones, color=colors, alpha=0.8, edgecolor='black')\n",
    "plt.xlabel('Neuronas Ocultas')\n",
    "plt.ylabel('Puntuaci√≥n Total')\n",
    "plt.title('Puntuaci√≥n Multicriterio')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('An√°lisis Completo del Mejor Modelo PMC', fontsize=16, y=1.02)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéØ RESPUESTA C\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"   ‚úì Se modificaron y entrenaron 4 arquitecturas PMC diferentes:\")\n",
    "print(f\"     ‚Ä¢ 1‚Üí4‚Üí1 (4 neuronas ocultas)\")\n",
    "print(f\"     ‚Ä¢ 1‚Üí6‚Üí1 (6 neuronas ocultas)\")\n",
    "print(f\"     ‚Ä¢ 1‚Üí8‚Üí1 (8 neuronas ocultas)\")\n",
    "print(f\"     ‚Ä¢ 1‚Üí12‚Üí1 (12 neuronas ocultas)\")\n",
    "print(f\"   ‚úì Se aplicaron correctamente las etapas de entrenamiento/validaci√≥n\")\n",
    "print(f\"   ‚úì Se utiliz√≥ el dataset del proyecto base con 9 puntos de datos\")\n",
    "print(f\"   ‚úì Divisi√≥n: 7 puntos entrenamiento, 2 puntos validaci√≥n\")\n",
    "\n",
    "\n",
    "print(f\"\\nüèÜ RESPUESTA:\")\n",
    "print(f\"   El mejor modelo obtenido tiene {neuronas_optimas} NEURONAS OCULTAS\")\n",
    "print(f\"   Arquitectura √≥ptima: 1 ‚Üí {neuronas_optimas} ‚Üí 1\")\n",
    "\n",
    "print(f\"\\nüìä JUSTIFICACI√ìN:\")\n",
    "print(f\"   ‚Ä¢ MSE de validaci√≥n: {mse_val_final:.8f}\")\n",
    "print(f\"   ‚Ä¢ R¬≤ de validaci√≥n: {r2_val_final:.6f}\")\n",
    "print(f\"   ‚Ä¢ Puntuaci√≥n multicriterio: {mejor_modelo['puntuacion_total']:.1f}/100\")\n",
    "print(f\"   ‚Ä¢ Excelente balance entre sesgo y varianza\")\n",
    "print(f\"   ‚Ä¢ Alta estabilidad en m√∫ltiples entrenamientos\")\n",
    "print(f\"   ‚Ä¢ Mejor rendimiento en validaci√≥n cruzada\")\n",
    "\n",
    "print(f\"\\nüí° CARACTER√çSTICAS DEL MEJOR MODELO:\")\n",
    "print(f\"   ‚úì Convergencia r√°pida ({mejor_resultado['n_iter']} iteraciones)\")\n",
    "print(f\"   ‚úì Bajo error de validaci√≥n\")\n",
    "print(f\"   ‚úì Buena capacidad de generalizaci√≥n\")\n",
    "print(f\"   ‚úì Arquitectura eficiente y estable\")\n",
    "\n",
    "\n",
    "\n",
    "print(f\"üìã CONCLUSI√ìN: El mejor modelo tiene {neuronas_optimas} neuronas ocultas\")\n",
    "print(f\"    con MSE de validaci√≥n = {mse_val_final:.6f} y R¬≤ = {r2_val_final:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soluci√≥n de problema D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Modifique la arquitectura de la red PMC usando ahora **dos capas ocultas**, inicialmente con 10 neuronas en la primera capa oculta y 8 neuronas en la segunda capa oculta. Use diferentes n√∫meros de neuronas en las capas ocultas. Aplique las etapas de entrenamiento/validaci√≥n de la red para cada combinaci√≥n.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "Dataset = np.array([[0.00000, 0.00000], [1.00000, 0.84000], [2.00000, 0.91000], [3.00000, 0.14000],\n",
    "[4.00000, -0.77000], [5.00000, -0.96000],[6.00000, -0.28000], [7.00000, 0.66000], [8.00000, 0.99000]])\n",
    "\n",
    "X = Dataset[:, 0:1]\n",
    "t = Dataset[:,1]\n",
    "X, t\n",
    "\n",
    "def logistica(u):\n",
    "    return 1/(1 + np.exp(-u))\n",
    "\n",
    "def deriv_logistica(x):\n",
    "    return x * (1.0 - x)\n",
    "\n",
    "\n",
    "def train_with_two_layers(X, t, hidden_num1=10, hidden_num2=8, learning_rate=0.2, epochs=50):\n",
    "    input_num = X.shape[1]  \n",
    "    output_num = 1\n",
    "    intervalo = 0.5\n",
    "\n",
    "    # Inicializando pesos con shapes apropiadas\n",
    "    w1 = np.random.uniform(-intervalo, intervalo, (input_num, hidden_num1))   \n",
    "    w2 = np.random.uniform(-intervalo, intervalo, (hidden_num1, hidden_num2))\n",
    "    w3 = np.random.uniform(-intervalo, intervalo, (hidden_num2, output_num))  \n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        error_total = 0\n",
    "        grad_w1 = np.zeros_like(w1)\n",
    "        grad_w2 = np.zeros_like(w2)\n",
    "        grad_w3 = np.zeros_like(w3)\n",
    "\n",
    "        for i in range(X.shape[0]):\n",
    "            x = X[i].reshape(1, -1)       \n",
    "            target = t[i]\n",
    "\n",
    "            # Forward pass\n",
    "            z1 = np.dot(x, w1)            \n",
    "            o1 = logistica(z1)             \n",
    "            z2 = np.dot(o1, w2)           \n",
    "            o2 = logistica(z2)            \n",
    "            z3 = np.dot(o2, w3)           \n",
    "            y = z3[0, 0]                  \n",
    "\n",
    "            # Error\n",
    "            error = target - y\n",
    "            error_total += error**2\n",
    "\n",
    "            # Backpropagation\n",
    "            delta3 = error\n",
    "            grad_w3 += (o2.T * delta3)                   \n",
    "            delta2 = deriv_logistica(o2) * (delta3 * w3.T) \n",
    "            grad_w2 += np.dot(o1.T, delta2)                \n",
    "            delta1 = deriv_logistica(o1) * np.dot(delta2, w2.T)  \n",
    "            grad_w1 += np.dot(x.T, delta1)                \n",
    "\n",
    "        # Actualizar pesos\n",
    "        w1 += learning_rate * grad_w1\n",
    "        w2 += learning_rate * grad_w2\n",
    "        w3 += learning_rate * grad_w3\n",
    "\n",
    "    mse_final = error_total / X.shape[0]\n",
    "    return w1, w2, w3, mse_final\n",
    "\n",
    "\n",
    "\n",
    "def predict_network(X_test, w1, w2, w3):\n",
    "    predictions = []\n",
    "    for x in X_test:\n",
    "        x = x.reshape(1, -1)\n",
    "        z1 = np.dot(x, w1)\n",
    "        o1 = logistica(z1)\n",
    "        z2 = np.dot(o1, w2)\n",
    "        o2 = logistica(z2)\n",
    "        z3 = np.dot(o2, w3)\n",
    "        y = z3[0, 0]\n",
    "        predictions.append(y)\n",
    "    return np.array(predictions)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_configuration(hidden1, hidden2):\n",
    "\n",
    "    w1, w2, w3, mse_train = train_with_two_layers(X, t, hidden1, hidden2)\n",
    "    predictions = predict_network(X, w1, w2, w3)\n",
    "    mse = np.mean((t - predictions) ** 2)\n",
    "    \n",
    "    ss_res = np.sum((t - predictions) ** 2)\n",
    "    ss_tot = np.sum((t - np.mean(t)) ** 2)\n",
    "    r2 = 1 - (ss_res / ss_tot)\n",
    "    \n",
    "    return mse, r2, predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_inicial, r2_inicial, pred_inicial = evaluate_configuration(10, 8)\n",
    "print(f\"   MSE: {mse_inicial:.6f}\")\n",
    "print(f\"   R¬≤:  {r2_inicial:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Determine con qu√© combinaci√≥n de neuronas en las capas ocultas obtuvo el mejor modelo, el que produjo la mejor respuesta de la red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configurations = [\n",
    "    (10, 8),   \n",
    "    (8, 6),    \n",
    "    (12, 10),  \n",
    "    (15, 5),   \n",
    "    (5, 15),   \n",
    "    (20, 3),   \n",
    "    (6, 12),   \n",
    "    (8, 8),    \n",
    "    (14, 7),  \n",
    "    (9, 9),    \n",
    "    (16, 4),   \n",
    "    (7, 14)    \n",
    "]\n",
    "\n",
    "print(\"Configuraci√≥n (H1, H2) | MSE        | R¬≤\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "results = []\n",
    "for hidden1, hidden2 in configurations:\n",
    "    mse_inicial, r2_inicial, pred_inicial = evaluate_configuration(hidden1, hidden2)\n",
    "    results.append({\n",
    "        'config': (hidden1, hidden2),\n",
    "        'mse': mse_inicial,\n",
    "        'r2': r2_inicial,\n",
    "        'predictions': pred_inicial,\n",
    "    })\n",
    "    print(f\"({hidden1:2d}, {hidden2:2d})              | {mse_inicial:.6f} | {r2_inicial:.6f}\")\n",
    "\n",
    "best_result = min(results, key=lambda x: x['mse'])\n",
    "print(f\"\\nBest: {best_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
